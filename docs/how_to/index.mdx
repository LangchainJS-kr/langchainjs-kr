---
sidebar_position: 0
sidebar_class_name: hidden
pagination_prev: null
pagination_next: null
---

# How-to guides

Here you'll find answers to “How do I….?” types of questions.
These guides are _goal-oriented_ and _concrete_; they're meant to help you complete a specific task.
For conceptual explanations see [Conceptual Guides](/docs/concepts/).
For end-to-end walkthroughs see [Tutorials](/docs/tutorials).
For comprehensive descriptions of every class and function see [API Reference](https://v2.v02.api.js.langchain.com/).

## Installation

- [How to: install LangChain packages](/docs/how_to/installation/)

## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from an LLM](/docs/how_to/structured_output/)
- [How to: use a chat model to call tools](/docs/how_to/tool_calling/)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: debug your LLM apps](/docs/how_to/debugging/)

## LangChain Expression Language (LCEL)

LangChain Expression Language is a way to create arbitrary custom chains. It is built on the [`Runnable`](https://api.js.langchain.com/classes/langchain_core_runnables.Runnable.html) protocol.

[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.

- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: attach runtime arguments to a runnable](/docs/how_to/binding/)
- [How to: run custom functions](/docs/how_to/functions)
- [How to: pass through arguments from one step to the next](/docs/how_to/passthrough)
- [How to: add values to a chain's state](/docs/how_to/assign)
- [How to: add message history](/docs/how_to/message_history)
- [How to: route execution within a chain](/docs/how_to/routing)
- [How to: add fallbacks](/docs/how_to/fallbacks)

## Components

These are the core building blocks you can use when building applications.

### Prompt templates

Prompt Templates are responsible for formatting user input into a format that can be passed to a language model.

- [How to: use few shot examples](/docs/how_to/few_shot_examples)
- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)
- [How to: partially format prompt templates](/docs/how_to/prompts_partial)
- [How to: compose prompts together](/docs/how_to/prompts_composition)

### Example selectors

Example Selectors are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs/how_to/example_selectors)
- [How to: select examples by length](/docs/how_to/example_selectors_length_based)
- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)

### Chat models

Chat Models are newer forms of language models that take messages in and output a message.

- [How to: do function/tool calling](/docs/how_to/tool_calling)
- [How to: get models to return structured output](/docs/how_to/structured_output)
- [How to: cache model responses](/docs/how_to/chat_model_caching)
- [How to: create a custom chat model class](/docs/how_to/custom_chat)
- [How to: get log probabilities](/docs/how_to/logprobs)
- [How to: stream a response back](/docs/how_to/chat_streaming)
- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)

### LLMs

What LangChain calls LLMs are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_caching)
- [How to: create a custom LLM class](/docs/how_to/custom_llm)
- [How to: stream a response back](/docs/how_to/streaming_llm)
- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)

### Output parsers

Output Parsers are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)
- [How to: parse JSON output](/docs/how_to/output_parser_json)
- [How to: parse XML output](/docs/how_to/output_parser_xml)
- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing/)

### Document loaders

Document Loaders are responsible for loading documents from a variety of sources.

- [How to: load CSV data](/docs/how_to/document_loader_csv)
- [How to: load data from a directory](/docs/how_to/document_loader_directory)
- [How to: load PDF files](/docs/how_to/document_loader_pdf)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom)
- [How to: load HTML data](/docs/how_to/document_loader_html)
- [How to: load Markdown data](/docs/how_to/document_loader_markdown)

### Text splitters

Text Splitters take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](/docs/how_to/recursive_text_splitter)
- [How to: split by character](/docs/how_to/character_text_splitter)
- [How to: split code](/docs/how_to/code_splitter)
- [How to: split by tokens](/docs/how_to/split_by_token)

### Embedding models

Embedding Models take a piece of text and create a numerical representation of it.

- [How to: embed text data](/docs/how_to/embed_text)
- [How to: cache embedding results](/docs/how_to/caching_embeddings)

### Vector stores

Vector stores are databases that can efficiently store and retrieve embeddings.

- [How to: create and query vector stores](/docs/how_to/vectorstores)

### Retrievers

Retrievers are responsible for taking a query and returning relevant documents.

- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/multiple_queries)
- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)
- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)
- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
- [How to: reduce retrieval latency](/docs/how_to/reduce_retrieval_latency)

### Indexing

Indexing is the process of keeping your vectorstore in-sync with the underlying data source.

- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)

### Tools

LangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call).

- [How to: create custom tools](/docs/how_to/custom_tools)
- [How to: use built-in tools and built-in toolkits](/docs/how_to/tools_builtin)
- [How to: use a chat model to call tools](/docs/how_to/tool_calling/)
- [How to: add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how_to/tools_prompting)
- [How to: call tools using multi-modal data](/docs/how_to/tool_calls_multi_modal)

### Agents

:::note

For in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraphjs/) documentation.

:::

- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)

### Callbacks

- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime)
- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach)
- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)
- [How to: make callbacks run in the background](/docs/how_to/callbacks_backgrounding)

### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat)
- [How to: create a custom LLM class](/docs/how_to/custom_llm)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom)
- [How to: define a custom tool](/docs/how_to/custom_tools)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.

- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)
- [How to: stream](/docs/how_to/qa_streaming/)
- [How to: return sources](/docs/how_to/qa_sources/)
- [How to: return citations](/docs/how_to/qa_citations/)
- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)

### Extraction

Extraction is when you use LLMs to extract structured information from unstructured text.

- [How to: use reference examples](/docs/how_to/extraction_examples/)
- [How to: handle long text](/docs/how_to/extraction_long_text/)
- [How to: do extraction without using function calling](/docs/how_to/extraction_parse)

### Chatbots

Chatbots involve using an LLM to have a conversation.

- [How to: manage memory](/docs/how_to/chatbots_memory)
- [How to: do retrieval](/docs/how_to/chatbots_retrieval)
- [How to: use tools](/docs/how_to/chatbots_tools)

### Query analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.

- [How to: add examples to the prompt](/docs/how_to/query_few_shot)
- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)
- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)
- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)
- [How to: construct filters](/docs/how_to/query_constructing_filters)
- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)

### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.

- [How to: use prompting to improve results](/docs/how_to/sql_prompting)
- [How to: do query validation](/docs/how_to/sql_query_checking)
- [How to: deal with large databases](/docs/how_to/sql_large_db)

### Q&A over graph databases

You can use an LLM to do question answering over graph databases.

- [How to: map values to a database](/docs/how_to/graph_mapping)
- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)
- [How to: improve results with prompting](/docs/how_to/graph_prompting)
- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)
