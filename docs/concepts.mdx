# 개념 가이드

이 섹션에는 LangChain의 주요 파트에 대한 소개를 담고 있습니다.

## 아키텍쳐

LangChain 프레임워크는 여러 구성 요소로 이루어져 있습니다. 아래 다이어그램은 요소들 간의 관계를 보여줍니다.

import ThemedImage from "@theme/ThemedImage";
import useBaseUrl from "@docusaurus/useBaseUrl";

<ThemedImage
  alt="Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers."
  sources={{
    light: useBaseUrl("/svg/langchain_stack.svg"),
    dark: useBaseUrl("/svg/langchain_stack_dark.svg"),
  }}
  title="LangChain Framework Overview"
/>

### `@langchain/core`

이 패키지는 다양한 구성 요소의 기본 추상화와 이를 결합하는 방법을 포함하고 있습니다.
LLMs, 벡터 스토어, 검색기 등 핵심 구성 요소의 인터페이스가 여기서 정의됩니다.
여기에는 써드파티 통합이 정의되지 않습니다.
의존성은 의도적으로 매우 가볍게 유지됩니다.

### `@langchain/community`

이 패키지는 LangChain 커뮤니티에서 유지 관리하는 써드파티 통합을 포함하고 있습니다.
주요 파트너 패키지는 별도로 분리되어 있습니다(아래 참조).
이 패키지에는 다양한 구성 요소(LLMs, 벡터 스토어, 검색기 등)에 대한 모든 통합이 포함됩니다.
이 패키지의 모든 의존성은 패키지를 가능한 가볍게 유지하기 위해 필요한 경우에만 설치할 수 있습니다.

### 파트너 패키지

`@langchain/community`에 다양한 소규모 패키지 통합이 지속적으로 일어나는 동안, 인기 많은 통합은 별도의 패키지로 분리했습니다 (예: `langchain-openai`, `langchain-anthropic` 등).
이는 이러한 중요한 통합에 대한 지원을 개선하기 위해 수행되었습니다.

### `langchain`

`langchain` 주요 패키지에는 애플리케이션의 인지 아키텍처를 구성하는 체인, 에이전트, 검색 전략이 포함되어 있습니다.
이 구성요소들은 써드파티 통합이 아닙니다.
여기에 있는 모든 체인, 에이전트, 검색 전략은 특정 통합에 국한되지 않고 모든 통합에서 일반적으로 사용될 수 있습니다.

### [LangGraph.js](https://langchain-ai.github.io/langgraphjs/)

LangGraph.js는 LLMs(대형 언어 모델)을 사용하여 강력하고 상태를 유지하는 멀티 액터 애플리케이션을 구축하기 위해 단계들을 그래프의 엣지와 노드로 모델링하는 `langchain`의 확장형입니다.

LangGraph는 일반적인 유형의 에이전트를 만들기 위한 고급 인터페이스뿐만 아니라 더 많은 제어를 위한 저수준 API도 제공합니다.

### [LangSmith](https://docs.smith.langchain.com)

LLM 애플리케이션을 디버그, 테스트, 평가 및 모니터링할 수 있는 개발자 플랫폼입니다.

## 설치

고수준의 추상화를 사용하고 싶다면 `langchain` 패키지를 설치해야 합니다.

```bash npm2yarn
npm i langchain
```

특정 통합 패키지를 사용하려면 해당 패키지를 별도로 설치해야 합니다.
통합 목록 및 설치 방법은 [여기](/docs/integrations/platforms/)에서 확인할 수 있습니다.

LangSmith를 사용하려면 [여기](https://smith.langchain.com)에서 LangSmith 개발자 계정을 설정하고 API 키를 받아야 합니다.
그 후, 환경 변수를 설정하여 이를 활성화할 수 있습니다:

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=ls__...
```

## LangChain Expression Language

LangChain Expression Language(LCEL)은 선언적으로 체인을 쉽게 구성할 수 있는 방법입니다.
LCEL은 처음부터 **코드 변경 없이 프로토타입을 프로덕션에 투입할 수 있도록** 설계되었습니다. 가장 간단한 “프롬프트 + LLM” 체인부터 가장 복잡한 체인까지 지원합니다
(우리는 사람들이 프로덕션에서 100단계 이상의 LCEL 체인을 성공적으로 실행하는 것을 보았습니다). LCEL을 사용하고 싶은 몇 가지 이유를 강조하자면 다음과 같습니다:

**일급 스트리밍 지원**
LCEL로 체인을 구성하면 첫 번째 토큰까지의 시간을 최적으로 단축할 수 있습니다(첫 번째 출력 청크가 나올 때까지 경과 시간). 일부 체인의 경우, 예를 들어, 우리는 LLM에서 스트리밍 출력 파서로 바로 토큰을 스트리밍하고, LLM 제공자가 원시 토큰을 출력하는 속도와 동일한 속도로 파싱된 증분 출력 청크를 받게 됩니다.

**최적화된 병렬 실행**
LCEL 체인의 단계들이 병렬로 실행될 수 있을 때마다(예: 여러 검색기에서 문서를 가져오는 경우) 가능한 최소 지연 시간으로 자동으로 실행됩니다.

**재시도 및 폴백**
LCEL 체인의 모든 부분에 대해 재시도 및 폴백을 구성할 수 있습니다. 이는 대규모로 체인의 신뢰성을 높이는 훌륭한 방법입니다. 현재 재시도/폴백에 대한 스트리밍 지원을 추가하는 작업을 진행 중이며, 지연 시간 비용 없이 추가적인 신뢰성을 얻을 수 있습니다.

**중간 결과 접근**
더 복잡한 체인의 경우 최종 출력이 생성되기 전에도 중간 단계의 결과에 액세스하는 것이 매우 유용할 때가 많습니다. 이를 통해 최종 사용자에게 무언가가 진행 중임을 알리거나 체인을 디버그할 수 있습니다. 중간 결과를 스트리밍할 수 있으며, 이는 모든 [LangServe](https://www.langchain.com/langserve/) 서버에서 사용할 수 있습니다.

**입력 및 출력 스키마**
입력 및 출력 스키마는 체인의 구조에서 유추된 스키마를 각 LCEL 체인에 제공합니다. 이는 입력 및 출력의 유효성 검증에 사용될 수 있으며, LangServe의 필수 구성 요소입니다.

[**매끄러운 LangSmith 추적**](https://docs.smith.langchain.com)
체인이 점점 더 복잡해짐에 따라 각 단계에서 정확히 무슨 일이 일어나고 있는지 이해하는 것이 점점 더 중요해집니다.
LCEL을 사용하면 **모든** 단계에 대해서 최대한의 관찰, 디버깅을 위해 자동으로 [LangSmith](https://docs.smith.langchain.com)에 로그됩니다.

### 인터페이스

커스텀 체인을 쉽게 만들 수 있도록 ["Runnable"](https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html) 프로토콜을 구현했습니다.
많은 LangChain 구성 요소는 `Runnable` 프로토콜을 구현하며, 여기에는 채팅 모델, LLM, 출력 파서, 검색기, 프롬프트 템플릿 등이 포함됩니다. 또한 런너블과 함께 작업할 때 유용한 여러 가지 기본 요소가 있으며, 이에 대해 아래에서 자세히 읽을 수 있습니다.

이것은 표준 인터페이스로, 맞춤형 체인을 정의하고 표준 방식으로 호출하기 쉽게 만듭니다.
표준 인터페이스에는 다음이 포함됩니다:

- [`stream`](#stream): 응답의 청크를 스트리밍으로 반환
- [`invoke`](#invoke): 입력에서 체인을 호출
- [`batch`](#batch): 입력 배열에서 체인을 호출

**입력 타입**과 **출력 타입**은 컴포넌트에 따라 다릅니다:

| 컴포넌트 | 입력 타입                                      | 출력 타입        |
| -------- | ---------------------------------------------- | ---------------- |
| 프롬프트 | 객체                                           | 프롬프트 값      |
| 채팅 모델 | 단일 문자열, 채팅 메시지 목록 또는 프롬프트 값 | 채팅 메세지      |
| LLM      | 단일 문자열, 채팅 메시지 목록 또는 프롬프트 값 | 문자열           |
| 출력파서 | LLM 또는 채팅 모델의 출력                       | 파서에 따라 다름 |
| 검색기   | 단일 문자열                                    | 문서 목록        |
| 도구     | 도구에 따라 단일 문자열 또는 객체              | 도구에 따라 다름 |

## 컴포넌트 ( Components ) 

LangChain은 LLM을 사용하여 빌드하는 데 유용한 다양한 컴포넌트에 대해 표준화되고 확장 가능한 인터페이스 및 외부 통합 기능을 제공합니다.
일부 컴포넌트는 LangChain에서 구현하고, 일부 컴포넌트는 써드파티 통합을 활용하며, 나머지는 혼용됩니다.

### LLMs

LLM은 문자열을 입력으로 받아 문자열을 반환하는 언어 모델입니다.
이들은 전통적으로 구형 모델이며(신형 모델은 일반적으로 `채팅 모델`입니다, 아래 참조).

기본 모델은 입력 문자열과 출력 문자열을 처리하지만, LangChain 래퍼는 이 모델이 메시지를 입력으로 받을 수 있도록 합니다.
이를 통해 이 모델들은 채팅 모델들과 상호 교환이 가능합니다.
메시지가 입력으로 전달될 때, 기본 모델에 전달되기 전에 내부적으로 문자열로 포매팅됩니다.

LangChain은 자체 LLM을 제공하지 않으며, 써드파티 통합에 의존합니다.

### 채팅 모델 ( Chat models )

일련의 메시지를 입력으로 사용하고 채팅 메시지를 결과값으로 반환하는 언어 모델입니다.(단순 텍스트를 사용하는 것과는 다릅니다)
이들은 대체로 신형 모델이며(구형 모델은 일반적으로 `LLMs`입니다, 위 참조).
채팅 모델은 대화 메시지에 고유한 역할을 할당하는 것을 지원하여, AI, 사용자 및 시스템 메시지와 같은 지시사항의 메시지를 구분하는 데 도움이 됩니다.

기본 모델은 메시지를 입력으로 받아 메시지를 출력하지만, LangChain 래퍼는 이 모델이 문자열을 입력으로 받을 수 있도록 합니다.
이를 통해 이 모델들은 LLM과 상호 교환이 가능하며, 사용이 더 간단해집니다.
문자열이 입력으로 전달되면, 기본 모델에 전달되기 전에 내부적으로 HumanMessage로 변환됩니다.

LangChain은 자체 채팅 모델들을 제공하지 않으며, 써드파티 통합에 의존합니다.

우리는 채팅 모델들을 구성할 때 일부 표준화된 매개변수를 사용합니다:

- `model`: 모델의 이름

채팅 모델은 또한 해당 통합에 특화된 다른 매개변수도 받기도 합니다.

### 함수/도구 호출 ( Function/Tool Calling )

:::info
우리는 도구 호출이라는 용어를 함수 호출과 교차하여 사용합니다. 함수 호출은 때때로 단일 함수의 호출을 의미하지만,
우리는 모든 모델이 각 메시지에서 여러 도구 또는 함수 호출을 반환할 수 있는 것처럼 취급합니다.
:::

도구 호출은 모델이 사용자 정의 스키마와 일치하는 출력을 생성하여 주어진 프롬프트에 응답할 수 있도록 합니다.
용어 자체는 모델이 어떤 작업을 수행하는 것처럼 보이지만, 실제로는 그렇지 않습니다! 모델은 도구의 인자를 제시하며,
실제로 도구를 실행할지 여부는 사용자에게 달려 있습니다. 예를 들어, 비구조화된 텍스트에서
[스키마와 일치하는 출력을 추출](/docs/tutorials/extraction/)하려는 경우,
원하는 스키마와 일치하는 매개변수를 갖는 "추출" 도구를 모델에 제공한 다음, 생성된 출력을 최종 결과로 취급할 수 있습니다.

도구 호출에는 이름, 인수 객체 및 선택적 식별자가 포함됩니다. 인수 객체는
`{ argumentName: argumentValue }` 형태로 구조화됩니다.

[Anthropic](https://www.anthropic.com/), [Cohere](https://cohere.com/),
[Google](https://cloud.google.com/vertex-ai), [Mistral](https://mistral.ai/),
[OpenAI](https://openai.com/) 등과 같은 LLM 프로바이더들은 도구 호출 기능의 변형을 지원합니다.
이러한 기능은 일반적으로 LLM에 대한 요청에 사용 가능한 도구와 그 스키마를 포함할 수 있도록 하며,
응답에는 이러한 도구 호출이 포함될 수 있습니다. 예를 들어, 검색 엔진 도구가 주어진 경우,
LLM은 검색 엔진에 대한 호출을 먼저 실행하여 쿼리를 처리할 수 있습니다.
LLM을 호출하는 시스템은 도구 호출을 수신하고, 이를 실행하여 그 출력을 LLM에 반환하여 응답을 알릴 수 있습니다.
LangChain은 [내장 도구](/docs/integrations/tools/) 세트를 포함하고 있으며,
[커스텀 도구](/docs/how_to/custom_tools)를 사용자가 직접 정의할 수 있는 여러 방법을 지원합니다.

함수/도구 호출에는 크게 두 가지 유즈 케이스가 있습니다:

- [LLM으로부터 정형 데이터를 반환하는 법](/docs/how_to/structured_output/)
- [모델을 사용해서 도구를 호출하는 법](/docs/how_to/tool_calling/)

### 메시지 타입 ( Message types )

일부 언어 모델은 메시지 배열을 입력으로 받아 메시지를 반환합니다.
몇 가지 다른 타입의 메시지가 있습니다.
모든 메시지는 `role`, `content`, `response_metadata` 속성을 가지고 있습니다.

`role`은 메시지를 누가 말하고 있는지를 설명합니다.
LangChain은 다양한 역할에 대해 다른 메시지 클래스를 가지고 있습니다.

`content` 속성은 메시지의 내용을 설명합니다.
이는 몇 가지 다른 타입일 수 있습니다:

- 문자열 (대부분의 모델은 이 타입으로 콘텐츠를 처리합니다)
- 객체 목록 (이는 객체가 해당 입력 타입과 입력 위치에 대한 정보를 포함하는 멀티모달 입력에 사용됩니다)

#### HumanMessage

유저로부터의 메시지를 나타냅니다.

#### AIMessage

이것은 모델에서 온 메시지를 나타냅니다. `content` 속성 외에도 이러한 메시지는 다음과 같은 속성을 가지고 있습니다:

**`response_metadata`**

`response_metadata` 속성은 응답에 대한 추가 메타데이터를 포함합니다. 이 데이터는 종종 각 모델 프로바이더에 따라 다릅니다. 여기에는 로그 확률 및 토큰 사용량과 같은 정보가 저장될 수 있습니다.

**`tool_calls`**

이것은 도구를 호출하려는 언어 모델의 결정을 나타냅니다. `AIMessage` 출력의 일부로 포함됩니다. 이들은 `.tool_calls` 속성을 통해 접근할 수 있습니다.

이 속성은 객체 배열을 반환합니다. 각 객체는 다음과 같은 키를 가집니다:

- `name`: 호출해야 하는 도구의 이름.
- `args`: 해당 도구에 대한 인수.
- `id`: 해당 도구 호출의 ID.

#### SystemMessage

이것은 모델에게 어떻게 행동해야 하는지 알려주는 시스템 메시지를 나타냅니다. 모든 모델 프로바이더가 이를 지원하는 것은 아닙니다.

#### FunctionMessage

이것은 함수 호출의 결과를 나타냅니다. `role`과 `content` 외에도, 이 메시지에는 이 결과를 생성하기 위해 호출된 함수의 이름을 전달하는 `name` 매개변수가 있습니다.

#### ToolMessage

이것은 도구 호출의 결과를 나타냅니다. 이는 OpenAI의 `function` 및 `tool` 메시지 타입과 일치하기 위해 FunctionMessage와 구별됩니다. `role`과 `content` 외에도, 이 메시지에는 이 결과를 생성하기 위해 호출된 도구 호출의 ID를 전달하는 `tool_call_id` 매개변수가 있습니다.

### 프롬프트 템플릿 ( Prompt templates )

프롬프트 템플릿은 사용자 입력과 매개변수를 언어 모델에 대한 지시로 변환하는 데 도움이 됩니다.
이를 통해 모델이 컨텍스트를 이해하고 관련성 있고 일관된 언어 기반 출력을 생성할 수 있도록 도와줍니다.

프롬프트 템플릿은 객체를 입력으로 받는데, 각 키는 프롬프트 템플릿에서 입력할 변수를 나타냅니다.

프롬프트 템플릿은 PromptValue를 출력합니다. 이 PromptValue는 LLM 또는 채팅 모델에 전달될 수 있으며, 문자열이나 메시지 배열로 변환될 수도 있습니다.
이 PromptValue가 존재하는 이유는 문자열과 메시지 간에 쉽게 전환할 수 있도록 하기 위함입니다.

프롬프트 템플릿에는 몇 가지 다른 타입이 있습니다.

#### 문자열 프롬프트 템플릿 ( String PromptTemplate )

이 프롬프트 템플릿은 단일 문자열을 포매팅하는 데 사용되며, 일반적으로 더 간단한 입력에 사용됩니다.
예를 들어, 프롬프트 템플릿을 구성하고 사용하는 일반적인 방법은 다음과 같습니다:

```typescript
import { PromptTemplate } from "@langchain/core/prompts";

const promptTemplate = PromptTemplate.fromTemplate("{topic}에 관련된 농담해줘");

await promptTemplate.invoke({ topic: "고양이" });
```

#### 채팅 프롬프트 텔플릿 ( ChatPromptTemplate )

이 프롬프트 템플릿은 메시지 배열을 포매팅하는 데 사용됩니다. 이러한 "템플릿"은 자체적으로 템플릿 배열로 구성됩니다.
예를 들어, ChatPromptTemplate을 구성하고 사용하는 일반적인 방법은 다음과 같습니다:

```typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "당신은 유능한 비서입니다"],
  ["user", "{topic}에 관련된 농담해줘"],
]);

await promptTemplate.invoke({ topic: "고양이" });
```

위 예제에서, 이 ChatPromptTemplate은 호출 시 두 개의 메시지로 구성됩니다.
첫 번째는 포매팅할 변수가 없는 시스템 메시지입니다.
두 번째는 HumanMessage이며, 사용자가 전달하는 `topic` 변수로 포매팅됩니다.

#### 메세지 플레이스홀더 ( MessagesPlaceholder )

이 프롬프트 템플릿은 특정 위치에 메시지 배열을 추가하는 역할을 합니다.
위의 ChatPromptTemplate에서 우리는 각각 문자열인 두 개의 메시지를 포매팅하는 방법을 보았습니다.
하지만 사용자가 특정 위치에 삽입할 메시지 배열을 전달하고 싶다면 어떻게 해야 할까요?
이것이 MessagesPlaceholder를 사용하는 방법입니다.

```typescript
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { HumanMessage } from "@langchain/core/messages";

const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "너는 유능한 비서입니다."],
  new MessagesPlaceholder("msgs"),
]);

promptTemplate.invoke({ msgs: [new HumanMessage({ content: "안녕!" })] });
```

이 방법은 두 개의 메시지 배열을 생성합니다. 첫 번째는 시스템 메시지이고, 두 번째는 우리가 전달한 HumanMessage입니다.
만약 우리가 5개의 메시지를 전달했다면, 총 6개의 메시지(시스템 메시지와 전달된 5개의 메시지)가 생성됩니다.
이것은 메시지 배열을 특정 위치에 삽입하는 데 유용합니다.

`MessagesPlaceholder` 클래스를 명시적으로 사용하지 않고 동일한 작업을 수행하는 또 다른 방법은 다음과 같습니다:

```typescript
const promptTemplate = ChatPromptTemplate.fromMessages([
  ["system", "너는 유능한 비서입니다."],
  ["placeholder", "{msgs}"], // <-- 변한 부분
]);
```

### 예제 선택기 ( Example Selectors )

더 나은 성능을 달성하기 위한 일반적인 프롬프트 기법 중 하나는 예제를 프롬프트의 일부로 포함하는 것입니다.
점점 더 많은 모델이 함수나 도구 호출을 자동으로 처리할 수 있게 지원하고 있습니다.
때때로 이러한 예제는 프롬프트에 하드코딩되지만, 더 복잡한 경우에서는 동적으로 선택하는 것이 좋을 수 있습니다.
예제 선택기(Example Selectors)는 예제를 선택한 다음 프롬프트에 포매팅하는 역할을 하는 클래스입니다.

### 출력 파서 ( Output parsers )

:::note

여기서 말하는 정보는 모델에서 텍스트 출력을 받아 보다 구조화된 표현으로 파싱하는 파서를 의미합니다.
점점 더 많은 모델이 함수(또는 도구) 호출을 자동 처리를 지원합니다.
출력 파싱보다는 함수/도구 호출을 사용하는 것이 좋습니다.
자세한 내용은 [여기](/docs/concepts/#function-tool-calling)에서 확인하세요.

:::

모델의 출력을 가져와 다운스트림 작업에 더 적합한 형식으로 변환하는 역할을 합니다.
LLM을 사용하여 구조화된 데이터를 생성하거나, 모델 및 LLM의 출력을 정규화할 때 유용합니다.

출력 파서가 구현해야 하는 두 가지 주요 메서드는 다음과 같습니다:

- "Get format instructions": 언어 모델의 출력이 어떻게 포매팅되어야 하는지에 대한 지침을 포함하는 문자열을 반환하는 메서드.
- "Parse": 문자열(언어 모델의 응답)을 입력으로 받아 이를 일부 구조로 파싱하는 메서드.

그리고 선택적으로 구현할 수 있는 메서드가 하나 더 있습니다:

- "Parse with prompt": 문자열(언어 모델의 응답)과 프롬프트(해당 응답을 생성한 프롬프트)를 입력으로 받아 이를 일부 구조로 파싱하는 메서드. 이 메서드는 OutputParser가 출력을 재시도하거나 수정하려는 경우, 프롬프트에서 정보를 얻기 위해 제공됩니다.

출력 파서는 문자열 또는 `BaseMessage`를 입력으로 받아 임의의 타입을 반환할 수 있습니다.

LangChain에는 다양한 타입의 출력 파서가 있습니다. 다음은 LangChain이 지원하는 출력 파서 목록입니다. 아래 표에는 다양한 정보가 포함되어 있습니다:

**이름**: 출력 파서의 이름

**스트리밍 지원**: 출력 파서가 스트리밍을 지원하는지 여부

**입력 타입**: 예상 입력 타입. 대부분의 출력 파서는 문자열과 메시지 모두에서 작동하지만, 일부(예: OpenAI Functions)는 특정 인수가 포함된 메시지가 필요합니다.

**출력 타입**: 파서가 반환하는 객체의 출력 유형

**설명**: 이 출력 파서에 대한 설명 및 사용 시기

| 이름                        | 스트리밍 지원 | 입력 타입         | 출력 타입                   | 설명                                                                                                                      |
| ----------------------------------------------------------------------------------------------------------------- | ------------------ | ------------------------- | --------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| [JSON](https://v02.api.js.langchain.com/classes/langchain_core_output_parsers.JsonOutputParser.html)              | ✅                 | `string` \| `BaseMessage` | `설명<T>`                      | 지정된 JSON 객체를 반환합니다. Zod 스키마를 지정할 수 있으며, 해당 모델에 대한 JSON을 반환합니다.                                |
| [XML](https://v02.api.js.langchain.com/classes/langchain_core_output_parsers.XMLOutputParser.html)                | ✅                 | `string` \| `BaseMessage` | `Promise<XMLResult>`              | 태그 객체를 반환합니다. XML 출력이 필요할 때 사용하십시오. Anthropic와 같이 XML 작성을 잘하는 모델과 함께 사용하십시오.          |
| [CSV](https://v02.api.js.langchain.com/classes/langchain_core_output_parsers.CommaSeparatedListOutputParser.html) | ✅                 | `string` \| `BaseMessage` | `Array[string]`                   | 쉼표로 구분된 값의 배열을 반환합니다.                                                                                            |
| [Structured](https://v02.api.js.langchain.com/classes/langchain_core_output_parsers.StructuredOutputParser.html)  |                    | `string` \| `BaseMessage` | `Promise<TypeOf<T>>`              | LLM 응답에서 구조화된 JSON을 파싱합니다.                                                                                         |
| [HTTP](https://v02.api.js.langchain.com/classes/langchain_output_parsers.HttpResponseOutputParser.html)           | ✅                 | `string`                  | `Promise<Uint8Array>`             | LLM 응답을 파싱하여 HTTP(s)로 전송합니다. 서버/에지에서 LLM을 호출한 후, 콘텐츠/스트림을 클라이언트로 다시 보내는 데 유용합니다. |
| [Bytes](https://v02.api.js.langchain.com/classes/langchain_core_output_parsers.BytesOutputParser.html)            | ✅                 | `string` \| `BaseMessage` | `Promise<Uint8Array>`             | LLM 응답을 파싱하여 HTTP(s)로 전송합니다. 서버/에지에서 클라이언트로 LLM 응답을 스트리밍하는 데 유용합니다.                      |
| [Datetime](https://v02.api.js.langchain.com/classes/langchain_output_parsers.DatetimeOutputParser.html)           |                    | `string`                  | `Promise<Date>`                   | 응답을 `Date` 형식으로 파싱합니다.                                                                                               |
| [Regex](https://v02.api.js.langchain.com/classes/langchain_output_parsers.RegexParser.html)                       |                    | `string`                  | `Promise<Record<string, string>>` | 정규식 패턴을 사용하여 주어진 텍스트를 파싱하고, 파싱된 출력이 포함된 객체를 반환합니다.                                         |

### 채팅 이력 ( Chat History ) 

대부분의 LLM 애플리케이션에는 대화형 인터페이스가 있습니다.
대화의 필수 컴포넌트는 대화에서 이전에 도입된 정보를 참조할 수 있는 것 입니다.
최소한, 대화 시스템은 과거 메시지의 일부를 직접 액세스할 수 있어야 합니다.

`채팅 이력` 개념은 임의의 체인을 래핑하는 데 사용할 수 있는 LangChain의 클래스에 해당합니다.
이 `채팅 이력`은 기본 체인의 입력과 출력을 추적하고, 이를 메시지 데이터베이스에 메시지로 추가합니다.
향후 상호 작용 시 이러한 메시지를 로드하여 입력의 일부로 체인에 전달합니다.

### 문서 ( Document )

LangChain의 문서 객체는 일부 데이터에 대한 정보를 포함합니다. 여기엔 두 가지 속성이 있습니다:

- `pageContent: string`: 이 문서의 콘텐츠. 현재는 문자열만 포함됩니다.
- `metadata: Record<string, any>`: 이 문서와 관련된 임의의 메타데이터로 문서 ID, 파일 이름 등을 추적할 수 있습니다.

### 문서 로더 ( Document loaders ) 

이 클래스들은 문서 객체를 로드합니다. LangChain은 Slack, Notion, Google Drive 등 다양한 데이터 소스와의 수백 가지 통합을 통해 데이터를 로드할 수 있습니다.

각 문서 로더는 고유한 특정 매개변수를 가지고 있지만, 모두 동일한 방식으로 `.load` 메서드를 사용하여 호출할 수 있습니다.
유즈케이스는 다음과 같습니다:

```typescript
import { CSVLoader } from "langchain/document_loaders/fs/csv";

const loader = new CSVLoader();
// <-- Integration specific parameters here

const docs = await loader.load();
```

### 텍스트 스플리터 ( text splitter )

문서를 로드한 후에는 본인의 애플리케이션에 더 적합하도록 변환하고 싶을 것입니다. 가장 간단한 예로, 긴 문서를 모델의 컨텍스트 창에 맞출 수 있는 더 작은 청크로 분할하고 싶을 수 있습니다. LangChain에는 문서를 분할, 결합, 필터링 및 기타 방법으로 조작할 수 있는 여러 내장 문서 변환기가 있습니다.

긴 텍스트를 처리하려면 텍스트를 청크로 분할해야 합니다. 
듣기엔 단순해 보이지만, 이는 꽤나 복잡할 수 있습니다. 
이상적으로, 의미적으로 관계가 있는 텍스트 청크들을 같은 선상에 두고 싶을 것입니다.
"의미론적 관계성"이라는 것은 텍스트의 타입에 따라 다를 수 있습니다.
이 챕터에서는 이에 대한 예시들을 여러가지 방법을 소개해줍니다.

고 수준에서는, 텍스트 스플리터는 다음과 같이 작동합니다:

1. 텍스트를 작고 의미적으로 관계가 있는 청크로 분할합니다 (종종 문장 단위).
2. 이러한 작은 청크를 결합하여 특정 크기에 도달할 때까지 더 큰 청크로 만듭니다 (어떤 함수에 의해 측정된 크기).
3. 그 크기에 도달하면 그 청크를 독립된 텍스트로 만들고, 청크 간의 컨텍스트를 유지하기 위해 약간 겹치는 부분을 포함하여 새로운 청크를 생성하기 시작합니다.

이는 텍스트 스플리터를 맞춤 설정할 수 있는 두 가지 다른 축이 있음을 의미합니다:

1. 텍스트가 분할되는 방식
2. 청크 크기를 측정하는 방식

### 임베딩 모델 ( Embedding models )

임베딩 클래스는 텍스트 임베딩 모델과 인터페이스하기 위해 설계된 클래스입니다. 많은 임베딩 모델 프로바이더(OpenAI, Cohere, Hugging Face 등)가 있으며, 이 클래스는 이들 모두에 대해 표준 인터페이스를 제공하도록 설계되었습니다.

임베딩은 텍스트 조각 표현하는 벡터를 생성합니다. 이는 벡터 공간에서 텍스트를 생각할 수 있게 해주며, 벡터 공간에서 가장 유사한 텍스트 조각을 찾는 의미 검색과 같은 작업을 수행할 수 있기 때문에 유용합니다.

LangChain의 기본 임베딩 클래스는 문서 임베딩과 쿼리 임베딩을 위한 두 가지 메서드를 제공합니다. 전자는 여러 텍스트를 입력으로 받고, 후자는 단일 텍스트를 입력으로 받습니다. 이를 두 개의 별도 메서드로 제공하는 이유는 일부 임베딩 프로바이더가 문서(검색 대상)와 쿼리(검색 쿼리 자체)에 대해 다른 임베딩 메서드를 가지고 있기 때문입니다.

### 벡터스토어 ( VectorStore )

비구조화된 데이터를 저장하고 검색하는 가장 일반적인 방법 중 하나는 데이터를 임베딩하고 결과 임베딩 벡터를 저장한 다음,
쿼리 시 비구조화된 쿼리를 임베딩하여 임베딩된 쿼리와 '가장 유사한' 임베딩 벡터를 검색하는 것입니다.
벡터 스토어는 임베딩된 데이터를 저장하고 벡터 검색을 수행하는 작업을 처리합니다.

벡터 스토어는 다음과 같이 검색 인터페이스로 변환할 수 있습니다:

```typescript
const vectorstore = new MyVectorStore();
const retriever = vectorstore.asRetriever();
```

### 검색기 ( Retriever )

`검색기`는 비구조화된 쿼리를 주면 관련 문서를 반환하는 인터페이스입니다.
벡터 스토어보다 더 일반적입니다.
검색기는 문서를 저장할 필요는 없고, 단지 반환(또는 검색)만 하면 됩니다.
검색기는 벡터 스토어에서 생성할 수 있지만, [Exa 검색](/docs/integrations/retrievers/exa/) (웹 검색) 및 [Amazon Kendra](/docs/integrations/retrievers/kendra-retriever/)를 포함할 만큼 광범위합니다.

검색기는 문자열 쿼리를 입력으로 받아 `Document` 배열을 출력으로 반환합니다.

### 도구 ( Tools )

도구는 에이전트, 체인 또는 LLM이 세상과 상호작용하는 데 사용할 수 있는 인터페이스입니다.
이들은 몇 가지를 결합합니다:

1. 도구의 이름
2. 도구에 대한 설명
3. 도구의 입력값에 대한 JSON 스키마
4. 호출할 함수
5. 도구의 결과를 사용자에게 직접 반환할지 여부

이 모든 정보를 갖는 것은 유용합니다. 왜냐하면 이 정보가 행동 수행 시스템을 구축하는 데 사용될 수 있기 때문입니다! 이름, 설명, 그리고 JSON 스키마는 LLM을 프롬프팅하여 어떤 행동을 취해야 할지를 알 수 있게 해주며, 호출할 함수는 그 행동을 취하는 것과 같습니다.

도구의 입력이 단순할수록 LLM이 이를 활용할 수 있는 가능성이 높아집니다.
많은 에이전트는 단일 문자열 입력을 갖는 도구와만 작동합니다.

중요한 점은, 이름, 설명, 그리고 JSON 스키마(사용되는 경우) 모두가 프롬프트에서 사용된다는 것입니다. 따라서, 이들이 명확하고 도구가 어떻게 사용되어야 하는지를 정확히 설명하는 것이 매우 중요합니다. LLM이 도구 사용 방법을 이해하지 못하는 경우 기본 이름, 설명 또는 JSON 스키마를 변경해야 할 수도 있습니다.

### 툴킷 ( Toolkits )

툴킷은 특정 작업을 위해 함께 사용하도록 설계된 도구 모음입니다. 이들은 편리한 로딩 방법을 제공합니다.

모든 툴킷은 도구 배열을 반환하는 `getTools` 메서드를 노출합니다.
따라서 다음과 같이 사용할 수 있습니다:

```typescript
// 툴킷을 초기화 합니다.
const toolkit = new ExampleTookit(...)

// 도구 리스트
const tools = toolkit.getTools()
```

### 에이전트 ( Agent )

언어 모델 자체로는 액션을 취할 수 없습니다. 단지 텍스트를 출력할 뿐이죠.
LangChain의 주 유즈케이스는 **에이전트**를 만드는 것입니다.
에이전트는 LLM을 추론 엔진으로 사용하여 어떤 액션을 취할지와 그 액션의 입력값이 무엇이어야 하는지를 결정하는 시스템입니다.
그 액션들의 결과는 다시 에이전트에 피드백되어 더 많은 액션이 필요한지, 아니면 완료해도 되는지를 결정합니다.

[LangGraph](https://github.com/langchain-ai/langgraphjs)는 고도로 제어 가능하고 맞춤화된 에이전트를 만드는 것을 목표로 한 LangChain의 확장형입니다.
에이전트 개념에 대한 더 깊이 알고 싶으시면 해당 [문서](https://langchain-ai.github.io/langgraphjs/)를 확인해 보세요.

LangChain에는 사용 중단을 장려하는 `AgentExecutor`라는 기존 에이전트 개념이 있습니다.
AgentExecutor는 본질적으로 에이전트의 런타임이었습니다.
시작하기에는 훌륭한 도구였지만, 더 맞춤화된 에이전트를 만들기 시작하면 충분히 유연하지 않았습니다.
이를 해결하기 위해 고도로 제어 가능한 런타임을 제공하는 유연한 LangGraph를 구축했습니다.

아직 AgentExecutor를 사용 중이라면 걱정하지 마세요. [AgentExecutor 사용 방법](/docs/how_to/agent_executor)에 대한 가이드를 여전히 제공하고 있습니다.
그러나 [LangGraph](https://github.com/langchain-ai/langgraphjs)로 전환하는 것을 권장합니다.

### 콜백 ( Callback )

LangChain은 LLM 애플리케이션의 다양한 단계에 연결할 수 있는 콜백 시스템을 제공합니다. 이는 로깅, 모니터링, 스트리밍 및 기타 작업에 유용합니다.

API 전반에서 사용할 수 있는 `callbacks` 인수를 사용하여 이러한 이벤트를 구독할 수 있습니다. 이 인수는 하나 이상의 메서드를 구현하는 핸들러 객체 리스트로, 아래에 자세히 설명되어 있습니다.

#### 콜백 핸들러 ( Callback handlers )

`CallbackHandlers`는 구독할 수 있는 각 이벤트에 대한 메서드를 갖는 [`CallbackHandler`](https://api.js.langchain.com/interfaces/langchain_core_callbacks_base.CallbackHandlerMethods.html) 인터페이스를 구현하는 객체입니다.
이벤트가 발생하면 `CallbackManager`가 각 핸들러의 적절한 메서드를 호출합니다.

```ts
interface CallbackHandlerMethods {
  handleAgentAction?(action, runId, parentRunId?, tags?): void | Promise<void>;
  handleAgentEnd?(action, runId, parentRunId?, tags?): void | Promise<void>;
  handleChainEnd?(outputs, runId, parentRunId?, tags?, kwargs?): any;
  handleChainError?(err, runId, parentRunId?, tags?, kwargs?): any;
  handleChainStart?(
    chain,
    inputs,
    runId,
    parentRunId?,
    tags?,
    metadata?,
    runType?,
    name?
  ): any;
  handleChatModelStart?(
    llm,
    messages,
    runId,
    parentRunId?,
    extraParams?,
    tags?,
    metadata?,
    name?
  ): any;
  handleLLMEnd?(output, runId, parentRunId?, tags?): any;
  handleLLMError?(err, runId, parentRunId?, tags?): any;
  handleLLMNewToken?(token, idx, runId, parentRunId?, tags?, fields?): any;
  handleLLMStart?(
    llm,
    prompts,
    runId,
    parentRunId?,
    extraParams?,
    tags?,
    metadata?,
    name?
  ): any;
  handleRetrieverEnd?(documents, runId, parentRunId?, tags?): any;
  handleRetrieverError?(err, runId, parentRunId?, tags?): any;
  handleRetrieverStart?(
    retriever,
    query,
    runId,
    parentRunId?,
    tags?,
    metadata?,
    name?
  ): any;
  handleText?(text, runId, parentRunId?, tags?): void | Promise<void>;
  handleToolEnd?(output, runId, parentRunId?, tags?): any;
  handleToolError?(err, runId, parentRunId?, tags?): any;
  handleToolStart?(
    tool,
    input,
    runId,
    parentRunId?,
    tags?,
    metadata?,
    name?
  ): any;
}
```

#### 콜백 전달

`콜백` 속성은 API 전반의 대부분의 객체(모델, 도구, 에이전트 등)에서 두 가지 방식으로 사용할 수 있습니다:

- **생성자 콜백**: 생성자에서 정의되며, 예를 들어 `new ChatAnthropic({ callbacks: [handler], tags: ["a-tag"] })`와 같이 사용됩니다. 이 경우, 해당 객체에서 수행되는 모든 호출에 대해 콜백이 사용되며, 해당 객체에만 범위가 한정됩니다.
  예를 들어, 생성자 콜백으로 채팅 모델을 초기화한 다음 체인 내에서 사용할 경우, 콜백은 해당 모델에 대한 호출에만 호출됩니다.
- **요청 콜백**: 요청을 호출하는 데 사용되는 `invoke` 메서드에 전달됩니다. 이 경우, 콜백은 해당 특정 요청과 그 요청에 포함된 모든 하위 요청(예: 모델 호출을 트리거하는 시퀀스 호출, `invoke()` 메서드에 전달된 동일한 핸들러 사용)에 대해서만 사용됩니다.
  `invoke()` 메서드에서는 콜백이 config 매개변수를 통해 전달됩니다.

## 기술

### 함수/도구 호출 ( Function/Tool Calling )

:::info
우리는 도구 호출이라는 용어를 함수 호출과 교차하여 사용합니다. 함수 호출은 때때로 단일 함수의 호출을 의미하지만, 우리는 모든 모델이 각 메시지에서 여러 도구 또는 함수 호출을 반환할 수 있는 것처럼 다룹니다.
:::

도구 호출은 모델이 사용자 정의 스키마와 일치하는 출력을 생성하여 주어진 프롬프트에 응답할 수 있도록 합니다.
이름은 모델이 어떤 작업을 수행하는 것처럼 보이지만, 실제로는 그렇지 않습니다! 모델은 도구의 인수를 제시하며,
실제로 도구를 실행할지 여부는 사용자에게 달려 있습니다. 예를 들어, 비구조화된 텍스트에서 [스키마와 일치하는 출력을 추출](/docs/tutorials/extraction)하려는 경우,
원하는 스키마와 일치하는 매개변수를 갖는 "추출" 도구를 모델에 제공한 다음, 생성된 출력을 최종 결과로 사용할 수 있습니다.

도구 호출에는 이름, 인수 사전, 그리고 선택적 식별자가 포함됩니다. 인수 사전은 `{argument_name: argument_value}` 형태로 구조화됩니다.

여러 LLM 프로바이더, 예를 들어 [Anthropic](https://www.anthropic.com/), [Cohere](https://cohere.com/),
[Google](https://cloud.google.com/vertex-ai), [Mistral](https://mistral.ai/), [OpenAI](https://openai.com/)
등은 도구 호출 기능 변형을 지원합니다. 이러한 기능은 일반적으로 LLM에 대한 요청에 사용 가능한 도구와 그 스키마를 포함할 수 있도록 하며,
응답에는 이러한 도구 호출이 포함될 수 있습니다. 예를 들어, 검색 엔진 도구가 주어진 경우, LLM은 검색 엔진에 대한 호출을 먼저 발행하여 쿼리를 처리할 수 있습니다.
LLM을 호출하는 시스템은 도구 호출을 수신하고, 이를 실행하여 그 출력을 LLM에 반환하여 응답에 반영할 수 있습니다.
LangChain은 [내장 도구](/docs/integrations/tools/) 세트를 포함하고 있으며, 사용자 정의 [커스텀 도구](/docs/how_to/custom_tools)를 정의하는 여러 방법을 지원합니다.

LangChain은 다양한 모델에서 일관된 도구 호출을 위한 표준 인터페이스를 제공합니다.

표준 인터페이스는 다음으로 구성됩니다:

- `ChatModel.bindTools()`: 모델이 호출할 수 있는 도구를 지정하는 메서드.
- `AIMessage.toolCalls`: 모델이 요청한 도구 호출에 접근할 수 있는 `AIMessage`의 속성.

함수/도구 호출의 주요 유즈 케이스는 두 가지입니다:

- [LLM에서 구조화된 데이터를 반환하는 방법](/docs/how_to/structured_output/)
- [모델을 사용하여 도구를 호출하는 방법](/docs/how_to/tool_calling/)

### 검색 ( Retrieval )

LangChain은 여러 고급 검색 유형을 제공합니다. 아래는 전체 목록과 함께 다음과 같은 정보가 포함되어 있습니다:

**이름**: 검색 알고리즘 이름.

**인덱스 타입**: 어떤 인덱스 유형(있는 경우)에 의존하는지.

**LLM 사용 여부**: 검색에서 LLM 사용 여부.

**사용 시기**: 검색을 고려해야 할 때에 대한 설명.

**설명**: 검색 알고리즘이 수행하는 작업 설명.

| 이름                                                                 | 인덱스 타입                   | LLM 사용 여부               | 사용 시기                                                                                                    | 설명                                                                                                                                                                                                |
| -------------------------------------------------------------------- | ---------------------------- | ------------------------- | -------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Vectorstore](/docs/how_to/vectorstore_retriever/)                   | Vectorstore                  | No                        | 빠르고 간단한 방법을 찾고 있는 초보자라면, 이 방법을 사용하세요.                                               | 가장 단순한 방법이며 시작하기에 가장 쉬운 방법입니다. 각 텍스트 조각에 대한 임베딩을 생성하는 것을 포함합니다.                                                                                             |
| [ParentDocument](/docs/how_to/parent_document_retriever/)            | Vectorstore + Document Store | No                        | 페이지에 개별적으로 인덱싱하는 것이 가장 좋지만, 함께 검색하는 것이 좋은 더 작은 정보 조각들이 많이 있는 경우. | 이는 각 문서에 대해 여러 청크를 인덱싱하는 것을 포함합니다. 그런 다음 임베딩 공간에서 가장 유사한 청크를 찾지만, 개별 청크가 아닌 전체 상위 문서를 검색하여 반환합니다.                                    |
| [Multi Vector](/docs/how_to/multi_vector/)                           | Vectorstore + Document Store | Sometimes during indexing | 문서에서 텍스트 자체보다 인덱싱하기 더 적합한 정보를 추출할 수 있는 경우.                                      | 이는 각 문서에 대해 여러 벡터를 생성하는 것을 포함합니다. 각 벡터는 다양한 방법으로 생성될 수 있으며, 예를 들어 텍스트 요약과 가상 질문이 포함될 수 있습니다.                                              |
| [Self Query](/docs/how_to/self_query/)                               | Vectorstore                  | Yes                       | 사용자가 텍스트와의 유사성보다는 메타데이터를 기반으로 문서를 검색하는 것이 더 나은 질문을 하는 경우.          | 이는 사용자의 입력을 두 가지로 변환하기 위해 LLM을 사용합니다: (1) 의미적으로 검색할 문자열, (2) 이를 보완하는 메타데이터 필터. 이는 종종 질문이 문서의 내용이 아니라 메타데이터에 관한 경우에 유용합니다. |
| [Contextual Compression](/docs/how_to/contextual_compression/)       | Any                          | Sometimes                 | 검색된 문서에 너무 많은 관련 없는 정보가 포함되어 있어 LLM이 산만해지는 경우.                                  | 이는 다른 검색기 위에 후처리 단계를 추가하여 검색된 문서에서 가장 관련성이 높은 정보만 추출합니다. 이는 임베딩이나 LLM을 사용하여 수행할 수 있습니다.                                                      |
| [Time-Weighted Vectorstore](/docs/how_to/time_weighted_vectorstore/) | Vectorstore                  | No                        | 문서에 타임스탬프가 연결되어 있고, 가장 최신의 문서를 검색하고자 하는 경우                                     | 이는 의미적 유사성(일반 벡터 검색처럼)과 최신성(색인화된 문서의 타임스탬프 확인)을 결합하여 문서를 검색합니다.                                                                                             |
| [Multi-Query Retriever](/docs/how_to/multiple_queries/)              | Any                          | Yes                       | 사용자가 복잡한 질문을 하고 여러 개의 독립된 정보가 필요한 경우                                                | 이는 LLM을 사용하여 원래의 질문에서 여러 개의 쿼리를 생성합니다. 이는 원래의 쿼리가 여러 주제에 대한 정보를 필요로 할 때 유용합니다. 여러 쿼리를 생성함으로써 각 쿼리에 대한 문서를 검색할 수 있습니다.    |

### 텍스트 분할 ( Text splitting )

LangChain은 다양한 유형의 `텍스트 스플리터`를 제공합니다.
이들은 주요 `langchain` 패키지에서 사용할 수 있지만, 별도의 [`@langchain/textsplitters`](https://www.npmjs.com/package/@langchain/textsplitters) 패키지에서 사용할 수도 있습니다.

표의 열:

- **이름**: 텍스트 스플리터의 이름
- **클래스**: 텍스트 스플리터를 구현하는 클래스
- **분할 기준**: 텍스트 스플리터가 텍스트를 분할하는 방식
- **메타데이터 추가 여부**: 청크의 출처에 대한 메타데이터를 추가 여부
- **설명**: splitter에 대한 설명 및 사용 권장사항

| 이름      | 클래스                                                                 | 분할 기준                   | 메타데이터 추가 여부 | 설명                                                                                                                               |
| --------- | ----------------------------------------------------------------------- | --------------------------- | ------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| Recursive | [RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter/) | 유저가 정의한 문자의 리스트 |               | 재귀적으로 텍스트를 분할합니다. 이 분할은 관련된 텍스트 조각을 서로 가까이 유지하려고 합니다. 이것으로 텍스트 분할하는 것을 권장드립니다. |
| Code      | [many languages](/docs/how_to/code_splitter/)                           | 코드(Python, JS) 특화 문자  |               | 코딩 언어에 특화된 문자 기반으로 텍스트를 분할합니다. 총 15개의 다른 프로그래밍 언어를 선택할 수 있습니다.                                |
| Token     | [many classes](/docs/how_to/split_by_token/)                            | 토큰                        |               | 토큰을 기준으로 텍스트를 분할합니다. 토큰을 측정하는 몇 가지 다른 방법이 존재합니다.                                                      |
| Character | [CharacterTextSplitter](/docs/how_to/character_text_splitter/)          | 유저가 정의한 문자          |               | 사용자가 정의한 문자를 기준으로 텍스트를 분할합니다. 더 간단한 방법 중 하나입니다.                                                        |

### 생성형 UI

LangChain.js는 몇 가지 템플릿과 예제를 제공하면서 생성형 UI를 보여주고,
특히 React나 Next.js에서 서버에서 클라이언트로 데이터 스트리밍을 하는 여러 방법 또한 제시합니다.

생성형 UI 템플릿은 [LangChain js Next.js 템플릿](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/generative_ui/README.md)에서
찾을 수 있습니다.

스트리밍 에이전트 응답과 중급 단계의 템플릿과 문서는 
[여기서](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/agent/README.md)
찾을 수 있습니다.

마지막으로, 스트리밍 도구 호출과 정형화된 출력에 관해서는
[여기서](https://github.com/langchain-ai/langchain-nextjs-template/blob/main/app/ai_sdk/tools/README.md)
확인할 수 있습니다.
