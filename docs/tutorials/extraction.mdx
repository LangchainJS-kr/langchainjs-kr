---
sidebar_position: 4
title: ì¶”ì¶œ ì²´ì¸ ë§Œë“¤ê¸°
sidebar_class_name: hidden
pagination_prev: null
pagination_next: null
---

ì´ íŠœí† ë¦¬ì–¼ ì—ì„œëŠ” ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ ì •í˜•í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì²´ì¸ì„ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤.

:::ì£¼ì˜!

ì´ íŠœí† ë¦¬ì–¼ì€ **í•¨ìˆ˜/ë„êµ¬ í˜¸ì¶œ ( Function/tool calling )** ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ì—ì„œë§Œ ì‘ë™í•©ë‹ˆë‹¤.

:::

## Concepts

ìš°ë¦¬ê°€ ë‹¤ë£° ê°œë…ë“¤: 
- [languagemodels](../../docs/concepts/#chat-models) ì‚¬ìš©í•˜ê¸°
- [function/toolcalling](../../docs/concepts/#function-tool-calling) ì‚¬ìš©í•˜ê¸°
- [LangSmith](../../docs/concepts/#langsmith)ë¥¼ ì‚¬ìš©í•´ ì–´í”Œë¦¬ì¼€ì´ì…˜ ë””ë²„ê¹… ë° ì¶”ì í•˜ê¸° 

## Setup

### Installation

LangChainì„ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:

```mdx-code-block
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  langchain
</Npm2Yarn>
```

For more details, see our [Installation
guide](../../docs/how_to/installation/).

### LangSmith


ë­ì²´ì¸ì„ ì´ìš©í•œ ìˆ˜ë§ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ë“¤ì€ ì—¬ëŸ¬ ë‹¨ê³„ì—ì„œ LLM í˜¸ì¶œì„ ì—¬ëŸ¬ë²ˆ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤.
ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ë³µì¡í•˜ë©´ ë³µì¡í•´ì§ˆìˆ˜ë¡, ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì²´ì¸ì´ë‚˜ ì—ì´ì „íŠ¸ ì•ˆì—ì„œ ë¬´ì—‡ì´ ì´ë£¨ì–´ì§€ê³  ìˆëŠ”ì§€ ê²€í† í•˜ëŠ”ê²ƒì€ ì¤‘ìš”í•´ì§‘ë‹ˆë‹¤.
ìµœê³ ì˜ ë°©ë²•ì€ [LangSmith](https://smith.langchain.com)ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ìœ„ì˜ ë§í¬ì— ê°€ì…í•˜ê³ , ë°˜ë“œì‹œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì„œ ë¡œê·¸ ì¶”ì ì„ ì‹œì‘í•˜ì„¸ìš”.

ë¡œê·¸ ì¶”ì ì„ ì‹œì‘í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ë“¤:

```shell
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

## The Schema

ë¨¼ì €, ìš°ë¦¬ê°€ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•˜ê³ ì í•˜ëŠ” ì •ë³´ê°€ ë¬´ì—‡ì¸ì§€ ì •í™•íˆ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.

[Zod](https://zod.dev)ì„ ì´ìš©í•´ì„œ ê°œì¸ì •ë³´ ì¶”ì¶œí•˜ëŠ” ì˜ˆì œ ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•  ê²ƒì…ë‹ˆë‹¤.

```mdx-code-block
<Npm2Yarn>
  zod @langchain/core
</Npm2Yarn>
```

```typescript
import { z } from "zod";

const personSchema = z.object({
  name: z.string().nullish().describe("The name of the person"),
  hair_color: z
    .string()
    .nullish()
    .describe("The color of the person's hair if known"),
  height_in_meters: z.string().nullish().describe("Height measured in meters"),
});
```

ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í• ë•Œ ë‘ê°€ì§€ ëª¨ë²” ì‚¬ë¡€ :

1. **ì†ì„±**ê³¼ **ìŠ¤í‚¤ë§ˆ** ìì²´ë¥¼ ë¬¸ì„œí™”í•˜ì„¸ìš” : ì´ ì •ë³´ëŠ” LLMì— ì „ë‹¬ë˜ì–´ ì •ë³´ ì¶”ì¶œì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
2.  LLMì´ ì •ë³´ë¥¼ ê¾¸ë©°ë‚´ê²Œ í•˜ì§€ ë§ˆì„¸ìš” : ìœ„ì—ì„œëŠ” LLMì´ ë‹µì„ ëª¨ë¥¼ê²½ìš° `.nullish()`ë¥¼ ì´ìš©í•˜ì—¬ LLMì—ê²Œ ê²°ê³¼ê°’ìœ¼ë¡œ `null` ë˜ëŠ” `undefined`ë¥¼ ì¶œë ¥í•˜ë„ë¡ í—ˆìš©í–ˆìŠµë‹ˆë‹¤.

:::ì¤‘ìš”

ì¢‹ì€ ì„±ëŠ¥ì„ ìœ„í•´ì„œ, ìŠ¤í‚¤ë§ˆë¥¼ ì˜ ë¬¸ì„œí™”í•˜ì„¸ìš”. 
í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œë  ì •ë³´ê°€ ì—†ì„ê²½ìš°, ëª¨ë¸ì—ê²Œ ê²°ê³¼ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ê°•ì œí•˜ì§€ ë§ˆì„¸ìš”.

:::

## The Extractor

ìš°ë¦¬ê°€ ìœ„ì— ì •ì˜í•œ ìŠ¤í‚¤ë§ˆë¥¼ ì´ìš©í•˜ì—¬ ì •ë³´ ì¶”ì¶œê¸°ë¥¼ ìƒì„±í•´ ë´…ì‹œë‹¤!

```typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

//LLMì—ê²Œ ì§€ì‹œì‚¬í•­ê³¼ ì¶”ê°€ì ì¸ ë§¥ë½ì„ ì œê³µí•˜ëŠ” ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•˜ì„¸ìš”
// 1) ì¶”ì¶œ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì˜ˆì‹œë¥¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
// 2) ì¶”ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ë„ì…í•˜ì—¬ ë§¥ë½ì„ ê³ ë ¤í•©ë‹ˆë‹¤.(ì˜ˆì‹œ, í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬)

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    `You are an expert extraction algorithm.
Only extract relevant information from the text.
If you do not know the value of an attribute asked to extract,
return null for the attribute's value.`,
  ],
  // Please see the how-to about improving performance with
  // reference examples.
  // ["placeholder", "{examples}"],
  ["human", "{text}"],
]);
```

We need to use a model that supports function/tool calling.

Please review [the
documentation](../../docs/concepts#function-tool-calling) for list of
some models that can be used with this API.

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const llm = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  temperature: 0,
});

const runnable = prompt.pipe(llm.withStructuredOutput(personSchema));

const text = "Alan Smith is 6 feet tall and has blond hair.";
await runnable.invoke({ text });
```

```text
{ name: "Alan Smith", hair_color: "blond", height_in_meters: "1.83" }
```

:::important

Extraction is Generative ğŸ¤¯

LLMs are generative models, so they can do some pretty cool things like
correctly extract the height of the person in meters even though it was
provided in feet!

:::

We can see the LangSmith trace
[here](https://smith.langchain.com/public/3d44b7e8-e7ca-4e02-951d-3290ccc89d64/r).

Even though we defined our schema with the variable name `personSchema`,
Zod is unable to infer this name and therefore does not pass it along to
the model. To help give the LLM more clues as to what your provided
schema represents, you can also give the schema you pass to
`withStructuredOutput()` a name:

```typescript
const runnable = prompt.pipe(
  llm.withStructuredOutput(personSchema, { name: "person" })
);

const text = "Alan Smith is 6 feet tall and has blond hair.";

await runnable.invoke({ text });
```

```text
{ name: "Alan Smith", hair_color: "blond", height_in_meters: "1.83" }
```

This can improve performance in many cases.

## Multiple Entities

In **most cases**, you should be extracting a list of entities rather
than a single entity.

This can be easily achieved using Zod by nesting models inside one
another.

```typescript
import { z } from "zod";

const personSchema = z.object({
  name: z.string().nullish().describe("The name of the person"),
  hair_color: z
    .string()
    .nullish()
    .describe("The color of the person's hair if known"),
  height_in_meters: z.number().nullish().describe("Height measured in meters"),
});

const dataSchema = z.object({
  people: z.array(personSchema).describe("Extracted data about people"),
});
```

:::important

Extraction might not be perfect here. Please continue to see how to use
**Reference Examples** to improve the quality of extraction, and see the
**guidelines** section!

:::

```typescript
const runnable = prompt.pipe(llm.withStructuredOutput(dataSchema));
const text =
  "My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.";
await runnable.invoke({ text });
```

```text
{
  people: [
    { name: "Jeff", hair_color: "black", height_in_meters: 1.83 },
    { name: "Anna", hair_color: "black", height_in_meters: null }
  ]
}
```

:::tip

When the schema accommodates the extraction of **multiple entities**, it
also allows the model to extract **no entities** if no relevant
information is in the text by providing an empty list.

This is usually a **good** thing! It allows specifying **required**
attributes on an entity without necessarily forcing the model to detect
this entity.

:::

We can see the LangSmith trace
[here](https://smith.langchain.com/public/272096ab-9ac5-43f9-aa00-3b8443477d17/r)

## Next steps

Now that you understand the basics of extraction with LangChain, youâ€™re
ready to proceed to the rest of the how-to guides:

- [Add Examples](../../docs/how_to/extraction_examples): Learn how to
  use **reference examples** to improve performance.
- [Handle Long Text](../../docs/how_to/extraction_long_text): What
  should you do if the text does not fit into the context window of
  the LLM?
- [Use a Parsing Approach](../../docs/how_to/extraction_parse): Use a
  prompt based approach to extract with models that do not support
  **tool/function calling**.
