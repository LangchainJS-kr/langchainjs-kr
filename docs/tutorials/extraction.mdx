---
sidebar_position: 4
title: 추출 체인 만들기
sidebar_class_name: hidden
pagination_prev: null
pagination_next: null
---

이 튜토리얼 에서는 비정형 텍스트에서 정형화된 정보를 추출하는 체인을 구축할 것입니다.

:::주의!

이 튜토리얼은 **함수/도구 호출 ( Function/tool calling )** 을 지원하는 모델에서만 작동합니다.

:::

## Concepts

우리가 다룰 개념들: 
- [languagemodels](../../docs/concepts/#chat-models) 사용하기
- [function/toolcalling](../../docs/concepts/#function-tool-calling) 사용하기
- [LangSmith](../../docs/concepts/#langsmith)를 사용해 어플리케이션 디버깅 및 추적하기 

## Setup

### Installation

LangChain을 설치하려면 다음 명령을 실행하세요:

```mdx-code-block
import Npm2Yarn from '@theme/Npm2Yarn';

<Npm2Yarn>
  langchain
</Npm2Yarn>
```

For more details, see our [Installation
guide](../../docs/how_to/installation/).

### LangSmith


랭체인을 이용한 수많은 애플리케이션들은 여러 단계에서 LLM 호출을 여러번 수행할 것입니다.
애플리케이션이 복잡하면 복잡해질수록, 애플리케이션의 체인이나 에이전트 안에서 무엇이 이루어지고 있는지 검토하는것은 중요해집니다.
최고의 방법은 [LangSmith](https://smith.langchain.com)를 이용하는 것입니다.

위의 링크에 가입하고, 반드시 환경변수를 설정해서 로그 추적을 시작하세요.

로그 추적을 시작하기 위한 변수들:

```shell
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

## The Schema

먼저, 우리가 텍스트에서 추출하고자 하는 정보가 무엇인지 정확히 설명해야 합니다.

[Zod](https://zod.dev)을 이용해서 개인정보 추출하는 예제 스키마를 정의할 것입니다.

```mdx-code-block
<Npm2Yarn>
  zod @langchain/core
</Npm2Yarn>
```

```typescript
import { z } from "zod";

const personSchema = z.object({
  name: z.string().nullish().describe("The name of the person"),
  hair_color: z
    .string()
    .nullish()
    .describe("The color of the person's hair if known"),
  height_in_meters: z.string().nullish().describe("Height measured in meters"),
});
```

스키마를 정의할때 두가지 모범 사례 :

1. **속성**과 **스키마** 자체를 문서화하세요 : 이 정보는 LLM에 전달되어 정보 추출의 품질을 향상시키는데 사용됩니다.
2.  LLM이 정보를 꾸며내게 하지 마세요 : 위에서는 LLM이 답을 모를경우 `.nullish()`를 이용하여 LLM에게 결과값으로 `null` 또는 `undefined`를 출력하도록 허용했습니다.

:::중요

좋은 성능을 위해서, 스키마를 잘 문서화하세요. 
텍스트에서 추출될 정보가 없을경우, 모델에게 결과값을 반환하도록 강제하지 마세요.

:::

## The Extractor

우리가 위에 정의한 스키마를 이용하여 정보 추출기를 생성해 봅시다!

```typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

//LLM에게 지시사항과 추가적인 맥락을 제공하는 사용자 정의 프롬프트를 정의하세요
// 1) 추출 품질을 향상시키기 위해 예시를 프롬프트 템플릿에 추가할 수 있습니다.
// 2) 추가 매개변수를 도입하여 맥락을 고려합니다.(예시, 텍스트에서 추출된 문서의 메타데이터를 포함하여)

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    `You are an expert extraction algorithm.
Only extract relevant information from the text.
If you do not know the value of an attribute asked to extract,
return null for the attribute's value.`,
  ],
  // Please see the how-to about improving performance with
  // reference examples.
  // ["placeholder", "{examples}"],
  ["human", "{text}"],
]);
```

We need to use a model that supports function/tool calling.

Please review [the
documentation](../../docs/concepts#function-tool-calling) for list of
some models that can be used with this API.

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const llm = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
  temperature: 0,
});

const runnable = prompt.pipe(llm.withStructuredOutput(personSchema));

const text = "Alan Smith is 6 feet tall and has blond hair.";
await runnable.invoke({ text });
```

```text
{ name: "Alan Smith", hair_color: "blond", height_in_meters: "1.83" }
```

:::important

Extraction is Generative 🤯

LLMs are generative models, so they can do some pretty cool things like
correctly extract the height of the person in meters even though it was
provided in feet!

:::

We can see the LangSmith trace
[here](https://smith.langchain.com/public/3d44b7e8-e7ca-4e02-951d-3290ccc89d64/r).

Even though we defined our schema with the variable name `personSchema`,
Zod is unable to infer this name and therefore does not pass it along to
the model. To help give the LLM more clues as to what your provided
schema represents, you can also give the schema you pass to
`withStructuredOutput()` a name:

```typescript
const runnable = prompt.pipe(
  llm.withStructuredOutput(personSchema, { name: "person" })
);

const text = "Alan Smith is 6 feet tall and has blond hair.";

await runnable.invoke({ text });
```

```text
{ name: "Alan Smith", hair_color: "blond", height_in_meters: "1.83" }
```

This can improve performance in many cases.

## Multiple Entities

In **most cases**, you should be extracting a list of entities rather
than a single entity.

This can be easily achieved using Zod by nesting models inside one
another.

```typescript
import { z } from "zod";

const personSchema = z.object({
  name: z.string().nullish().describe("The name of the person"),
  hair_color: z
    .string()
    .nullish()
    .describe("The color of the person's hair if known"),
  height_in_meters: z.number().nullish().describe("Height measured in meters"),
});

const dataSchema = z.object({
  people: z.array(personSchema).describe("Extracted data about people"),
});
```

:::important

Extraction might not be perfect here. Please continue to see how to use
**Reference Examples** to improve the quality of extraction, and see the
**guidelines** section!

:::

```typescript
const runnable = prompt.pipe(llm.withStructuredOutput(dataSchema));
const text =
  "My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.";
await runnable.invoke({ text });
```

```text
{
  people: [
    { name: "Jeff", hair_color: "black", height_in_meters: 1.83 },
    { name: "Anna", hair_color: "black", height_in_meters: null }
  ]
}
```

:::tip

When the schema accommodates the extraction of **multiple entities**, it
also allows the model to extract **no entities** if no relevant
information is in the text by providing an empty list.

This is usually a **good** thing! It allows specifying **required**
attributes on an entity without necessarily forcing the model to detect
this entity.

:::

We can see the LangSmith trace
[here](https://smith.langchain.com/public/272096ab-9ac5-43f9-aa00-3b8443477d17/r)

## Next steps

Now that you understand the basics of extraction with LangChain, you’re
ready to proceed to the rest of the how-to guides:

- [Add Examples](../../docs/how_to/extraction_examples): Learn how to
  use **reference examples** to improve performance.
- [Handle Long Text](../../docs/how_to/extraction_long_text): What
  should you do if the text does not fit into the context window of
  the LLM?
- [Use a Parsing Approach](../../docs/how_to/extraction_parse): Use a
  prompt based approach to extract with models that do not support
  **tool/function calling**.
