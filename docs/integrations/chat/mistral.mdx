---
sidebar_label: Mistral AI
---

import CodeBlock from "@theme/CodeBlock";

# ChatMistralAI

[Mistral AI](https://mistral.ai/) is a research organization and hosting platform for LLMs.
They're most known for their family of 7B models ([`mistral7b` // `mistral-tiny`](https://mistral.ai/news/announcing-mistral-7b/), [`mixtral8x7b` // `mistral-small`](https://mistral.ai/news/mixtral-of-experts/)).

The LangChain implementation of Mistral's models uses their hosted generation API, making it easier to access their models without needing to run them locally.

## Models

Mistral's API offers access to two of their open source, and proprietary models:

- `open-mistral-7b` (aka `mistral-tiny-2312`)
- `open-mixtral-8x7b` (aka `mistral-small-2312`)
- `mistral-small-latest` (aka `mistral-small-2402`) (default)
- `mistral-medium-latest` (aka `mistral-medium-2312`)
- `mistral-large-latest` (aka `mistral-large-2402`)

See [this page](https://docs.mistral.ai/guides/model-selection/) for an up to date list.

## Setup

In order to use the Mistral API you'll need an API key. You can sign up for a Mistral account and create an API key [here](https://console.mistral.ai/).

You'll first need to install the [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/mistralai
```

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

## Usage

When sending chat messages to mistral, there are a few requirements to follow:

- The first message can _*not*_ be an assistant (ai) message.
- Messages _*must*_ alternate between user and assistant (ai) messages.
- Messages can _*not*_ end with an assistant (ai) or system message.

import ChatMistralAIExample from "@examples/models/chat/chat_mistralai.ts";

<CodeBlock language="typescript">{ChatMistralAIExample}</CodeBlock>

:::info
You can see a LangSmith trace of this example [here](https://smith.langchain.com/public/d69d0db9-f29e-45aa-a40d-b53f6273d7d0/r)
:::

### Streaming

Mistral's API also supports streaming token responses. The example below demonstrates how to use this feature.

import ChatStreamMistralAIExample from "@examples/models/chat/chat_stream_mistralai.ts";

<CodeBlock language="typescript">{ChatStreamMistralAIExample}</CodeBlock>

:::info
You can see a LangSmith trace of this example [here](https://smith.langchain.com/public/061d90f2-ac7e-44c5-8790-8b23299f9217/r)
:::

### Tool calling

Mistral's API now supports tool calling and JSON mode!
The examples below demonstrates how to use them, along with how to use the `withStructuredOutput` method to easily compose structured output LLM calls.

import ToolCalling from "@examples/models/chat/chat_mistralai_tools.ts";

<CodeBlock language="typescript">{ToolCalling}</CodeBlock>

### `.withStructuredOutput({ ... })`

:::info
The `.withStructuredOutput` method is in beta. It is actively being worked on, so the API may change.
:::

Using the `.withStructuredOutput` method, you can easily make the LLM return structured output, given only a Zod or JSON schema:

:::note
The Mistral tool calling API requires descriptions for each tool field. If descriptions are not supplied, the API will error.
:::

import WSAExample from "@examples/models/chat/chat_mistralai_wsa.ts";

<CodeBlock language="typescript">{WSAExample}</CodeBlock>

### Using JSON schema:

import WSAJSONExample from "@examples/models/chat/chat_mistralai_wsa_json.ts";

<CodeBlock language="typescript">{WSAJSONExample}</CodeBlock>

### Tool calling agent

The larger Mistral models not only support tool calling, but can also be used in the Tool Calling agent.
Here's an example:

import AgentsExample from "@examples/models/chat/chat_mistralai_agents.ts";

<CodeBlock language="typescript">{AgentsExample}</CodeBlock>
