# Pinecone

You can use [Pinecone](https://www.pinecone.io/) vectorstores with LangChain.
To get started, install the integration package and the official Pinecone SDK with:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install -S @langchain/pinecone @pinecone-database/pinecone
```

The below examples use OpenAI embeddings, but you can swap in whichever provider you'd like.
Keep in mind different embeddings models may have a different number of dimensions:

```bash npm2yarn
npm install -S @langchain/openai
```

## Index docs

import CodeBlock from "@theme/CodeBlock";
import IndexExample from "@examples/indexes/vector_stores/pinecone/index_docs.ts";

<CodeBlock language="typescript">{IndexExample}</CodeBlock>

## Query docs

import QueryExample from "@examples/indexes/vector_stores/pinecone/query_docs.ts";

<CodeBlock language="typescript">{QueryExample}</CodeBlock>

## Delete docs

import DeleteExample from "@examples/indexes/vector_stores/pinecone/delete_docs.ts";

<CodeBlock language="typescript">{DeleteExample}</CodeBlock>

## Maximal marginal relevance search

Pinecone supports maximal marginal relevance search, which takes a combination of documents
that are most similar to the inputs, then reranks and optimizes for diversity.

import MMRExample from "@examples/indexes/vector_stores/pinecone/mmr.ts";

<CodeBlock language="typescript">{MMRExample}</CodeBlock>
