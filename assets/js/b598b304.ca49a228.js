(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[950],{95723:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>m,default:()=>x,frontMatter:()=>d,metadata:()=>p,toc:()=>g});var s=t(74848),a=t(28453),o=t(64428),r=t(73611),i=t.n(r),l=t(78847),c=t(91436),h=t.n(c);const d={sidebar_class_name:"hidden",sidebar_position:1},m="How to stream responses from an LLM",p={id:"how_to/streaming_llm",title:"How to stream responses from an LLM",description:"All LLMs implement the Runnable interface, which comes with default implementations of standard runnable methods (i.e. ainvoke, batch, abatch, stream, astream, astream_events).",source:"@site/docs/how_to/streaming_llm.mdx",sourceDirName:"how_to",slug:"/how_to/streaming_llm",permalink:"/docs/how_to/streaming_llm",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/streaming_llm.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_class_name:"hidden",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Installation",permalink:"/docs/how_to/installation"},next:{title:"How to stream chat model responses",permalink:"/docs/how_to/chat_streaming"}},u={},g=[{value:"Using <code>.stream()</code>",id:"using-stream",level:2},...l.toc,{value:"Using a callback handler",id:"using-a-callback-handler",level:2}];function k(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"how-to-stream-responses-from-an-llm",children:"How to stream responses from an LLM"}),"\n",(0,s.jsxs)(n.p,{children:["All ",(0,s.jsxs)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_language_models_llms.BaseLLM.html",children:[(0,s.jsx)(n.code,{children:"LLM"}),"s"]})," implement the ",(0,s.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html",children:"Runnable interface"}),", which comes with ",(0,s.jsx)(n.strong,{children:"default"})," implementations of standard runnable methods (i.e. ",(0,s.jsx)(n.code,{children:"ainvoke"}),", ",(0,s.jsx)(n.code,{children:"batch"}),", ",(0,s.jsx)(n.code,{children:"abatch"}),", ",(0,s.jsx)(n.code,{children:"stream"}),", ",(0,s.jsx)(n.code,{children:"astream"}),", ",(0,s.jsx)(n.code,{children:"astream_events"}),")."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"default"})," streaming implementations provide an ",(0,s.jsx)(n.code,{children:"AsyncGenerator"})," that yields a single value: the final output from the underlying chat model provider."]}),"\n",(0,s.jsx)(n.p,{children:"The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support."}),"\n",(0,s.jsxs)(n.p,{children:["See which ",(0,s.jsx)(n.a,{href:"/docs/integrations/llms/",children:"integrations support token-by-token streaming here"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"default"})," implementation does ",(0,s.jsx)(n.strong,{children:"not"})," provide support for token-by-token streaming, but it ensures that the model can be swapped in for any other model as it supports the same standard interface."]}),"\n",(0,s.jsxs)(n.h2,{id:"using-stream",children:["Using ",(0,s.jsx)(n.code,{children:".stream()"})]}),"\n","\n",(0,s.jsxs)(n.p,{children:["The easiest way to stream is to use the ",(0,s.jsx)(n.code,{children:".stream()"})," method. This returns an readable stream that you can also iterate over:"]}),"\n","\n","\n",(0,s.jsx)(l.default,{}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai\n"})}),"\n",(0,s.jsx)(o.A,{language:"typescript",children:i()}),"\n",(0,s.jsx)(n.p,{children:"For models that do not support streaming, the entire response will be returned as a single chunk."}),"\n",(0,s.jsx)(n.h2,{id:"using-a-callback-handler",children:"Using a callback handler"}),"\n",(0,s.jsxs)(n.p,{children:["You can also use a ",(0,s.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_callbacks_base.BaseCallbackHandler.html",children:(0,s.jsx)(n.code,{children:"CallbackHandler"})})," like so:"]}),"\n","\n",(0,s.jsx)(o.A,{language:"typescript",children:h()}),"\n",(0,s.jsxs)(n.p,{children:["We still have access to the end ",(0,s.jsx)(n.code,{children:"LLMResult"})," if using ",(0,s.jsx)(n.code,{children:"generate"}),". However, ",(0,s.jsx)(n.code,{children:"tokenUsage"})," may not be currently supported for all model providers when streaming."]})]})}function x(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(k,{...e})}):k(e)}},91436:e=>{e.exports={content:"import { OpenAI } from \"@langchain/openai\";\n\n// To enable streaming, we pass in `streaming: true` to the LLM constructor.\n// Additionally, we pass in a handler for the `handleLLMNewToken` event.\nconst model = new OpenAI({\n  maxTokens: 25,\n  streaming: true,\n});\n\nconst response = await model.invoke(\"Tell me a joke.\", {\n  callbacks: [\n    {\n      handleLLMNewToken(token: string) {\n        console.log({ token });\n      },\n    },\n  ],\n});\nconsole.log(response);\n/*\n{ token: '\\n' }\n{ token: '\\n' }\n{ token: 'Q' }\n{ token: ':' }\n{ token: ' Why' }\n{ token: ' did' }\n{ token: ' the' }\n{ token: ' chicken' }\n{ token: ' cross' }\n{ token: ' the' }\n{ token: ' playground' }\n{ token: '?' }\n{ token: '\\n' }\n{ token: 'A' }\n{ token: ':' }\n{ token: ' To' }\n{ token: ' get' }\n{ token: ' to' }\n{ token: ' the' }\n{ token: ' other' }\n{ token: ' slide' }\n{ token: '.' }\n\n\nQ: Why did the chicken cross the playground?\nA: To get to the other slide.\n*/\n",imports:[{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"}]}},73611:e=>{e.exports={content:'import { OpenAI } from "@langchain/openai";\n\nconst model = new OpenAI({\n  maxTokens: 25,\n});\n\nconst stream = await model.stream("Tell me a joke.");\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\n\n\nQ\n:\n What\n did\n the\n fish\n say\n when\n it\n hit\n the\n wall\n?\n\n\nA\n:\n Dam\n!\n*/\n',imports:[{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"}]}}}]);