(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9731],{69606:(e,n,i)=>{"use strict";i.r(n),i.d(n,{assets:()=>m,contentTitle:()=>c,default:()=>g,frontMatter:()=>d,metadata:()=>h,toc:()=>u});var t=i(74848),s=i(28453),l=i(78847),r=i(64428),o=i(76237),a=i.n(o);const d={},c="Friendli",h={id:"integrations/llms/friendli",title:"Friendli",description:"Friendli enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.",source:"@site/docs/integrations/llms/friendli.mdx",sourceDirName:"integrations/llms",slug:"/integrations/llms/friendli",permalink:"/docs/integrations/llms/friendli",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/llms/friendli.mdx",tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Fireworks",permalink:"/docs/integrations/llms/fireworks"},next:{title:"(Legacy) Google PaLM/VertexAI",permalink:"/docs/integrations/llms/google_palm"}},m={},u=[{value:"Setup",id:"setup",level:2},...l.toc,{value:"Usage",id:"usage",level:2}];function p(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"friendli",children:"Friendli"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://friendli.ai/",children:"Friendli"})," enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This tutorial guides you through integrating ",(0,t.jsx)(n.code,{children:"Friendli"})," with LangChain."]}),"\n",(0,t.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsxs)(n.p,{children:["Ensure the ",(0,t.jsx)(n.code,{children:"@langchain/community"})," is installed."]}),"\n","\n",(0,t.jsx)(l.default,{}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/community\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Sign in to ",(0,t.jsx)(n.a,{href:"https://suite.friendli.ai/",children:"Friendli Suite"})," to create a Personal Access Token, and set it as the ",(0,t.jsx)(n.code,{children:"FRIENDLI_TOKEN"})," environment.\nYou can set team id as ",(0,t.jsx)(n.code,{children:"FRIENDLI_TEAM"})," environment."]}),"\n",(0,t.jsxs)(n.p,{children:["You can initialize a Friendli chat model with selecting the model you want to use. The default model is ",(0,t.jsx)(n.code,{children:"mixtral-8x7b-instruct-v0-1"}),". You can check the available models at ",(0,t.jsx)(n.a,{href:"https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models",children:"docs.friendli.ai"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n","\n",(0,t.jsx)(r.A,{language:"typescript",children:a()})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},76237:e=>{e.exports={content:'import { Friendli } from "@langchain/community/llms/friendli";\n\nconst model = new Friendli({\n  model: "mixtral-8x7b-instruct-v0-1", // Default value\n  friendliToken: process.env.FRIENDLI_TOKEN,\n  friendliTeam: process.env.FRIENDLI_TEAM,\n  maxTokens: 18,\n  temperature: 0.75,\n  topP: 0.25,\n  frequencyPenalty: 0,\n  stop: [],\n});\n\nconst response = await model.invoke(\n  "Check the Grammar: She dont like to eat vegetables, but she loves fruits."\n);\n\nconsole.log(response);\n\n/*\nCorrect: She doesn\'t like to eat vegetables, but she loves fruits\n*/\n\nconst stream = await model.stream(\n  "Check the Grammar: She dont like to eat vegetables, but she loves fruits."\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\nCor\nrect\n:\n She\n doesn\n...\nshe\n loves\n fruits\n*/\n',imports:[{local:"Friendli",imported:"Friendli",source:"@langchain/community/llms/friendli"}]}}}]);