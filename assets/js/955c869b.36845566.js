(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7606],{45977:(e,n,s)=>{"use strict";s.r(n),s.d(n,{assets:()=>h,contentTitle:()=>r,default:()=>p,frontMatter:()=>l,metadata:()=>d,toc:()=>m});var o=s(74848),t=s(28453),i=s(64428),a=s(96661),c=s.n(a);const l={sidebar_class_name:"hidden",sidebar_position:3},r="How to cache chat model responses",d={id:"how_to/chat_model_caching",title:"How to cache chat model responses",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/chat_model_caching.mdx",sourceDirName:"how_to",slug:"/how_to/chat_model_caching",permalink:"/docs/how_to/chat_model_caching",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/chat_model_caching.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_class_name:"hidden",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"How to cache model responses",permalink:"/docs/how_to/llm_caching"},next:{title:"How to create a custom LLM class",permalink:"/docs/how_to/custom_llm"}},h={},m=[{value:"In Memory Cache",id:"in-memory-cache",level:2},{value:"Caching with Redis",id:"caching-with-redis",level:2},{value:"Caching on the File System",id:"caching-on-the-file-system",level:2},{value:"Next steps",id:"next-steps",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"how-to-cache-chat-model-responses",children:"How to cache chat model responses"}),"\n",(0,o.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,o.jsx)(n.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"/docs/concepts/#chat-models",children:"Chat models"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"/docs/concepts/#llms",children:"LLMs"})}),"\n"]})]}),"\n",(0,o.jsx)(n.p,{children:"LangChain provides an optional caching layer for chat models. This is useful for two reasons:"}),"\n",(0,o.jsx)(n.p,{children:"It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\nIt can speed up your application by reducing the number of API calls you make to the LLM provider."}),"\n","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:'import { ChatOpenAI } from "@langchain/openai";\n\n// To make the caching really obvious, lets use a slower model.\nconst model = new ChatOpenAI({\n  model: "gpt-4",\n  cache: true,\n});\n'})}),"\n",(0,o.jsx)(n.h2,{id:"in-memory-cache",children:"In Memory Cache"}),"\n",(0,o.jsx)(n.p,{children:"The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"console.time();\n\n// The first time, it is not yet in cache, so it should take longer\nconst res = await model.invoke(\"Tell me a joke!\");\nconsole.log(res);\n\nconsole.timeEnd();\n\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n      additional_kwargs: { function_call: undefined, tool_calls: undefined }\n    },\n    lc_namespace: [ 'langchain_core', 'messages' ],\n    content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  }\n  default: 2.224s\n*/\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"console.time();\n\n// The second time it is, so it goes faster\nconst res2 = await model.invoke(\"Tell me a joke!\");\nconsole.log(res2);\n\nconsole.timeEnd();\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n      additional_kwargs: { function_call: undefined, tool_calls: undefined }\n    },\n    lc_namespace: [ 'langchain_core', 'messages' ],\n    content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  }\n  default: 181.98ms\n*/\n"})}),"\n",(0,o.jsx)(n.h2,{id:"caching-with-redis",children:"Caching with Redis"}),"\n",(0,o.jsxs)(n.p,{children:["LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers.\nTo use it, you'll need to install the ",(0,o.jsx)(n.code,{children:"redis"})," package:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install ioredis @langchain/community\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Then, you can pass a ",(0,o.jsx)(n.code,{children:"cache"})," option when you instantiate the LLM. For example:"]}),"\n","\n",(0,o.jsx)(i.A,{language:"typescript",children:c()}),"\n",(0,o.jsx)(n.h2,{id:"caching-on-the-file-system",children:"Caching on the File System"}),"\n",(0,o.jsx)(n.admonition,{type:"warning",children:(0,o.jsx)(n.p,{children:"This cache is not recommended for production use. It is only intended for local development."})}),"\n",(0,o.jsx)(n.p,{children:"LangChain provides a simple file system cache.\nBy default the cache is stored a temporary directory, but you can specify a custom directory if you want."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"const cache = await LocalFileCache.create();\n"})}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,o.jsx)(n.p,{children:"You've now learned how to cache model responses to save time and money."}),"\n",(0,o.jsxs)(n.p,{children:["Next, check out the other how-to guides on chat models, like ",(0,o.jsx)(n.a,{href:"/docs/how_to/structured_output",children:"how to get a model to return structured output"})," or ",(0,o.jsx)(n.a,{href:"/docs/how_to/custom_chat",children:"how to create your own custom chat model"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},96661:e=>{e.exports={content:'import { ChatOpenAI } from "@langchain/openai";\nimport { Redis } from "ioredis";\nimport { RedisCache } from "@langchain/community/caches/ioredis";\n\nconst client = new Redis("redis://localhost:6379");\n\nconst cache = new RedisCache(client, {\n  ttl: 60, // Optional key expiration value\n});\n\nconst model = new ChatOpenAI({ cache });\n\nconst response1 = await model.invoke("Do something random!");\nconsole.log(response1);\n/*\n  AIMessage {\n    content: "Sure! I\'ll generate a random number for you: 37",\n    additional_kwargs: {}\n  }\n*/\n\nconst response2 = await model.invoke("Do something random!");\nconsole.log(response2);\n/*\n  AIMessage {\n    content: "Sure! I\'ll generate a random number for you: 37",\n    additional_kwargs: {}\n  }\n*/\n\nawait client.disconnect();\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"RedisCache",imported:"RedisCache",source:"@langchain/community/caches/ioredis"}]}}}]);