(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4],{57705:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>g,contentTitle:()=>h,default:()=>_,frontMatter:()=>u,metadata:()=>d,toc:()=>f});var a=t(74848),o=t(28453),i=t(64428),s=t(38917),r=t.n(s),l=t(34303),c=t.n(l),m=t(99162),p=t.n(m);const u={sidebar_label:"Ollama Functions"},h="Ollama Functions",d={id:"integrations/chat/ollama_functions",title:"Ollama Functions",description:"LangChain offers an experimental wrapper around open source models run locally via Ollama",source:"@site/docs/integrations/chat/ollama_functions.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/ollama_functions",permalink:"/docs/integrations/chat/ollama_functions",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/ollama_functions.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"Ollama Functions"},sidebar:"integrations",previous:{title:"Ollama",permalink:"/docs/integrations/chat/ollama"},next:{title:"OpenAI",permalink:"/docs/integrations/chat/openai"}},g={},f=[{value:"Setup",id:"setup",level:2},{value:"Initialize model",id:"initialize-model",level:2},{value:"Passing in functions",id:"passing-in-functions",level:2},{value:"Using for extraction",id:"using-for-extraction",level:2},{value:"Customization",id:"customization",level:2}];function x(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"ollama-functions",children:"Ollama Functions"}),"\n",(0,a.jsxs)(e.p,{children:["LangChain offers an experimental wrapper around open source models run locally via ",(0,a.jsx)(e.a,{href:"https://github.com/jmorganca/ollama",children:"Ollama"}),"\nthat gives it the same API as OpenAI Functions."]}),"\n",(0,a.jsxs)(e.p,{children:["Note that more powerful and capable models will perform better with complex schema and/or multiple functions. The examples below\nuse ",(0,a.jsx)(e.a,{href:"https://ollama.ai/library/mistral",children:"Mistral"}),"."]}),"\n",(0,a.jsx)(e.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsxs)(e.p,{children:["Follow ",(0,a.jsx)(e.a,{href:"https://github.com/jmorganca/ollama",children:"these instructions"})," to set up and run a local Ollama instance."]}),"\n",(0,a.jsx)(e.h2,{id:"initialize-model",children:"Initialize model"}),"\n",(0,a.jsxs)(e.p,{children:["You can initialize this wrapper the same way you'd initialize a standard ",(0,a.jsx)(e.code,{children:"ChatOllama"})," instance:"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-typescript",children:'import { OllamaFunctions } from "langchain/experimental/chat_models/ollama_functions";\n\nconst model = new OllamaFunctions({\n  temperature: 0.1,\n  model: "mistral",\n});\n'})}),"\n",(0,a.jsx)(e.h2,{id:"passing-in-functions",children:"Passing in functions"}),"\n",(0,a.jsx)(e.p,{children:"You can now pass in functions the same way as OpenAI:"}),"\n","\n",(0,a.jsx)(i.A,{language:"typescript",children:r()}),"\n",(0,a.jsx)(e.h2,{id:"using-for-extraction",children:"Using for extraction"}),"\n","\n",(0,a.jsx)(i.A,{language:"typescript",children:c()}),"\n",(0,a.jsxs)(e.p,{children:["You can see a LangSmith trace of what this looks like here: ",(0,a.jsx)(e.a,{href:"https://smith.langchain.com/public/31457ea4-71ca-4e29-a1e0-aa80e6828883/r",children:"https://smith.langchain.com/public/31457ea4-71ca-4e29-a1e0-aa80e6828883/r"})]}),"\n",(0,a.jsx)(e.h2,{id:"customization",children:"Customization"}),"\n",(0,a.jsx)(e.p,{children:"Behind the scenes, this uses Ollama's JSON mode to constrain output to JSON, then passes tools schemas as JSON schema into the prompt."}),"\n",(0,a.jsx)(e.p,{children:"Because different models have different strengths, it may be helpful to pass in your own system prompt. Here's an example:"}),"\n","\n",(0,a.jsx)(i.A,{language:"typescript",children:p()})]})}function _(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(x,{...n})}):x(n)}},99162:n=>{n.exports={content:'import { OllamaFunctions } from "@langchain/community/experimental/chat_models/ollama_functions";\nimport { HumanMessage } from "@langchain/core/messages";\n\n// Custom system prompt to format tools. You must encourage the model\n// to wrap output in a JSON object with "tool" and "tool_input" properties.\nconst toolSystemPromptTemplate = `You have access to the following tools:\n\n{tools}\n\nTo use a tool, respond with a JSON object with the following structure:\n{{\n  "tool": <name of the called tool>,\n  "tool_input": <parameters for the tool matching the above JSON schema>\n}}`;\n\nconst model = new OllamaFunctions({\n  temperature: 0.1,\n  model: "mistral",\n  toolSystemPromptTemplate,\n}).bind({\n  functions: [\n    {\n      name: "get_current_weather",\n      description: "Get the current weather in a given location",\n      parameters: {\n        type: "object",\n        properties: {\n          location: {\n            type: "string",\n            description: "The city and state, e.g. San Francisco, CA",\n          },\n          unit: { type: "string", enum: ["celsius", "fahrenheit"] },\n        },\n        required: ["location"],\n      },\n    },\n  ],\n  // You can set the `function_call` arg to force the model to use a function\n  function_call: {\n    name: "get_current_weather",\n  },\n});\n\nconst response = await model.invoke([\n  new HumanMessage({\n    content: "What\'s the weather in Boston?",\n  }),\n]);\n\nconsole.log(response);\n\n/*\n  AIMessage {\n    content: \'\',\n    additional_kwargs: {\n      function_call: {\n        name: \'get_current_weather\',\n        arguments: \'{"location":"Boston, MA","unit":"fahrenheit"}\'\n      }\n    }\n  }\n*/\n',imports:[{local:"OllamaFunctions",imported:"OllamaFunctions",source:"@langchain/community/experimental/chat_models/ollama_functions"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},34303:n=>{n.exports={content:'import { z } from "zod";\nimport { zodToJsonSchema } from "zod-to-json-schema";\n\nimport { OllamaFunctions } from "@langchain/community/experimental/chat_models/ollama_functions";\nimport { PromptTemplate } from "@langchain/core/prompts";\nimport { JsonOutputFunctionsParser } from "@langchain/core/output_parsers/openai_functions";\n\nconst EXTRACTION_TEMPLATE = `Extract and save the relevant entities mentioned in the following passage together with their properties.\n\nPassage:\n{input}\n`;\n\nconst prompt = PromptTemplate.fromTemplate(EXTRACTION_TEMPLATE);\n\n// Use Zod for easier schema declaration\nconst schema = z.object({\n  people: z.array(\n    z.object({\n      name: z.string().describe("The name of a person"),\n      height: z.number().describe("The person\'s height"),\n      hairColor: z.optional(z.string()).describe("The person\'s hair color"),\n    })\n  ),\n});\n\nconst model = new OllamaFunctions({\n  temperature: 0.1,\n  model: "mistral",\n}).bind({\n  functions: [\n    {\n      name: "information_extraction",\n      description: "Extracts the relevant information from the passage.",\n      parameters: {\n        type: "object",\n        properties: zodToJsonSchema(schema),\n      },\n    },\n  ],\n  function_call: {\n    name: "information_extraction",\n  },\n});\n\n// Use a JsonOutputFunctionsParser to get the parsed JSON response directly.\nconst chain = await prompt.pipe(model).pipe(new JsonOutputFunctionsParser());\n\nconst response = await chain.invoke({\n  input:\n    "Alex is 5 feet tall. Claudia is 1 foot taller than Alex and jumps higher than him. Claudia has orange hair and Alex is blonde.",\n});\n\nconsole.log(response);\n\n/*\n  {\n    people: [\n      { name: \'Alex\', height: 5, hairColor: \'blonde\' },\n      { name: \'Claudia\', height: 6, hairColor: \'orange\' }\n    ]\n  }\n*/\n',imports:[{local:"OllamaFunctions",imported:"OllamaFunctions",source:"@langchain/community/experimental/chat_models/ollama_functions"},{local:"PromptTemplate",imported:"PromptTemplate",source:"@langchain/core/prompts"},{local:"JsonOutputFunctionsParser",imported:"JsonOutputFunctionsParser",source:"@langchain/core/output_parsers/openai_functions"}]}},38917:n=>{n.exports={content:'import { OllamaFunctions } from "@langchain/community/experimental/chat_models/ollama_functions";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst model = new OllamaFunctions({\n  temperature: 0.1,\n  model: "mistral",\n}).bind({\n  functions: [\n    {\n      name: "get_current_weather",\n      description: "Get the current weather in a given location",\n      parameters: {\n        type: "object",\n        properties: {\n          location: {\n            type: "string",\n            description: "The city and state, e.g. San Francisco, CA",\n          },\n          unit: { type: "string", enum: ["celsius", "fahrenheit"] },\n        },\n        required: ["location"],\n      },\n    },\n  ],\n  // You can set the `function_call` arg to force the model to use a function\n  function_call: {\n    name: "get_current_weather",\n  },\n});\n\nconst response = await model.invoke([\n  new HumanMessage({\n    content: "What\'s the weather in Boston?",\n  }),\n]);\n\nconsole.log(response);\n\n/*\n  AIMessage {\n    content: \'\',\n    additional_kwargs: {\n      function_call: {\n        name: \'get_current_weather\',\n        arguments: \'{"location":"Boston, MA","unit":"fahrenheit"}\'\n      }\n    }\n  }\n*/\n',imports:[{local:"OllamaFunctions",imported:"OllamaFunctions",source:"@langchain/community/experimental/chat_models/ollama_functions"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}}}]);