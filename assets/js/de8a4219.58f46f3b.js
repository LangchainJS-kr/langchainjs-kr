(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2975,65],{1596:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>p,contentTitle:()=>d,default:()=>u,frontMatter:()=>c,metadata:()=>h,toc:()=>m});var i=t(74848),o=t(28453),a=t(78847),s=t(64428),r=t(76208),l=t.n(r);const c={},d="Friendli",h={id:"integrations/chat/friendli",title:"Friendli",description:"Friendli enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads.",source:"@site/docs/integrations/chat/friendli.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/friendli",permalink:"/docs/integrations/chat/friendli",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/friendli.mdx",tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Fireworks",permalink:"/docs/integrations/chat/fireworks"},next:{title:"Google GenAI",permalink:"/docs/integrations/chat/google_generativeai"}},p={},m=[{value:"Setup",id:"setup",level:2},...a.toc,{value:"Usage",id:"usage",level:2}];function g(n){const e={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"friendli",children:"Friendli"}),"\n",(0,i.jsxs)(e.blockquote,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.a,{href:"https://friendli.ai/",children:"Friendli"})," enhances AI application performance and optimizes cost savings with scalable, efficient deployment options, tailored for high-demand AI workloads."]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["This tutorial guides you through integrating ",(0,i.jsx)(e.code,{children:"ChatFriendli"})," for chat applications using LangChain. ",(0,i.jsx)(e.code,{children:"ChatFriendli"})," offers a flexible approach to generating conversational AI responses, supporting both synchronous and asynchronous calls."]}),"\n",(0,i.jsx)(e.h2,{id:"setup",children:"Setup"}),"\n",(0,i.jsxs)(e.p,{children:["Ensure the ",(0,i.jsx)(e.code,{children:"@langchain/community"})," is installed."]}),"\n","\n",(0,i.jsx)(a.default,{}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/community\n"})}),"\n",(0,i.jsxs)(e.p,{children:["Sign in to ",(0,i.jsx)(e.a,{href:"https://suite.friendli.ai/",children:"Friendli Suite"})," to create a Personal Access Token, and set it as the ",(0,i.jsx)(e.code,{children:"FRIENDLI_TOKEN"})," environment.\nYou can set team id as ",(0,i.jsx)(e.code,{children:"FRIENDLI_TEAM"})," environment."]}),"\n",(0,i.jsxs)(e.p,{children:["You can initialize a Friendli chat model with selecting the model you want to use. The default model is ",(0,i.jsx)(e.code,{children:"llama-2-13b-chat"}),". You can check the available models at ",(0,i.jsx)(e.a,{href:"https://docs.friendli.ai/guides/serverless_endpoints/pricing#text-generation-models",children:"docs.friendli.ai"}),"."]}),"\n",(0,i.jsx)(e.h2,{id:"usage",children:"Usage"}),"\n","\n",(0,i.jsx)(s.A,{language:"typescript",children:l()})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(g,{...n})}):g(n)}},78847:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var i=t(74848),o=t(28453);const a={},s=void 0,r={id:"mdx_components/integration_install_tooltip",title:"integration_install_tooltip",description:"See this section for general instructions on installing integration packages.",source:"@site/docs/mdx_components/integration_install_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/integration_install_tooltip",permalink:"/docs/mdx_components/integration_install_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/integration_install_tooltip.mdx",tags:[],version:"current",frontMatter:{}},l={},c=[];function d(n){const e={a:"a",admonition:"admonition",p:"p",...(0,o.R)(),...n.components};return(0,i.jsx)(e.admonition,{type:"tip",children:(0,i.jsxs)(e.p,{children:["See ",(0,i.jsx)(e.a,{href:"/docs/how_to/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},76208:n=>{n.exports={content:'import { ChatFriendli } from "@langchain/community/chat_models/friendli";\n\nconst model = new ChatFriendli({\n  model: "llama-2-13b-chat", // Default value\n  friendliToken: process.env.FRIENDLI_TOKEN,\n  friendliTeam: process.env.FRIENDLI_TEAM,\n  maxTokens: 800,\n  temperature: 0.9,\n  topP: 0.9,\n  frequencyPenalty: 0,\n  stop: [],\n});\n\nconst response = await model.invoke(\n  "Draft a cover letter for a role in software engineering."\n);\n\nconsole.log(response.content);\n\n/*\nDear [Hiring Manager],\n\nI am excited to apply for the role of Software Engineer at [Company Name]. With my passion for innovation, creativity, and problem-solving, I am confident that I would be a valuable asset to your team.\n\nAs a highly motivated and detail-oriented individual, ...\n*/\n\nconst stream = await model.stream(\n  "Draft a cover letter for a role in software engineering."\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk.content);\n}\n\n/*\nD\near\n [\nH\niring\n...\n[\nYour\n Name\n]\n*/\n',imports:[{local:"ChatFriendli",imported:"ChatFriendli",source:"@langchain/community/chat_models/friendli"}]}}}]);