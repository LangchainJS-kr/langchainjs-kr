"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1829],{27947:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>l,metadata:()=>r,toc:()=>d});var t=a(74848),s=a(28453),o=a(63142);const l={sidebar_class_name:"hidden",title:"How to stream"},i="Using Stream",r={id:"how_to/streaming",title:"How to stream",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/streaming.mdx",sourceDirName:"how_to",slug:"/how_to/streaming",permalink:"/docs/how_to/streaming",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/streaming.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to stream"},sidebar:"tutorialSidebar",previous:{title:"How to do query validation",permalink:"/docs/how_to/sql_query_checking"},next:{title:"How to create a time-weighted retriever",permalink:"/docs/how_to/time_weighted_vectorstore"}},c={},d=[{value:"LLMs and Chat Models",id:"llms-and-chat-models",level:2},{value:"Chains",id:"chains",level:2},{value:"Working with Input Streams",id:"working-with-input-streams",level:3},{value:"Non-streaming components",id:"non-streaming-components",level:3},{value:"Using Stream Events",id:"using-stream-events",level:2},{value:"Event Reference",id:"event-reference",level:3},{value:"Chat Model",id:"chat-model",level:3},{value:"Chain",id:"chain",level:3},{value:"Filtering Events",id:"filtering-events",level:3},{value:"By Name",id:"by-name",level:4},{value:"By type",id:"by-type",level:4},{value:"By Tags",id:"by-tags",level:4},{value:"Streaming events over HTTP",id:"streaming-events-over-http",level:3},{value:"Non-streaming components",id:"non-streaming-components-1",level:3}];function h(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.admonition,{title:"Prerequisites",type:"info",children:[(0,t.jsx)(e.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"../../docs/concepts/#chat-models",children:"Chat models"})}),"\n"]})]}),"\n",(0,t.jsx)(e.p,{children:"Streaming is critical in making applications based on LLMs feel\nresponsive to end-users."}),"\n",(0,t.jsx)(e.p,{children:"Important LangChain primitives like LLMs, parsers, prompts, retrievers,\nand agents implement the LangChain Runnable Interface."}),"\n",(0,t.jsx)(e.p,{children:"This interface provides two general approaches to stream content:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:".stream()"}),": a default implementation of streaming that streams the\nfinal output from the chain."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"streamEvents()"})," and ",(0,t.jsx)(e.code,{children:"streamLog()"}),": these provide a way to stream\nboth intermediate steps and final output from the chain."]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Let\u2019s take a look at both approaches!"}),"\n",(0,t.jsx)(e.h1,{id:"using-stream",children:"Using Stream"}),"\n",(0,t.jsxs)(e.p,{children:["All ",(0,t.jsx)(e.code,{children:"Runnable"})," objects implement a method called stream."]}),"\n",(0,t.jsx)(e.p,{children:"These methods are designed to stream the final output in chunks,\nyielding each chunk as soon as it is available."}),"\n",(0,t.jsxs)(e.p,{children:["Streaming is only possible if all steps in the program know how to\nprocess an ",(0,t.jsx)(e.strong,{children:"input stream"}),"; i.e., process an input chunk one at a time,\nand yield a corresponding output chunk."]}),"\n",(0,t.jsx)(e.p,{children:"The complexity of this processing can vary, from straightforward tasks\nlike emitting tokens produced by an LLM, to more challenging ones like\nstreaming parts of JSON results before the entire JSON is complete."}),"\n",(0,t.jsx)(e.p,{children:"The best place to start exploring streaming is with the single most\nimportant components in LLM apps \u2013 the models themselves!"}),"\n",(0,t.jsx)(e.h2,{id:"llms-and-chat-models",children:"LLMs and Chat Models"}),"\n",(0,t.jsxs)(e.p,{children:["Large language models can take several seconds to generate a complete\nresponse to a query. This is far slower than the ",(0,t.jsx)(e.strong,{children:"~200-300 ms"}),"\nthreshold at which an application feels responsive to an end user."]}),"\n",(0,t.jsx)(e.p,{children:"The key strategy to make the application feel more responsive is to show\nintermediate progress; e.g., to stream the output from the model token\nby token."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'import "dotenv/config";\n'})}),"\n","\n",(0,t.jsx)(o.A,{}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'// | output: false\n// | echo: false\n\nimport { ChatOpenAI } from "@langchain/openai";\nconst model = new ChatOpenAI({\n  model: "gpt-4o",\n});\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const stream = await model.stream("Hello! Tell me about yourself.");\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n  console.log(`${chunk.content}|`);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"|\nHello|\n!|\n I'm|\n an|\n AI|\n language|\n model|\n created|\n by|\n Open|\nAI|\n,|\n designed|\n to|\n assist|\n with|\n a|\n wide|\n range|\n of|\n tasks|\n by|\n understanding|\n and|\n generating|\n human|\n-like|\n text|\n based|\n on|\n the|\n input|\n I|\n receive|\n.|\n I|\n can|\n help|\n answer|\n questions|\n,|\n provide|\n explanations|\n,|\n offer|\n advice|\n,|\n write|\n creatively|\n,|\n and|\n much|\n more|\n.|\n How|\n can|\n I|\n assist|\n you|\n today|\n?|\n|\n"})}),"\n",(0,t.jsx)(e.p,{children:"Let\u2019s have a look at one of the raw chunks:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:"chunks[0];\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'AIMessageChunk {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "",\n    tool_call_chunks: [],\n    additional_kwargs: {},\n    tool_calls: [],\n    invalid_tool_calls: [],\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "",\n  name: undefined,\n  additional_kwargs: {},\n  response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n  tool_calls: [],\n  invalid_tool_calls: [],\n  tool_call_chunks: []\n}\n'})}),"\n",(0,t.jsxs)(e.p,{children:["We got back something called an ",(0,t.jsx)(e.code,{children:"AIMessageChunk"}),". This chunk represents\na part of an ",(0,t.jsx)(e.code,{children:"AIMessage"}),"."]}),"\n",(0,t.jsxs)(e.p,{children:["Message chunks are additive by design \u2013 one can simply add them up using\nthe ",(0,t.jsx)(e.code,{children:".concat()"})," method to get the state of the response so far!"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:"let finalChunk = chunks[0];\n\nfor (const chunk of chunks.slice(1, 5)) {\n  finalChunk = finalChunk.concat(chunk);\n}\n\nfinalChunk;\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'AIMessageChunk {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "Hello! I\'m an",\n    additional_kwargs: {},\n    response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n    tool_call_chunks: [],\n    tool_calls: [],\n    invalid_tool_calls: []\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "Hello! I\'m an",\n  name: undefined,\n  additional_kwargs: {},\n  response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n  tool_calls: [],\n  invalid_tool_calls: [],\n  tool_call_chunks: []\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"chains",children:"Chains"}),"\n",(0,t.jsx)(e.p,{children:"Virtually all LLM applications involve more steps than just a call to a\nlanguage model."}),"\n",(0,t.jsxs)(e.p,{children:["Let\u2019s build a simple chain using ",(0,t.jsx)(e.code,{children:"LangChain Expression Language"}),"\n(",(0,t.jsx)(e.code,{children:"LCEL"}),") that combines a prompt, model and a parser and verify that\nstreaming works."]}),"\n",(0,t.jsxs)(e.p,{children:["We will use ",(0,t.jsx)(e.code,{children:"StringOutputParser"})," to parse the output from the model.\nThis is a simple parser that extracts the content field from an\n",(0,t.jsx)(e.code,{children:"AIMessageChunk"}),", giving us the ",(0,t.jsx)(e.code,{children:"token"})," returned by the model."]}),"\n",(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsx)(e.p,{children:"LCEL is a declarative way to specify a \u201cprogram\u201d by chainining together\ndifferent LangChain primitives. Chains created using LCEL benefit from\nan automatic implementation of stream, allowing streaming of the final\noutput. In fact, chains created with LCEL implement the entire standard\nRunnable interface."})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'import { StringOutputParser } from "@langchain/core/output_parsers";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\nconst prompt = ChatPromptTemplate.fromTemplate("Tell me a joke about {topic}");\n\nconst parser = new StringOutputParser();\n\nconst chain = prompt.pipe(model).pipe(parser);\n\nconst stream = await chain.stream({\n  topic: "parrot",\n});\n\nfor await (const chunk of stream) {\n  console.log(`${chunk}|`);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'|\nSure|\n!|\n Here\'s|\n a|\n par|\nrot|\n joke|\n for|\n you|\n:\n\n|\nWhy|\n did|\n the|\n par|\nrot|\n get|\n a|\n job|\n?\n\n|\nBecause|\n he|\n was|\n tired|\n of|\n being|\n "|\npol|\nly|\n-em|\nployment|\n!"|\n \ud83c\udf89|\n\ud83e\udd9c|\n|\n'})}),"\n",(0,t.jsxs)(e.admonition,{type:"note",children:[(0,t.jsxs)(e.p,{children:["You do not have to use the ",(0,t.jsx)(e.code,{children:"LangChain Expression Language"})," to use\nLangChain and can instead rely on a standard ",(0,t.jsx)(e.strong,{children:"imperative"})," programming\napproach by caling ",(0,t.jsx)(e.code,{children:"invoke"}),", ",(0,t.jsx)(e.code,{children:"batch"})," or ",(0,t.jsx)(e.code,{children:"stream"})," on each component\nindividually, assigning the results to variables and then using them\ndownstream as you see fit."]}),(0,t.jsx)(e.p,{children:"If that works for your needs, then that\u2019s fine by us \ud83d\udc4c!"})]}),"\n",(0,t.jsx)(e.h3,{id:"working-with-input-streams",children:"Working with Input Streams"}),"\n",(0,t.jsx)(e.p,{children:"What if you wanted to stream JSON from the output as it was being\ngenerated?"}),"\n",(0,t.jsxs)(e.p,{children:["If you were to rely on ",(0,t.jsx)(e.code,{children:"JSON.parse"})," to parse the partial json, the\nparsing would fail as the partial json wouldn\u2019t be valid json."]}),"\n",(0,t.jsx)(e.p,{children:"You\u2019d likely be at a complete loss of what to do and claim that it\nwasn\u2019t possible to stream JSON."}),"\n",(0,t.jsxs)(e.p,{children:["Well, turns out there is a way to do it - the parser needs to operate on\nthe ",(0,t.jsx)(e.strong,{children:"input stream"}),", and attempt to \u201cauto-complete\u201d the partial json\ninto a valid state."]}),"\n",(0,t.jsx)(e.p,{children:"Let\u2019s see such a parser in action to understand what this means."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'import { JsonOutputParser } from "@langchain/core/output_parsers";\n\nconst chain = model.pipe(new JsonOutputParser());\nconst stream = await chain.stream(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'{\n  countries: [\n    { name: "France", population: 67372000 },\n    { name: "Spain", population: 47450795 },\n    { name: "Japan", population: 125960000 }\n  ]\n}\n'})}),"\n",(0,t.jsxs)(e.p,{children:["Now, let\u2019s ",(0,t.jsx)(e.strong,{children:"break"})," streaming. We\u2019ll use the previous example and\nappend an extraction function at the end that extracts the country names\nfrom the finalized JSON. Since this new last step is just a function\ncall with no defined streaming behavior, the streaming output from\nprevious steps is aggregated, then passed as a single input to the\nfunction."]}),"\n",(0,t.jsx)(e.admonition,{type:"warning",children:(0,t.jsxs)(e.p,{children:["Any steps in the chain that operate on ",(0,t.jsx)(e.strong,{children:"finalized inputs"})," rather than\non ",(0,t.jsx)(e.strong,{children:"input streams"})," can break streaming functionality via ",(0,t.jsx)(e.code,{children:"stream"}),"."]})}),"\n",(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsxs)(e.p,{children:["Later, we will discuss the ",(0,t.jsx)(e.code,{children:"streamEvents"})," API which streams results from\nintermediate steps. This API will stream results from intermediate steps\neven if the chain contains steps that only operate on ",(0,t.jsx)(e.strong,{children:"finalized\ninputs"}),"."]})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'// A function that operates on finalized inputs\n// rather than on an input_stream\n\n// A function that does not operates on input streams and breaks streaming.\nconst extractCountryNames = (inputs: Record<string, any>) => {\n  if (!Array.isArray(inputs.countries)) {\n    return "";\n  }\n  return JSON.stringify(inputs.countries.map((country) => country.name));\n};\n\nconst chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);\n\nconst stream = await chain.stream(\n  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'["France","Spain","Japan"]\n'})}),"\n",(0,t.jsx)(e.h3,{id:"non-streaming-components",children:"Non-streaming components"}),"\n",(0,t.jsxs)(e.p,{children:["Like the above example, some built-in components like Retrievers do not\noffer any streaming. What happens if we try to ",(0,t.jsx)(e.code,{children:"stream"})," them?"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'import { OpenAIEmbeddings } from "@langchain/openai";\nimport { MemoryVectorStore } from "langchain/vectorstores/memory";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\nconst template = `Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n`;\nconst prompt = ChatPromptTemplate.fromTemplate(template);\n\nconst vectorstore = await MemoryVectorStore.fromTexts(\n  ["mitochondria is the powerhouse of the cell", "buildings are made of brick"],\n  [{}, {}],\n  new OpenAIEmbeddings()\n);\n\nconst retriever = vectorstore.asRetriever();\n\nconst chunks = [];\n\nfor await (const chunk of await retriever.stream(\n  "What is the powerhouse of the cell?"\n)) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks);\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'[\n  [\n    Document {\n      pageContent: "mitochondria is the powerhouse of the cell",\n      metadata: {}\n    },\n    Document {\n      pageContent: "buildings are made of brick",\n      metadata: {}\n    }\n  ]\n]\n'})}),"\n",(0,t.jsx)(e.p,{children:"Stream just yielded the final result from that component."}),"\n",(0,t.jsx)(e.p,{children:"This is OK! Not all components have to implement streaming \u2013 in some\ncases streaming is either unnecessary, difficult or just doesn\u2019t make\nsense."}),"\n",(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsx)(e.p,{children:"An LCEL chain constructed using some non-streaming components will still\nbe able to stream in a lot of cases, with streaming of partial output\nstarting after the last non-streaming step in the chain."})}),"\n",(0,t.jsx)(e.p,{children:"Here\u2019s an example of this:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'import {\n  RunnablePassthrough,\n  RunnableSequence,\n} from "@langchain/core/runnables";\nimport type { Document } from "@langchain/core/documents";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join("\\n-----\\n");\n};\n\nconst retrievalChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst stream = await retrievalChain.stream(\n  "What is the powerhouse of the cell?"\n);\n\nfor await (const chunk of stream) {\n  console.log(`${chunk}|`);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"|\nM|\nito|\nch|\nond|\nria|\n is|\n the|\n powerhouse|\n of|\n the|\n cell|\n.|\n|\n"})}),"\n",(0,t.jsxs)(e.p,{children:["Now that we\u2019ve seen how the ",(0,t.jsx)(e.code,{children:"stream"})," method works, let\u2019s venture into\nthe world of streaming events!"]}),"\n",(0,t.jsx)(e.h2,{id:"using-stream-events",children:"Using Stream Events"}),"\n",(0,t.jsxs)(e.p,{children:["Event Streaming is a ",(0,t.jsx)(e.strong,{children:"beta"})," API. This API may change a bit based on\nfeedback."]}),"\n",(0,t.jsx)(e.admonition,{type:"note",children:(0,t.jsxs)(e.p,{children:["Introduced in (",(0,t.jsx)(e.strong,{children:"langchain/core?"}),") ",(0,t.jsx)(e.strong,{children:"0.1.27"}),"."]})}),"\n",(0,t.jsxs)(e.p,{children:["For the ",(0,t.jsx)(e.code,{children:"streamEvents"})," method to work properly:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Any custom functions / runnables must propragate callbacks"}),"\n",(0,t.jsx)(e.li,{children:"Set proper parameters on models to force the LLM to stream tokens."}),"\n",(0,t.jsx)(e.li,{children:"Let us know if anything doesn\u2019t work as expected!"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"event-reference",children:"Event Reference"}),"\n",(0,t.jsx)(e.p,{children:"Below is a reference table that shows some events that might be emitted\nby the various Runnable objects."}),"\n",(0,t.jsx)(e.admonition,{type:"note",children:(0,t.jsxs)(e.p,{children:["When streaming is implemented properly, the inputs to a runnable will\nnot be known until after the input stream has been entirely consumed.\nThis means that ",(0,t.jsx)(e.code,{children:"inputs"})," will often be included only for ",(0,t.jsx)(e.code,{children:"end"})," events\nand rather than for ",(0,t.jsx)(e.code,{children:"start"})," events."]})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-md",children:'| event              | name               | chunk                                      | input                 | output                                          |\n| ------------------ | ------------------ | ------------------------------------------ | --------------------- | ----------------------------------------------- |\n| on_llm_start       | "[model name]"     |                                            | {"input": "hello"}    |                                                 |\n| on_llm_stream      | "[model name]"     | "Hello" or AIMessageChunk(content="hello") |                       |                                                 |\n| on_llm_end         | "[model name]"     |                                            | "Hello human!"        | {"generations": [...], "llmOutput": None, ...}  |\n| on_chain_start     | format_docs        |                                            |                       |                                                 |\n| on_chain_stream    | format_docs        | "hello world!, goodbye world!"             |                       |                                                 |\n| on_chain_end       | format_docs        |                                            | [Document(...)]       | "hello world!, goodbye world!"                  |\n| on_tool_start      | some_tool          |                                            | {"x": 1, "y": "2"}    |                                                 |\n| on_tool_stream     | some_tool          | {"x": 1, "y": "2"}                         |                       |                                                 |\n| on_tool_end        | some_tool          |                                            |                       | {"x": 1, "y": "2"}                              |\n| on_retriever_start | "[retriever name]" |                                            | {"query": "hello"}    |                                                 |\n| on_retriever_chunk | "[retriever name]" | {"documents": [...]}                       |                       |                                                 |\n| on_retriever_end   | "[retriever name]" |                                            | {"query": "hello"}    | {"documents": [...]}                            |\n| on_prompt_start    | "[template_name]"  |                                            | {"question": "hello"} |                                                 |\n| on_prompt_end      | "[template_name]"  |                                            | {"question": "hello"} | ChatPromptValue(messages: [SystemMessage, ...]) |\n'})}),"\n",(0,t.jsx)(e.h3,{id:"chat-model",children:"Chat Model"}),"\n",(0,t.jsx)(e.p,{children:"Let\u2019s start off by looking at the events produced by a chat model."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const events = [];\n\nconst eventStream = await model.streamEvents("hello", { version: "v1" });\n\nfor await (const event of eventStream) {\n  events.push(event);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"13\n"})}),"\n",(0,t.jsxs)(e.admonition,{type:"note",children:[(0,t.jsx)(e.p,{children:"Hey what\u2019s that funny version=\u201cv1\u201d parameter in the API?! \ud83d\ude3e"}),(0,t.jsxs)(e.p,{children:["This is a ",(0,t.jsx)(e.strong,{children:"beta API"}),", and we\u2019re almost certainly going to make some\nchanges to it."]}),(0,t.jsx)(e.p,{children:"This version parameter will allow us to mimimize such breaking changes\nto your code."}),(0,t.jsx)(e.p,{children:"In short, we are annoying you now, so we don\u2019t have to annoy you later."})]}),"\n",(0,t.jsx)(e.p,{children:"Let\u2019s take a look at the few of the start event and a few of the end\nevents."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:"events.slice(0, 3);\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'[\n  {\n    run_id: "3394874b-6a19-4d2c-a80f-bd3ff7f25e85",\n    event: "on_llm_start",\n    name: "ChatOpenAI",\n    tags: [],\n    metadata: {},\n    data: { input: "hello" }\n  },\n  {\n    event: "on_llm_stream",\n    run_id: "3394874b-6a19-4d2c-a80f-bd3ff7f25e85",\n    tags: [],\n    metadata: {},\n    name: "ChatOpenAI",\n    data: {\n      chunk: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  },\n  {\n    event: "on_llm_stream",\n    run_id: "3394874b-6a19-4d2c-a80f-bd3ff7f25e85",\n    tags: [],\n    metadata: {},\n    name: "ChatOpenAI",\n    data: {\n      chunk: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "Hello",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "Hello",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n]\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:"events.slice(-2);\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'[\n  {\n    event: "on_llm_stream",\n    run_id: "3394874b-6a19-4d2c-a80f-bd3ff7f25e85",\n    tags: [],\n    metadata: {},\n    name: "ChatOpenAI",\n    data: {\n      chunk: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: "stop" },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  },\n  {\n    event: "on_llm_end",\n    name: "ChatOpenAI",\n    run_id: "3394874b-6a19-4d2c-a80f-bd3ff7f25e85",\n    tags: [],\n    metadata: {},\n    data: { output: { generations: [ [Array] ] } }\n  }\n]\n'})}),"\n",(0,t.jsx)(e.h3,{id:"chain",children:"Chain"}),"\n",(0,t.jsx)(e.p,{children:"Let\u2019s revisit the example chain that parsed streaming JSON to explore\nthe streaming events API."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const chain = model.pipe(new JsonOutputParser());\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n  { version: "v1" }\n);\n\nconst events = [];\nfor await (const event of eventStream) {\n  events.push(event);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"84\n"})}),"\n",(0,t.jsxs)(e.p,{children:["If you examine at the first few events, you\u2019ll notice that there are\n",(0,t.jsx)(e.strong,{children:"3"})," different start events rather than ",(0,t.jsx)(e.strong,{children:"2"})," start events."]}),"\n",(0,t.jsx)(e.p,{children:"The three start events correspond to:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"The chain (model + parser)"}),"\n",(0,t.jsx)(e.li,{children:"The model"}),"\n",(0,t.jsx)(e.li,{children:"The parser"}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:"events.slice(0, 3);\n"})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'[\n  {\n    run_id: "289af8b8-7047-44e6-a475-26b88ddc7e34",\n    event: "on_chain_start",\n    name: "RunnableSequence",\n    tags: [],\n    metadata: {},\n    data: {\n      input: "Output a list of the countries france, spain and japan and their populations in JSON format. Use a d"... 129 more characters\n    }\n  },\n  {\n    event: "on_llm_start",\n    name: "ChatOpenAI",\n    run_id: "d43b539d-23ae-42ad-9bec-64faf58cf423",\n    tags: [ "seq:step:1" ],\n    metadata: {},\n    data: { input: { messages: [ [Array] ] } }\n  },\n  {\n    event: "on_parser_start",\n    name: "JsonOutputParser",\n    run_id: "91b6f786-0838-4888-8c2d-25ecd5d62d47",\n    tags: [ "seq:step:2" ],\n    metadata: {},\n    data: {}\n  }\n]\n'})}),"\n",(0,t.jsx)(e.p,{children:"What do you think you\u2019d see if you looked at the last 3 events? what\nabout the middle?"}),"\n",(0,t.jsx)(e.p,{children:"Let\u2019s use this API to take output the stream events from the model and\nthe parser. We\u2019re ignoring start events, end events and events from the\nchain."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'let eventCount = 0;\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n  { version: "v1" }\n);\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 30) {\n    continue;\n  }\n  const eventType = event.event;\n  if (eventType === "on_llm_stream") {\n    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);\n  } else if (eventType === "on_parser_stream") {\n    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);\n  }\n  eventCount += 1;\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'Chat model chunk:\nChat model chunk: ```\nChat model chunk: json\nChat model chunk:\n\nChat model chunk: {\n\nChat model chunk:\nChat model chunk:  "\nChat model chunk: countries\nChat model chunk: ":\nChat model chunk:  [\n\nChat model chunk:\nChat model chunk:  {\n\nChat model chunk:\nChat model chunk:  "\nChat model chunk: name\nChat model chunk: ":\nChat model chunk:  "\nChat model chunk: France\nChat model chunk: ",\n\nChat model chunk:\nChat model chunk:  "\nChat model chunk: population\nChat model chunk: ":\nChat model chunk:\nChat model chunk: 673\nChat model chunk: 480\nChat model chunk: 00\nChat model chunk:\n'})}),"\n",(0,t.jsx)(e.p,{children:"Because both the model and the parser support streaming, we see\nstreaming events from both components in real time! Neat! \ud83e\udd9c"}),"\n",(0,t.jsx)(e.h3,{id:"filtering-events",children:"Filtering Events"}),"\n",(0,t.jsx)(e.p,{children:"Because this API produces so many events, it is useful to be able to\nfilter on events."}),"\n",(0,t.jsxs)(e.p,{children:["You can filter by either component ",(0,t.jsx)(e.code,{children:"name"}),", component ",(0,t.jsx)(e.code,{children:"tags"})," or component\n",(0,t.jsx)(e.code,{children:"type"}),"."]}),"\n",(0,t.jsx)(e.h4,{id:"by-name",children:"By Name"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const chain = model\n  .withConfig({ runName: "model" })\n  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }));\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n  { version: "v1" },\n  { includeNames: ["my_parser"] }\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 10) {\n    continue;\n  }\n  console.log(event);\n  eventCount += 1;\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'{\n  event: "on_parser_start",\n  name: "my_parser",\n  run_id: "bd05589a-0725-486b-b814-81af62ba5d80",\n  tags: [ "seq:step:2" ],\n  metadata: {},\n  data: {}\n}\n{\n  event: "on_parser_stream",\n  name: "my_parser",\n  run_id: "bd05589a-0725-486b-b814-81af62ba5d80",\n  tags: [ "seq:step:2" ],\n  metadata: {},\n  data: {\n    chunk: {\n      countries: [\n        { name: "France", population: 65273511 },\n        { name: "Spain", population: 46754778 },\n        { name: "Japan", population: 126476461 }\n      ]\n    }\n  }\n}\n{\n  event: "on_parser_end",\n  name: "my_parser",\n  run_id: "bd05589a-0725-486b-b814-81af62ba5d80",\n  tags: [ "seq:step:2" ],\n  metadata: {},\n  data: {\n    output: {\n      countries: [\n        { name: "France", population: 65273511 },\n        { name: "Spain", population: 46754778 },\n        { name: "Japan", population: 126476461 }\n      ]\n    }\n  }\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"3\n"})}),"\n",(0,t.jsx)(e.h4,{id:"by-type",children:"By type"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const chain = model\n  .withConfig({ runName: "model" })\n  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }));\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n  { version: "v1" },\n  { includeTypes: ["llm"] }\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 10) {\n    continue;\n  }\n  console.log(event);\n  eventCount += 1;\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'{\n  event: "on_llm_start",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    input: { messages: [ [ [HumanMessage] ] ] }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "Sure",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "Sure",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "Sure",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: ",",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: ",",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: ",",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " here\'s",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " here\'s",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " here\'s",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " the",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " the",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " the",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " JSON",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " JSON",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " JSON",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " representation",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " representation",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " representation",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " of",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " of",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " of",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " the",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " the",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " the",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "model",\n  run_id: "4c7dbe4a-57ea-40b9-9fc0-a77d7851c5fd",\n  tags: [ "seq:step:1" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " countries",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " countries",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " countries",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n'})}),"\n",(0,t.jsx)(e.h4,{id:"by-tags",children:"By Tags"}),"\n",(0,t.jsxs)(e.admonition,{type:"caution",children:[(0,t.jsx)(e.p,{children:"Tags are inherited by child components of a given runnable."}),(0,t.jsx)(e.p,{children:"If you\u2019re using tags to filter, make sure that this is what you want."})]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const chain = model\n  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }))\n  .withConfig({ tags: ["my_chain"] });\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n  { version: "v1" },\n  { includeTags: ["my_chain"] }\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 10) {\n    continue;\n  }\n  console.log(event);\n  eventCount += 1;\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'{\n  run_id: "83f4ef67-4970-44f7-8ae1-5ebace8cbce0",\n  event: "on_chain_start",\n  name: "RunnableSequence",\n  tags: [ "my_chain" ],\n  metadata: {},\n  data: {\n    input: "Output a list of the countries france, spain and japan and their populations in JSON format. Use a d"... 129 more characters\n  }\n}\n{\n  event: "on_llm_start",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    input: { messages: [ [ [HumanMessage] ] ] }\n  }\n}\n{\n  event: "on_parser_start",\n  name: "my_parser",\n  run_id: "346234e7-b109-4bf7-a568-70edd67bc209",\n  tags: [ "seq:step:2", "my_chain" ],\n  metadata: {},\n  data: {}\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "```",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "```",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "```",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "json",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "json",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "json",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "\\n",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "\\n",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "\\n",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "{\\n",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "{\\n",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "{\\n",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: " ",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: " ",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: " ",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: \' "\',\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: \' "\',\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: \' "\',\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  name: "ChatOpenAI",\n  run_id: "d84bca89-bc85-4d1d-a7af-3403f5789bd0",\n  tags: [ "seq:step:1", "my_chain" ],\n  metadata: {},\n  data: {\n    chunk: ChatGenerationChunk {\n      text: "countries",\n      generationInfo: { prompt: 0, completion: 0, finish_reason: null },\n      message: AIMessageChunk {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "countries",\n          tool_call_chunks: [],\n          additional_kwargs: {},\n          tool_calls: [],\n          invalid_tool_calls: [],\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "countries",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: { prompt: 0, completion: 0, finish_reason: null },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: []\n      }\n    }\n  }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"streaming-events-over-http",children:"Streaming events over HTTP"}),"\n",(0,t.jsxs)(e.p,{children:["For convenience, ",(0,t.jsx)(e.code,{children:"streamEvents"})," supports encoding streamed intermediate\nevents as HTTP ",(0,t.jsx)(e.a,{href:"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events",children:"server-sent\nevents"}),",\nencoded as bytes. Here\u2019s what that looks like (using a\n",(0,t.jsx)(e.a,{href:"https://developer.mozilla.org/en-US/docs/Web/API/TextDecoder",children:(0,t.jsx)(e.code,{children:"TextDecoder"})}),"\nto reconvert the binary data back into a human readable string):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const chain = model\n  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }))\n  .withConfig({ tags: ["my_chain"] });\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n  {\n    version: "v1",\n    encoding: "text/event-stream",\n  }\n);\n\nlet eventCount = 0;\n\nconst textDecoder = new TextDecoder();\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 3) {\n    continue;\n  }\n  console.log(textDecoder.decode(event));\n  eventCount += 1;\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'event: data\ndata: {"run_id":"9344be82-f4e6-49be-9eea-88eb2ae53340","event":"on_chain_start","name":"RunnableSequence","tags":["my_chain"],"metadata":{},"data":{"input":"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \\"countries\\" which contains a list of countries. Each country should have the key \\"name\\" and \\"population\\""}}\n\n\nevent: data\ndata: {"event":"on_llm_start","name":"ChatOpenAI","run_id":"20640210-4b45-4ac3-9e5e-ad6e6d48431f","tags":["seq:step:1","my_chain"],"metadata":{},"data":{"input":{"messages":[[{"lc":1,"type":"constructor","id":["langchain_core","messages","HumanMessage"],"kwargs":{"content":"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \\"countries\\" which contains a list of countries. Each country should have the key \\"name\\" and \\"population\\"","additional_kwargs":{},"response_metadata":{}}}]]}}}\n\n\nevent: data\ndata: {"event":"on_parser_start","name":"my_parser","run_id":"0d035118-36bc-49a3-9bdd-5fcf8afcc5da","tags":["seq:step:2","my_chain"],"metadata":{},"data":{}}\n\n\nevent: data\ndata: {"event":"on_llm_stream","name":"ChatOpenAI","run_id":"20640210-4b45-4ac3-9e5e-ad6e6d48431f","tags":["seq:step:1","my_chain"],"metadata":{},"data":{"chunk":{"text":"","generationInfo":{"prompt":0,"completion":0,"finish_reason":null},"message":{"lc":1,"type":"constructor","id":["langchain_core","messages","AIMessageChunk"],"kwargs":{"content":"","tool_call_chunks":[],"additional_kwargs":{},"tool_calls":[],"invalid_tool_calls":[],"response_metadata":{"prompt":0,"completion":0,"finish_reason":null}}}}}}\n\n'})}),"\n",(0,t.jsxs)(e.p,{children:["A nice feature of this format is that you can pass the resulting stream\ndirectly into a native ",(0,t.jsx)(e.a,{href:"https://developer.mozilla.org/en-US/docs/Web/API/Response",children:"HTTP response\nobject"})," with\nthe correct headers (commonly used by frameworks like\n",(0,t.jsx)(e.a,{href:"https://hono.dev/",children:"Hono"})," and ",(0,t.jsx)(e.a,{href:"https://nextjs.org/",children:"Next.js"}),"), then\nparse that stream on the frontend. Your server-side handler would look\nsomething like this:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const handler = async () => {\n  const eventStream = await chain.streamEvents(\n    `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n    {\n      version: "v1",\n      encoding: "text/event-stream",\n    }\n  );\n  return new Response(eventStream, {\n    headers: {\n      "content-type": "text/event-stream",\n    },\n  });\n};\n'})}),"\n",(0,t.jsxs)(e.p,{children:["And your frontend could look like this (using the\n",(0,t.jsx)(e.a,{href:"https://www.npmjs.com/package/@microsoft/fetch-event-source",children:(0,t.jsx)(e.code,{children:"@microsoft/fetch-event-source"})}),"\npacakge to fetch and parse the event source):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'import { fetchEventSource } from "@microsoft/fetch-event-source";\n\nconst makeChainRequest = async () => {\n  await fetchEventSource("https://your_url_here", {\n    method: "POST",\n    body: JSON.stringify({\n      foo: "bar",\n    }),\n    onmessage: (message) => {\n      if (message.event === "data") {\n        console.log(message.data);\n      }\n    },\n    onerror: (err) => {\n      console.log(err);\n    },\n  });\n};\n'})}),"\n",(0,t.jsx)(e.h3,{id:"non-streaming-components-1",children:"Non-streaming components"}),"\n",(0,t.jsxs)(e.p,{children:["Remember how some components don\u2019t stream well because they don\u2019t\noperate on ",(0,t.jsx)(e.strong,{children:"input streams"}),"?"]}),"\n",(0,t.jsxs)(e.p,{children:["While such components can break streaming of the final output when using\n",(0,t.jsx)(e.code,{children:"stream"}),", ",(0,t.jsx)(e.code,{children:"streamEvents"})," will still yield streaming events from\nintermediate steps that support streaming!"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'// A function that operates on finalized inputs\n// rather than on an input_stream\n\n// A function that does not operates on input streams and breaks streaming.\nconst extractCountryNames = (inputs: Record<string, any>) => {\n  if (!Array.isArray(inputs.countries)) {\n    return "";\n  }\n  return JSON.stringify(inputs.countries.map((country) => country.name));\n};\n\nconst chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);\n\nconst stream = await chain.stream(\n  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'["France","Spain","Japan"]\n'})}),"\n",(0,t.jsxs)(e.p,{children:["As expected, the ",(0,t.jsx)(e.code,{children:"stream"})," API doesn\u2019t work correctly because\n",(0,t.jsx)(e.code,{children:"extractCountryNames"})," doesn\u2019t operate on streams."]}),"\n",(0,t.jsxs)(e.p,{children:["Now, let\u2019s confirm that with ",(0,t.jsx)(e.code,{children:"streamEvents"})," we\u2019re still seeing streaming\noutput from the model and the parser."]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'const eventStream = await chain.streamEvents(\n  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,\n  { version: "v1" }\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 30) {\n    continue;\n  }\n  const eventType = event.event;\n  if (eventType === "on_llm_stream") {\n    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);\n  } else if (eventType === "on_parser_stream") {\n    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);\n  }\n  eventCount += 1;\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"Chat model chunk:\nChat model chunk: Here's\nChat model chunk:  how\nChat model chunk:  you\nChat model chunk:  can\nChat model chunk:  represent\nChat model chunk:  the\nChat model chunk:  countries\nChat model chunk:  France\nChat model chunk: ,\nChat model chunk:  Spain\nChat model chunk: ,\nChat model chunk:  and\nChat model chunk:  Japan\nChat model chunk: ,\nChat model chunk:  along\nChat model chunk:  with\nChat model chunk:  their\nChat model chunk:  populations\nChat model chunk: ,\nChat model chunk:  in\nChat model chunk:  JSON\nChat model chunk:  format\nChat model chunk: :\n\n\nChat model chunk: ```\nChat model chunk: json\nChat model chunk:\n\nChat model chunk: {\n"})})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(h,{...n})}):h(n)}},63142:(n,e,a)=>{a.d(e,{A:()=>m});a(96540);var t=a(11470),s=a(19365),o=a(21432),l=a(27846),i=a(27293),r=a(74848);function c(n){let{children:e}=n;return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.A,{type:"tip",children:(0,r.jsxs)("p",{children:["See"," ",(0,r.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,r.jsx)(l.A,{children:e})]})}const d={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},h=["openai","anthropic","mistral","groq","vertex"];function m(n){const{customVarName:e,additionalDependencies:a}=n,l=e??"model",i=n.openaiParams??d.openaiParams,m=n.anthropicParams??d.anthropicParams,p=n.fireworksParams??d.fireworksParams,u=n.mistralParams??d.mistralParams,_=n.groqParams??d.groqParams,g=n.vertexParams??d.vertexParams,f=n.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],k={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${l} = new ChatOpenAI(${i});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${l} = new ChatAnthropic(${m});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${l} = new ChatFireworks(${p});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${l} = new ChatMistralAI(${u});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${l} = new ChatGroq(${_});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${l} = new ChatVertexAI(${g});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},v=(n.onlyWso?h:f).map((n=>k[n]));return(0,r.jsxs)("div",{children:[(0,r.jsx)("h3",{children:"\uc0ac\uc6a9\ud560 \ucc44\ud305 \ubaa8\ub378 \uc120\ud0dd:"}),(0,r.jsx)(t.A,{groupId:"modelTabs",children:v.map((n=>(0,r.jsxs)(s.A,{value:n.value,label:n.label,children:[(0,r.jsx)("h4",{children:"\uc758\uc874\uc131 \ucd94\uac00"}),(0,r.jsx)(c,{children:[n.dependencies,a].join(" ")}),(0,r.jsx)("h4",{children:"\ud658\uacbd\ubcc0\uc218 \ucd94\uac00"}),(0,r.jsx)(o.A,{language:"bash",children:n.envs}),(0,r.jsx)("h4",{children:"\ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654"}),(0,r.jsx)(o.A,{language:"typescript",children:n.text})]},n.value)))})]})}},27846:(n,e,a)=>{a.d(e,{A:()=>i});a(96540);var t=a(11470),s=a(19365),o=a(21432),l=a(74848);function i(n){let{children:e}=n;return(0,l.jsxs)(t.A,{groupId:"npm2yarn",children:[(0,l.jsx)(s.A,{value:"npm",label:"npm",children:(0,l.jsxs)(o.A,{language:"bash",children:["npm i ",e]})}),(0,l.jsx)(s.A,{value:"yarn",label:"yarn",default:!0,children:(0,l.jsxs)(o.A,{language:"bash",children:["yarn add ",e]})}),(0,l.jsx)(s.A,{value:"pnpm",label:"pnpm",children:(0,l.jsxs)(o.A,{language:"bash",children:["pnpm add ",e]})})]})}}}]);