(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1039,7817,65],{2101:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>_,contentTitle:()=>T,default:()=>M,frontMatter:()=>w,metadata:()=>b,toc:()=>A});var o=t(74848),a=t(28453),s=t(64428),r=t(78847),i=t(2280),l=t(27368),c=t.n(l),m=t(65505),p=t.n(m),d=t(52348),h=t.n(d),u=t(53198),g=t.n(u),f=t(49971),x=t.n(f),j=t(2741),y=t.n(j);const w={sidebar_label:"Mistral AI"},T="ChatMistralAI",b={id:"integrations/chat/mistral",title:"ChatMistralAI",description:"Mistral AI is a research organization and hosting platform for LLMs.",source:"@site/docs/integrations/chat/mistral.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/mistral",permalink:"/docs/integrations/chat/mistral",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/mistral.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"Mistral AI"},sidebar:"integrations",previous:{title:"Minimax",permalink:"/docs/integrations/chat/minimax"},next:{title:"NIBittensorChatModel",permalink:"/docs/integrations/chat/ni_bittensor"}},_={},A=[{value:"Models",id:"models",level:2},{value:"Setup",id:"setup",level:2},...r.toc,...i.toc,{value:"Usage",id:"usage",level:2},{value:"Streaming",id:"streaming",level:3},{value:"Tool calling",id:"tool-calling",level:3},{value:"<code>.withStructuredOutput({ ... })</code>",id:"withstructuredoutput--",level:3},{value:"Using JSON schema:",id:"using-json-schema",level:3},{value:"Tool calling agent",id:"tool-calling-agent",level:3}];function C(n){const e={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"chatmistralai",children:"ChatMistralAI"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.a,{href:"https://mistral.ai/",children:"Mistral AI"})," is a research organization and hosting platform for LLMs.\nThey're most known for their family of 7B models (",(0,o.jsxs)(e.a,{href:"https://mistral.ai/news/announcing-mistral-7b/",children:[(0,o.jsx)(e.code,{children:"mistral7b"})," // ",(0,o.jsx)(e.code,{children:"mistral-tiny"})]}),", ",(0,o.jsxs)(e.a,{href:"https://mistral.ai/news/mixtral-of-experts/",children:[(0,o.jsx)(e.code,{children:"mixtral8x7b"})," // ",(0,o.jsx)(e.code,{children:"mistral-small"})]}),")."]}),"\n",(0,o.jsx)(e.p,{children:"The LangChain implementation of Mistral's models uses their hosted generation API, making it easier to access their models without needing to run them locally."}),"\n",(0,o.jsx)(e.h2,{id:"models",children:"Models"}),"\n",(0,o.jsx)(e.p,{children:"Mistral's API offers access to two of their open source, and proprietary models:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"open-mistral-7b"})," (aka ",(0,o.jsx)(e.code,{children:"mistral-tiny-2312"}),")"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"open-mixtral-8x7b"})," (aka ",(0,o.jsx)(e.code,{children:"mistral-small-2312"}),")"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"mistral-small-latest"})," (aka ",(0,o.jsx)(e.code,{children:"mistral-small-2402"}),") (default)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"mistral-medium-latest"})," (aka ",(0,o.jsx)(e.code,{children:"mistral-medium-2312"}),")"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.code,{children:"mistral-large-latest"})," (aka ",(0,o.jsx)(e.code,{children:"mistral-large-2402"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:["See ",(0,o.jsx)(e.a,{href:"https://docs.mistral.ai/guides/model-selection/",children:"this page"})," for an up to date list."]}),"\n",(0,o.jsx)(e.h2,{id:"setup",children:"Setup"}),"\n",(0,o.jsxs)(e.p,{children:["In order to use the Mistral API you'll need an API key. You can sign up for a Mistral account and create an API key ",(0,o.jsx)(e.a,{href:"https://console.mistral.ai/",children:"here"}),"."]}),"\n",(0,o.jsxs)(e.p,{children:["You'll first need to install the ",(0,o.jsx)(e.a,{href:"https://www.npmjs.com/package/@langchain/mistralai",children:(0,o.jsx)(e.code,{children:"@langchain/mistralai"})})," package:"]}),"\n","\n",(0,o.jsx)(r.default,{}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/mistralai\n"})}),"\n","\n",(0,o.jsx)(i.default,{}),"\n",(0,o.jsx)(e.h2,{id:"usage",children:"Usage"}),"\n",(0,o.jsx)(e.p,{children:"When sending chat messages to mistral, there are a few requirements to follow:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["The first message can ",(0,o.jsx)(e.em,{children:(0,o.jsx)(e.em,{children:"not"})})," be an assistant (ai) message."]}),"\n",(0,o.jsxs)(e.li,{children:["Messages ",(0,o.jsx)(e.em,{children:(0,o.jsx)(e.em,{children:"must"})})," alternate between user and assistant (ai) messages."]}),"\n",(0,o.jsxs)(e.li,{children:["Messages can ",(0,o.jsx)(e.em,{children:(0,o.jsx)(e.em,{children:"not"})})," end with an assistant (ai) or system message."]}),"\n"]}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:c()}),"\n",(0,o.jsx)(e.admonition,{type:"info",children:(0,o.jsxs)(e.p,{children:["You can see a LangSmith trace of this example ",(0,o.jsx)(e.a,{href:"https://smith.langchain.com/public/d69d0db9-f29e-45aa-a40d-b53f6273d7d0/r",children:"here"})]})}),"\n",(0,o.jsx)(e.h3,{id:"streaming",children:"Streaming"}),"\n",(0,o.jsx)(e.p,{children:"Mistral's API also supports streaming token responses. The example below demonstrates how to use this feature."}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:p()}),"\n",(0,o.jsx)(e.admonition,{type:"info",children:(0,o.jsxs)(e.p,{children:["You can see a LangSmith trace of this example ",(0,o.jsx)(e.a,{href:"https://smith.langchain.com/public/061d90f2-ac7e-44c5-8790-8b23299f9217/r",children:"here"})]})}),"\n",(0,o.jsx)(e.h3,{id:"tool-calling",children:"Tool calling"}),"\n",(0,o.jsxs)(e.p,{children:["Mistral's API now supports tool calling and JSON mode!\nThe examples below demonstrates how to use them, along with how to use the ",(0,o.jsx)(e.code,{children:"withStructuredOutput"})," method to easily compose structured output LLM calls."]}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:h()}),"\n",(0,o.jsx)(e.h3,{id:"withstructuredoutput--",children:(0,o.jsx)(e.code,{children:".withStructuredOutput({ ... })"})}),"\n",(0,o.jsx)(e.admonition,{type:"info",children:(0,o.jsxs)(e.p,{children:["The ",(0,o.jsx)(e.code,{children:".withStructuredOutput"})," method is in beta. It is actively being worked on, so the API may change."]})}),"\n",(0,o.jsxs)(e.p,{children:["Using the ",(0,o.jsx)(e.code,{children:".withStructuredOutput"})," method, you can easily make the LLM return structured output, given only a Zod or JSON schema:"]}),"\n",(0,o.jsx)(e.admonition,{type:"note",children:(0,o.jsx)(e.p,{children:"The Mistral tool calling API requires descriptions for each tool field. If descriptions are not supplied, the API will error."})}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:g()}),"\n",(0,o.jsx)(e.h3,{id:"using-json-schema",children:"Using JSON schema:"}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:x()}),"\n",(0,o.jsx)(e.h3,{id:"tool-calling-agent",children:"Tool calling agent"}),"\n",(0,o.jsx)(e.p,{children:"The larger Mistral models not only support tool calling, but can also be used in the Tool Calling agent.\nHere's an example:"}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:y()})]})}function M(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(C,{...n})}):C(n)}},78847:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var o=t(74848),a=t(28453);const s={},r=void 0,i={id:"mdx_components/integration_install_tooltip",title:"integration_install_tooltip",description:"See this section for general instructions on installing integration packages.",source:"@site/docs/mdx_components/integration_install_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/integration_install_tooltip",permalink:"/docs/mdx_components/integration_install_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/integration_install_tooltip.mdx",tags:[],version:"current",frontMatter:{}},l={},c=[];function m(n){const e={a:"a",admonition:"admonition",p:"p",...(0,a.R)(),...n.components};return(0,o.jsx)(e.admonition,{type:"tip",children:(0,o.jsxs)(e.p,{children:["See ",(0,o.jsx)(e.a,{href:"/docs/how_to/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(m,{...n})}):m(n)}},2280:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var o=t(74848),a=t(28453);const s={},r=void 0,i={id:"mdx_components/unified_model_params_tooltip",title:"unified_model_params_tooltip",description:"We're unifying model params across all packages. We now suggest using model instead of modelName, and apiKey for API keys.",source:"@site/docs/mdx_components/unified_model_params_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/unified_model_params_tooltip",permalink:"/docs/mdx_components/unified_model_params_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/unified_model_params_tooltip.mdx",tags:[],version:"current",frontMatter:{}},l={},c=[];function m(n){const e={admonition:"admonition",code:"code",p:"p",...(0,a.R)(),...n.components};return(0,o.jsx)(e.admonition,{type:"tip",children:(0,o.jsxs)(e.p,{children:["We're unifying model params across all packages. We now suggest using ",(0,o.jsx)(e.code,{children:"model"})," instead of ",(0,o.jsx)(e.code,{children:"modelName"}),", and ",(0,o.jsx)(e.code,{children:"apiKey"})," for API keys."]})})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(m,{...n})}):m(n)}},27368:n=>{n.exports={content:"import { ChatMistralAI } from \"@langchain/mistralai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst model = new ChatMistralAI({\n  apiKey: process.env.MISTRAL_API_KEY,\n  model: \"mistral-small\",\n});\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"human\", \"{input}\"],\n]);\nconst chain = prompt.pipe(model);\nconst response = await chain.invoke({\n  input: \"Hello\",\n});\nconsole.log(\"response\", response);\n/**\nresponse AIMessage {\n  lc_namespace: [ 'langchain_core', 'messages' ],\n  content: \"Hello! I'm here to help answer any questions you might have or provide information on a variety of topics. How can I assist you today?\\n\" +\n    '\\n' +\n    'Here are some common tasks I can help with:\\n' +\n    '\\n' +\n    '* Setting alarms or reminders\\n' +\n    '* Sending emails or messages\\n' +\n    '* Making phone calls\\n' +\n    '* Providing weather information\\n' +\n    '* Creating to-do lists\\n' +\n    '* Offering suggestions for restaurants, movies, or other local activities\\n' +\n    '* Providing definitions and explanations for words or concepts\\n' +\n    '* Translating text into different languages\\n' +\n    '* Playing music or podcasts\\n' +\n    '* Setting timers\\n' +\n    '* Providing directions or traffic information\\n' +\n    '* And much more!\\n' +\n    '\\n' +\n    \"Let me know how I can help you specifically, and I'll do my best to make your day easier and more productive!\\n\" +\n    '\\n' +\n    'Best regards,\\n' +\n    'Your helpful assistant.',\n  name: undefined,\n  additional_kwargs: {}\n}\n */\n",imports:[{local:"ChatMistralAI",imported:"ChatMistralAI",source:"@langchain/mistralai"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"}]}},2741:n=>{n.exports={content:'import { z } from "zod";\n\nimport { ChatMistralAI } from "@langchain/mistralai";\nimport { DynamicStructuredTool } from "@langchain/core/tools";\nimport { AgentExecutor, createToolCallingAgent } from "langchain/agents";\n\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\nconst llm = new ChatMistralAI({\n  temperature: 0,\n  model: "mistral-large-latest",\n});\n\n// Prompt template must have "input" and "agent_scratchpad input variables"\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", "You are a helpful assistant"],\n  ["placeholder", "{chat_history}"],\n  ["human", "{input}"],\n  ["placeholder", "{agent_scratchpad}"],\n]);\n\nconst currentWeatherTool = new DynamicStructuredTool({\n  name: "get_current_weather",\n  description: "Get the current weather in a given location",\n  schema: z.object({\n    location: z.string().describe("The city and state, e.g. San Francisco, CA"),\n  }),\n  func: async () => Promise.resolve("28 \xb0C"),\n});\n\nconst agent = await createToolCallingAgent({\n  llm,\n  tools: [currentWeatherTool],\n  prompt,\n});\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools: [currentWeatherTool],\n});\n\nconst input = "What\'s the weather like in Paris?";\nconst { output } = await agentExecutor.invoke({ input });\n\nconsole.log(output);\n\n/* \n  The current weather in Paris is 28 \xb0C.\n*/\n',imports:[{local:"ChatMistralAI",imported:"ChatMistralAI",source:"@langchain/mistralai"},{local:"DynamicStructuredTool",imported:"DynamicStructuredTool",source:"@langchain/core/tools"},{local:"AgentExecutor",imported:"AgentExecutor",source:"langchain/agents"},{local:"createToolCallingAgent",imported:"createToolCallingAgent",source:"langchain/agents"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"}]}},52348:n=>{n.exports={content:'import { ChatMistralAI } from "@langchain/mistralai";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { JsonOutputKeyToolsParser } from "@langchain/core/output_parsers/openai_tools";\nimport { z } from "zod";\nimport { StructuredTool } from "@langchain/core/tools";\n\nconst calculatorSchema = z.object({\n  operation: z\n    .enum(["add", "subtract", "multiply", "divide"])\n    .describe("The type of operation to execute."),\n  number1: z.number().describe("The first number to operate on."),\n  number2: z.number().describe("The second number to operate on."),\n});\n\n// Extend the StructuredTool class to create a new tool\nclass CalculatorTool extends StructuredTool {\n  name = "calculator";\n\n  description = "A simple calculator tool";\n\n  schema = calculatorSchema;\n\n  async _call(input: z.infer<typeof calculatorSchema>) {\n    return JSON.stringify(input);\n  }\n}\n\n// Or you can convert the tool to a JSON schema using\n// a library like zod-to-json-schema\n// Uncomment the lines below to use tools this way.\n// import { zodToJsonSchema } from "zod-to-json-schema";\n// const calculatorJsonSchema = zodToJsonSchema(calculatorSchema);\n\nconst model = new ChatMistralAI({\n  apiKey: process.env.MISTRAL_API_KEY,\n  model: "mistral-large",\n});\n\n// Bind the tool to the model\nconst modelWithTool = model.bind({\n  tools: [new CalculatorTool()],\n});\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    "You are a helpful assistant who always needs to use a calculator.",\n  ],\n  ["human", "{input}"],\n]);\n\n// Define an output parser that can handle tool responses\nconst outputParser = new JsonOutputKeyToolsParser({\n  keyName: "calculator",\n  returnSingle: true,\n});\n\n// Chain your prompt, model, and output parser together\nconst chain = prompt.pipe(modelWithTool).pipe(outputParser);\n\nconst response = await chain.invoke({\n  input: "What is 2 + 2?",\n});\nconsole.log(response);\n/*\n{ operation: \'add\', number1: 2, number2: 2 }\n */\n',imports:[{local:"ChatMistralAI",imported:"ChatMistralAI",source:"@langchain/mistralai"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"JsonOutputKeyToolsParser",imported:"JsonOutputKeyToolsParser",source:"@langchain/core/output_parsers/openai_tools"},{local:"StructuredTool",imported:"StructuredTool",source:"@langchain/core/tools"}]}},53198:n=>{n.exports={content:'import { ChatMistralAI } from "@langchain/mistralai";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { z } from "zod";\n\nconst calculatorSchema = z\n  .object({\n    operation: z\n      .enum(["add", "subtract", "multiply", "divide"])\n      .describe("The type of operation to execute."),\n    number1: z.number().describe("The first number to operate on."),\n    number2: z.number().describe("The second number to operate on."),\n  })\n  .describe("A simple calculator tool");\n\nconst model = new ChatMistralAI({\n  apiKey: process.env.MISTRAL_API_KEY,\n  model: "mistral-large",\n});\n\n// Pass the schema and tool name to the withStructuredOutput method\nconst modelWithTool = model.withStructuredOutput(calculatorSchema);\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    "You are a helpful assistant who always needs to use a calculator.",\n  ],\n  ["human", "{input}"],\n]);\n\n// Chain your prompt and model together\nconst chain = prompt.pipe(modelWithTool);\n\nconst response = await chain.invoke({\n  input: "What is 2 + 2?",\n});\nconsole.log(response);\n/*\n  { operation: \'add\', number1: 2, number2: 2 }\n*/\n\n/**\n * You can supply a "name" field to give the LLM additional context\n * around what you are trying to generate. You can also pass\n * \'includeRaw\' to get the raw message back from the model too.\n */\nconst includeRawModel = model.withStructuredOutput(calculatorSchema, {\n  name: "calculator",\n  includeRaw: true,\n});\nconst includeRawChain = prompt.pipe(includeRawModel);\n\nconst includeRawResponse = await includeRawChain.invoke({\n  input: "What is 2 + 2?",\n});\nconsole.log(JSON.stringify(includeRawResponse, null, 2));\n/*\n  {\n    "raw": {\n      "kwargs": {\n        "content": "",\n        "additional_kwargs": {\n          "tool_calls": [\n            {\n              "id": "null",\n              "type": "function",\n              "function": {\n                "name": "calculator",\n                "arguments": "{\\"operation\\": \\"add\\", \\"number1\\": 2, \\"number2\\": 2}"\n              }\n            }\n          ]\n        }\n      }\n    },\n    "parsed": {\n      "operation": "add",\n      "number1": 2,\n      "number2": 2\n    }\n  }\n*/\n',imports:[{local:"ChatMistralAI",imported:"ChatMistralAI",source:"@langchain/mistralai"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"}]}},49971:n=>{n.exports={content:'import { ChatMistralAI } from "@langchain/mistralai";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\nconst calculatorJsonSchema = {\n  type: "object",\n  properties: {\n    operation: {\n      type: "string",\n      enum: ["add", "subtract", "multiply", "divide"],\n      description: "The type of operation to execute.",\n    },\n    number1: { type: "number", description: "The first number to operate on." },\n    number2: {\n      type: "number",\n      description: "The second number to operate on.",\n    },\n  },\n  required: ["operation", "number1", "number2"],\n  description: "A simple calculator tool",\n};\n\nconst model = new ChatMistralAI({\n  apiKey: process.env.MISTRAL_API_KEY,\n  model: "mistral-large",\n});\n\n// Pass the schema and tool name to the withStructuredOutput method\nconst modelWithTool = model.withStructuredOutput(calculatorJsonSchema);\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    "You are a helpful assistant who always needs to use a calculator.",\n  ],\n  ["human", "{input}"],\n]);\n\n// Chain your prompt and model together\nconst chain = prompt.pipe(modelWithTool);\n\nconst response = await chain.invoke({\n  input: "What is 2 + 2?",\n});\nconsole.log(response);\n/*\n  { operation: \'add\', number1: 2, number2: 2 }\n*/\n',imports:[{local:"ChatMistralAI",imported:"ChatMistralAI",source:"@langchain/mistralai"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"}]}},65505:n=>{n.exports={content:'import { ChatMistralAI } from "@langchain/mistralai";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\n\nconst model = new ChatMistralAI({\n  apiKey: process.env.MISTRAL_API_KEY,\n  model: "mistral-small",\n});\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", "You are a helpful assistant"],\n  ["human", "{input}"],\n]);\nconst outputParser = new StringOutputParser();\nconst chain = prompt.pipe(model).pipe(outputParser);\nconst response = await chain.stream({\n  input: "Hello",\n});\nfor await (const item of response) {\n  console.log("stream item:", item);\n}\n/**\nstream item:\nstream item: Hello! I\'m here to help answer any questions you\nstream item:  might have or assist you with any task you\'d like to\nstream item:  accomplish. I can provide information\nstream item:  on a wide range of topics\nstream item: , from math and science to history and literature. I can\nstream item:  also help you manage your schedule, set reminders, and\nstream item:  much more. Is there something specific you need help with? Let\nstream item:  me know!\nstream item:\n */\n',imports:[{local:"ChatMistralAI",imported:"ChatMistralAI",source:"@langchain/mistralai"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"StringOutputParser",imported:"StringOutputParser",source:"@langchain/core/output_parsers"}]}}}]);