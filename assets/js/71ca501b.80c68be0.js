"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[381],{24108:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>c,toc:()=>h});var s=t(74848),a=t(28453),r=t(63142);const o={sidebar_class_name:"hidden",sidebar_position:3,title:"How to return structured data from a model"},i=void 0,c={id:"how_to/structured_output",title:"How to return structured data from a model",description:"It is often useful to have a model return output that matches some",source:"@site/docs/how_to/structured_output.mdx",sourceDirName:"how_to",slug:"/how_to/structured_output",permalink:"/docs/how_to/structured_output",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/structured_output.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_class_name:"hidden",sidebar_position:3,title:"How to return structured data from a model"},sidebar:"tutorialSidebar",previous:{title:"How to use output parsers to parse an LLM response into structured format",permalink:"/docs/how_to/output_parser_structured"},next:{title:"How to add ad-hoc tool calling capability to LLMs and Chat Models",permalink:"/docs/how_to/tools_prompting"}},l={},h=[{value:"The <code>.withStructuredOutput()</code> method",id:"the-.withstructuredoutput-method",level:2},{value:"Specifying the output method (Advanced)",id:"specifying-the-output-method-advanced",level:3},{value:"Prompting techniques",id:"prompting-techniques",level:2},{value:"Using <code>JsonOutputParser</code>",id:"using-jsonoutputparser",level:3},{value:"Custom Parsing",id:"custom-parsing",level:3},{value:"Next steps",id:"next-steps",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"It is often useful to have a model return output that matches some\nspecific schema. One common use-case is extracting data from arbitrary\ntext to insert into a traditional database or use with some other\ndownstrem system. This guide will show you a few different strategies\nyou can use to do this."}),"\n",(0,s.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,s.jsx)(n.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"../../docs/concepts/#chat-models",children:"Chat models"})}),"\n"]})]}),"\n",(0,s.jsxs)(n.h2,{id:"the-.withstructuredoutput-method",children:["The ",(0,s.jsx)(n.code,{children:".withStructuredOutput()"})," method"]}),"\n",(0,s.jsxs)(n.p,{children:["There are several strategies that models can use under the hood. For\nsome of the most popular model providers, including\n",(0,s.jsx)(n.a,{href:"../../docs/integrations/platforms/anthropic/",children:"Anthropic"}),", ",(0,s.jsx)(n.a,{href:"../../docs/integrations/platforms/google/",children:"Google\nVertexAI"}),",\n",(0,s.jsx)(n.a,{href:"../../docs/integrations/chat/mistral/",children:"Mistral"}),", and\n",(0,s.jsx)(n.a,{href:"../../docs/integrations/platforms/openai/",children:"OpenAI"})," LangChain implements\na common interface that abstracts away these strategies called\n",(0,s.jsx)(n.code,{children:".withStructuredOutput"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["By invoking this method (and passing in ",(0,s.jsx)(n.a,{href:"https://json-schema.org/",children:"JSON\nschema"})," or a ",(0,s.jsx)(n.a,{href:"https://zod.dev/",children:"Zod schema"}),")\nthe model will add whatever model parameters + output parsers are\nnecessary to get back structured output matching the requested schema.\nIf the model supports more than one way to do this (e.g., function\ncalling vs JSON mode) - you can configure which method to use by passing\ninto that method."]}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s look at some examples of this in action! We\u2019ll use Zod to create a\nsimple response schema."}),"\n","\n",(0,s.jsx)(r.A,{onlyWso:!0}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { z } from "zod";\n\nconst joke = z.object({\n  setup: z.string().describe("The setup of the joke"),\n  punchline: z.string().describe("The punchline to the joke"),\n  rating: z.number().optional().describe("How funny the joke is, from 1 to 10"),\n});\n\nconst structuredLlm = model.withStructuredOutput(joke);\n\nawait structuredLlm.invoke("Tell me a joke about cats");\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'{\n  setup: "Why don\'t cats play poker in the wild?",\n  punchline: "Too many cheetahs.",\n  rating: 7\n}\n'})}),"\n",(0,s.jsxs)(n.p,{children:["One key point is that though we set our Zod schema as a variable named\n",(0,s.jsx)(n.code,{children:"joke"}),", Zod is not able to access that variable name, and therefore\ncannot pass it to the model. Though it is not required, we can pass a\nname for our schema in order to give the model additional context as to\nwhat our schema represents, improving performance:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const structuredLlm = model.withStructuredOutput(joke, { name: "joke" });\n\nawait structuredLlm.invoke("Tell me a joke about cats");\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'{\n  setup: "Why don\'t cats play poker in the wild?",\n  punchline: "Too many cheetahs!",\n  rating: 7\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"The result is a JSON object."}),"\n",(0,s.jsx)(n.p,{children:"We can also pass in an OpenAI-style JSON schema dict if you prefer not\nto use Zod. This object should contain three properties:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"name"}),": The name of the schema to output."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"description"}),": A high level description of the schema to output."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"parameters"}),": The nested details of the schema you want to extract,\nformatted as a ",(0,s.jsx)(n.a,{href:"https://json-schema.org/",children:"JSON schema"})," dict."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"In this case, the response is also a dict:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const structuredLlm = model.withStructuredOutput({\n  name: "joke",\n  description: "Joke to tell user.",\n  parameters: {\n    title: "Joke",\n    type: "object",\n    properties: {\n      setup: { type: "string", description: "The setup for the joke" },\n      punchline: { type: "string", description: "The joke\'s punchline" },\n    },\n    required: ["setup", "punchline"],\n  },\n});\n\nawait structuredLlm.invoke("Tell me a joke about cats");\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'{\n  setup: "Why was the cat sitting on the computer?",\n  punchline: "Because it wanted to keep an eye on the mouse!"\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"If you are using JSON Schema, you can take advantage of other more\ncomplex schema descriptions to create a similar effect."}),"\n",(0,s.jsxs)(n.p,{children:["You can also use tool calling directly to allow the model to choose\nbetween options, if your chosen model supports it. This involves a bit\nmore parsing and setup. See ",(0,s.jsx)(n.a,{href:"../../docs/how_to/tool_calling/",children:"this how-to\nguide"})," for more details."]}),"\n",(0,s.jsx)(n.h3,{id:"specifying-the-output-method-advanced",children:"Specifying the output method (Advanced)"}),"\n",(0,s.jsx)(n.p,{children:"For models that support more than one means of outputting data, you can\nspecify the preferred one like this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const structuredLlm = model.withStructuredOutput(joke, {\n  method: "json_mode",\n  name: "joke",\n});\n\nawait structuredLlm.invoke(\n  "Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys"\n);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'{\n  setup: "Why don\'t cats play poker in the jungle?",\n  punchline: "Too many cheetahs!"\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"In the above example, we use OpenAI\u2019s alternate JSON mode capability\nalong with a more specific prompt."}),"\n",(0,s.jsxs)(n.p,{children:["For specifics about the model you choose, peruse its entry in the ",(0,s.jsx)(n.a,{href:"https://v02.api.js.langchain.com/",children:"API\nreference pages"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"prompting-techniques",children:"Prompting techniques"}),"\n",(0,s.jsxs)(n.p,{children:["You can also prompt models to outputting information in a given format.\nThis approach relies on designing good prompts and then parsing the\noutput of the models. This is the only option for models that don\u2019t\nsupport ",(0,s.jsx)(n.code,{children:".with_structured_output()"})," or other built-in approaches."]}),"\n",(0,s.jsxs)(n.h3,{id:"using-jsonoutputparser",children:["Using ",(0,s.jsx)(n.code,{children:"JsonOutputParser"})]}),"\n",(0,s.jsxs)(n.p,{children:["The following example uses the built-in\n",(0,s.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_output_parsers.JsonOutputParser.html",children:(0,s.jsx)(n.code,{children:"JsonOutputParser"})}),"\nto parse the output of a chat model prompted to match a the given JSON\nschema. Note that we are adding ",(0,s.jsx)(n.code,{children:"format_instructions"})," directly to the\nprompt from a method on the parser:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { JsonOutputParser } from "@langchain/core/output_parsers";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\ntype Person = {\n  name: string;\n  height_in_meters: number;\n};\n\ntype People = {\n  people: Person[];\n};\n\nconst formatInstructions = `Respond only in valid JSON. The JSON object you return should match the following schema:\n{{ people: [{{ name: "string", height_in_meters: "number" }}] }}\n\nWhere people is an array of objects, each with a name and height_in_meters field.\n`;\n\n// Set up a parser\nconst parser = new JsonOutputParser<People>();\n\n// Prompt\nconst prompt = await ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    "Answer the user query. Wrap the output in `json` tags\\n{format_instructions}",\n  ],\n  ["human", "{query}"],\n]).partial({\n  format_instructions: formatInstructions,\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s take a look at what information is sent to the model:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const query = "Anna is 23 years old and she is 6 feet tall";\n\nconsole.log((await prompt.format({ query })).toString());\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'System: Answer the user query. Wrap the output in `json` tags\nRespond only in valid JSON. The JSON object you return should match the following schema:\n{{ people: [{{ name: "string", height_in_meters: "number" }}] }}\n\nWhere people is an array of objects, each with a name and height_in_meters field.\n\nHuman: Anna is 23 years old and she is 6 feet tall\n'})}),"\n",(0,s.jsx)(n.p,{children:"And now let\u2019s invoke it:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"const chain = prompt.pipe(model).pipe(parser);\n\nawait chain.invoke({ query });\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'{ people: [ { name: "Anna", height_in_meters: 1.83 } ] }\n'})}),"\n",(0,s.jsxs)(n.p,{children:["For a deeper dive into using output parsers with prompting techniques\nfor structured output, see ",(0,s.jsx)(n.a,{href:"../../docs/how_to/output_parser_structured",children:"this\nguide"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"custom-parsing",children:"Custom Parsing"}),"\n",(0,s.jsxs)(n.p,{children:["You can also create a custom prompt and parser with ",(0,s.jsx)(n.a,{href:"../../docs/concepts/#langchain-expression-language",children:"LangChain\nExpression Language\n(LCEL)"}),", using a\nplain function to parse the output from the model:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { AIMessage } from "@langchain/core/messages";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\ntype Person = {\n  name: string;\n  height_in_meters: number;\n};\n\ntype People = {\n  people: Person[];\n};\n\nconst schema = `{ people: [{{ name: "string", height_in_meters: "number" }] }}`;\n\n// Prompt\nconst prompt = await ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    `Answer the user query. Output your answer as JSON that\nmatches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`.\nMake sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags`,\n  ],\n  ["human", "{query}"],\n]).partial({\n  schema,\n});\n\n/**\n * Custom extractor\n *\n * Extracts JSON content from a string where\n * JSON is embedded between ```json and ``` tags.\n */\nconst extractJson = (output: AIMessage): Array<People> => {\n  const text = output.content as string;\n  // Define the regular expression pattern to match JSON blocks\n  const pattern = /```json(.*?)```/gs;\n\n  // Find all non-overlapping matches of the pattern in the string\n  const matches = text.match(pattern);\n\n  // Process each match, attempting to parse it as JSON\n  try {\n    return (\n      matches?.map((match) => {\n        // Remove the markdown code block syntax to isolate the JSON string\n        const jsonStr = match.replace(/```json|```/g, "").trim();\n        return JSON.parse(jsonStr);\n      }) ?? []\n    );\n  } catch (error) {\n    throw new Error(`Failed to parse: ${output}`);\n  }\n};\n'})}),"\n",(0,s.jsx)(n.p,{children:"Here is the prompt sent to the model:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const query = "Anna is 23 years old and she is 6 feet tall";\n\nconsole.log((await prompt.format({ query })).toString());\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'System: Answer the user query. Output your answer as JSON that\nmatches the given schema: ```json\n{{ people: [{{ name: "string", height_in_meters: "number" }}] }}\n```.\nMake sure to wrap the answer in ```json and ``` tags\nHuman: Anna is 23 years old and she is 6 feet tall\n'})}),"\n",(0,s.jsx)(n.p,{children:"And here\u2019s what it looks like when we invoke it:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { RunnableLambda } from "@langchain/core/runnables";\n\nconst chain = prompt\n  .pipe(model)\n  .pipe(new RunnableLambda({ func: extractJson }));\n\nawait chain.invoke({ query });\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'[\n  { people: [ { name: "Anna", height_in_meters: 1.83 } ] }\n]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,s.jsx)(n.p,{children:"Now you\u2019ve learned a few methods to make a model output structured data."}),"\n",(0,s.jsx)(n.p,{children:"To learn more, check out the other how-to guides in this section, or the\nconceptual guide on tool calling."})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},63142:(e,n,t)=>{t.d(n,{A:()=>d});t(96540);var s=t(11470),a=t(19365),r=t(21432),o=t(27846),i=t(27293),c=t(74848);function l(e){let{children:n}=e;return(0,c.jsxs)(c.Fragment,{children:[(0,c.jsx)(i.A,{type:"tip",children:(0,c.jsxs)("p",{children:["See"," ",(0,c.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,c.jsx)(o.A,{children:n})]})}const h={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},p=["openai","anthropic","mistral","groq","vertex"];function d(e){const{customVarName:n,additionalDependencies:t}=e,o=n??"model",i=e.openaiParams??h.openaiParams,d=e.anthropicParams??h.anthropicParams,u=e.fireworksParams??h.fireworksParams,m=e.mistralParams??h.mistralParams,g=e.groqParams??h.groqParams,x=e.vertexParams??h.vertexParams,j=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],f={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${o} = new ChatOpenAI(${i});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${o} = new ChatAnthropic(${d});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${o} = new ChatFireworks(${u});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${o} = new ChatMistralAI(${m});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${o} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${o} = new ChatVertexAI(${x});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},w=(e.onlyWso?p:j).map((e=>f[e]));return(0,c.jsxs)("div",{children:[(0,c.jsx)("h3",{children:"\uc0ac\uc6a9\ud560 \ucc44\ud305 \ubaa8\ub378 \uc120\ud0dd:"}),(0,c.jsx)(s.A,{groupId:"modelTabs",children:w.map((e=>(0,c.jsxs)(a.A,{value:e.value,label:e.label,children:[(0,c.jsx)("h4",{children:"\uc758\uc874\uc131 \ucd94\uac00"}),(0,c.jsx)(l,{children:[e.dependencies,t].join(" ")}),(0,c.jsx)("h4",{children:"\ud658\uacbd\ubcc0\uc218 \ucd94\uac00"}),(0,c.jsx)(r.A,{language:"bash",children:e.envs}),(0,c.jsx)("h4",{children:"\ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654"}),(0,c.jsx)(r.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var s=t(11470),a=t(19365),r=t(21432),o=t(74848);function i(e){let{children:n}=e;return(0,o.jsxs)(s.A,{groupId:"npm2yarn",children:[(0,o.jsx)(a.A,{value:"npm",label:"npm",children:(0,o.jsxs)(r.A,{language:"bash",children:["npm i ",n]})}),(0,o.jsx)(a.A,{value:"yarn",label:"yarn",default:!0,children:(0,o.jsxs)(r.A,{language:"bash",children:["yarn add ",n]})}),(0,o.jsx)(a.A,{value:"pnpm",label:"pnpm",children:(0,o.jsxs)(r.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);