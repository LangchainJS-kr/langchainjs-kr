"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8958],{23549:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>l,metadata:()=>r,toc:()=>m});var t=a(74848),s=a(28453),o=a(63142);const l={sidebar_class_name:"hidden",sidebar_position:2,title:"How to use few shot examples in chat models"},i=void 0,r={id:"how_to/few_shot_examples_chat",title:"How to use few shot examples in chat models",description:"This guide covers how to prompt a chat model with example inputs and",source:"@site/docs/how_to/few_shot_examples_chat.mdx",sourceDirName:"how_to",slug:"/how_to/few_shot_examples_chat",permalink:"/docs/how_to/few_shot_examples_chat",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/few_shot_examples_chat.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_class_name:"hidden",sidebar_position:2,title:"How to use few shot examples in chat models"},sidebar:"tutorialSidebar",previous:{title:"How to embed text data",permalink:"/docs/how_to/embed_text"},next:{title:"How to cache model responses",permalink:"/docs/how_to/llm_caching"}},c={},m=[{value:"Fixed Examples",id:"fixed-examples",level:2},{value:"Dynamic few-shot prompting",id:"dynamic-few-shot-prompting",level:2},{value:"Create the <code>exampleSelector</code>",id:"create-the-exampleselector",level:3},{value:"Create prompt template",id:"create-prompt-template",level:3},{value:"Use with an chat model",id:"use-with-an-chat-model",level:3},{value:"Next steps",id:"next-steps",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"This guide covers how to prompt a chat model with example inputs and\noutputs. Providing the model with a few such examples is called\nfew-shotting, and is a simple yet powerful way to guide generation and\nin some cases drastically improve model performance."}),"\n",(0,t.jsxs)(n.p,{children:["There does not appear to be solid consensus on how best to do few-shot\nprompting, and the optimal prompt compilation will likely vary by model.\nBecause of this, we provide few-shot prompt templates like the\n",(0,t.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_prompts.FewShotChatMessagePromptTemplate.html",children:"FewShotChatMessagePromptTemplate"}),"\nas a flexible starting point, and you can modify or replace them as you\nsee fit."]}),"\n",(0,t.jsx)(n.p,{children:"The goal of few-shot prompt templates are to dynamically select examples\nbased on an input, and then format the examples in a final prompt to\nprovide for the model."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," The following code examples are for chat models only, since\n",(0,t.jsx)(n.code,{children:"FewShotChatMessagePromptTemplates"})," are designed to output formatted\n",(0,t.jsx)(n.a,{href:"../../docs/concepts/#message-types",children:"chat messages"})," rather than pure\nstrings. For similar few-shot prompt examples for pure string templates\ncompatible with completion models (LLMs), see the ",(0,t.jsx)(n.a,{href:"../../docs/how_to/few_shot_examples/",children:"few-shot prompt\ntemplates"})," guide."]}),"\n",(0,t.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,t.jsx)(n.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#prompt-templates",children:"Prompt templates"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#example-selectors",children:"Example selectors"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#chat-model",children:"Chat models"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#vectorstores",children:"Vectorstores"})}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"fixed-examples",children:"Fixed Examples"}),"\n",(0,t.jsx)(n.p,{children:"The most basic (and common) few-shot prompting technique is to use fixed\nprompt examples. This way you can select a chain, evaluate it, and avoid\nworrying about additional moving parts in production."}),"\n",(0,t.jsxs)(n.p,{children:["The basic components of the template are: - ",(0,t.jsx)(n.code,{children:"examples"}),": An array of\nobject examples to include in the final prompt. - ",(0,t.jsx)(n.code,{children:"examplePrompt"}),":\nconverts each example into 1 or more messages through its\n",(0,t.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_prompts.FewShotChatMessagePromptTemplate.html#formatMessages",children:(0,t.jsx)(n.code,{children:"formatMessages"})}),"\nmethod. A common example would be to convert each example into one human\nmessage and one AI message response, or a human message followed by a\nfunction call message."]}),"\n",(0,t.jsx)(n.p,{children:"Below is a simple demonstration. First, define the examples you\u2019d like\nto include:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import {\n  ChatPromptTemplate,\n  FewShotChatMessagePromptTemplate,\n} from "@langchain/core/prompts";\n\nconst examples = [\n  { input: "2+2", output: "4" },\n  { input: "2+3", output: "5" },\n];\n'})}),"\n",(0,t.jsx)(n.p,{children:"Next, assemble them into the few-shot prompt template."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'// This is a prompt template used to format each individual example.\nconst examplePrompt = ChatPromptTemplate.fromMessages([\n  ["human", "{input}"],\n  ["ai", "{output}"],\n]);\nconst fewShotPrompt = new FewShotChatMessagePromptTemplate({\n  examplePrompt,\n  examples,\n  inputVariables: [], // no input variables\n});\n\nconst result = await fewShotPrompt.invoke({});\nconsole.log(result.toChatMessages());\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'[\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "2+2", additional_kwargs: {}, response_metadata: {} },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "2+2",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "4",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "4",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {},\n    tool_calls: [],\n    invalid_tool_calls: []\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "2+3", additional_kwargs: {}, response_metadata: {} },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "2+3",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "5",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "5",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {},\n    tool_calls: [],\n    invalid_tool_calls: []\n  }\n]\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Finally, we assemble the final prompt as shown below, passing\n",(0,t.jsx)(n.code,{children:"fewShotPrompt"})," directly into the ",(0,t.jsx)(n.code,{children:"fromMessages"})," factory method, and use\nit with a model:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const finalPrompt = ChatPromptTemplate.fromMessages([\n  ["system", "You are a wondrous wizard of math."],\n  fewShotPrompt,\n  ["human", "{input}"],\n]);\n'})}),"\n","\n",(0,t.jsx)(o.A,{}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const chain = finalPrompt.pipe(model);\n\nawait chain.invoke({ input: "What\'s the square of a triangle?" });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "A triangle does not have a square. The square of a number is the result of multiplying the number by"... 8 more characters,\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "A triangle does not have a square. The square of a number is the result of multiplying the number by"... 8 more characters,\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 23, promptTokens: 52, totalTokens: 75 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"dynamic-few-shot-prompting",children:"Dynamic few-shot prompting"}),"\n",(0,t.jsxs)(n.p,{children:["Sometimes you may want to select only a few examples from your overall\nset to show based on the input. For this, you can replace the ",(0,t.jsx)(n.code,{children:"examples"}),"\npassed into ",(0,t.jsx)(n.code,{children:"FewShotChatMessagePromptTemplate"})," with an\n",(0,t.jsx)(n.code,{children:"exampleSelector"}),". The other components remain the same as above! Our\ndynamic few-shot prompt template would look like:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"exampleSelector"}),": responsible for selecting few-shot examples (and\nthe order in which they are returned) for a given input. These\nimplement the\n",(0,t.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_example_selectors.BaseExampleSelector.html",children:"BaseExampleSelector"}),"\ninterface. A common example is the vectorstore-backed\n",(0,t.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_example_selectors.SemanticSimilarityExampleSelector.html",children:"SemanticSimilarityExampleSelector"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"examplePrompt"}),": convert each example into 1 or more messages\nthrough its\n",(0,t.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_prompts.FewShotChatMessagePromptTemplate.html#formatMessages",children:(0,t.jsx)(n.code,{children:"formatMessages"})}),"\nmethod. A common example would be to convert each example into one\nhuman message and one AI message response, or a human message\nfollowed by a function call message."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These once again can be composed with other messages and chat templates\nto assemble your final prompt."}),"\n",(0,t.jsxs)(n.p,{children:["Let\u2019s walk through an example with the\n",(0,t.jsx)(n.code,{children:"SemanticSimilarityExampleSelector"}),". Since this implementation uses a\nvectorstore to select examples based on semantic similarity, we will\nwant to first populate the store. Since the basic idea here is that we\nwant to search for and return examples most similar to the text input,\nwe embed the ",(0,t.jsx)(n.code,{children:"values"})," of our prompt examples rather than considering the\nkeys:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { SemanticSimilarityExampleSelector } from "@langchain/core/example_selectors";\nimport { MemoryVectorStore } from "langchain/vectorstores/memory";\nimport { OpenAIEmbeddings } from "@langchain/openai";\n\nconst examples = [\n  { input: "2+2", output: "4" },\n  { input: "2+3", output: "5" },\n  { input: "2+4", output: "6" },\n  { input: "What did the cow say to the moon?", output: "nothing at all" },\n  {\n    input: "Write me a poem about the moon",\n    output:\n      "One for the moon, and one for me, who are we to talk about the moon?",\n  },\n];\n\nconst toVectorize = examples.map(\n  (example) => `${example.input} ${example.output}`\n);\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  toVectorize,\n  examples,\n  embeddings\n);\n'})}),"\n",(0,t.jsxs)(n.h3,{id:"create-the-exampleselector",children:["Create the ",(0,t.jsx)(n.code,{children:"exampleSelector"})]}),"\n",(0,t.jsxs)(n.p,{children:["With a vectorstore created, we can create the ",(0,t.jsx)(n.code,{children:"exampleSelector"}),". Here we\nwill call it in isolation, and set ",(0,t.jsx)(n.code,{children:"k"})," on it to only fetch the two\nexample closest to the input."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const exampleSelector = new SemanticSimilarityExampleSelector({\n  vectorStore,\n  k: 2,\n});\n\n// The prompt template will load examples by passing the input do the `select_examples` method\nawait exampleSelector.selectExamples({ input: "horse" });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'[\n  {\n    input: "What did the cow say to the moon?",\n    output: "nothing at all"\n  },\n  { input: "2+4", output: "6" }\n]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"create-prompt-template",children:"Create prompt template"}),"\n",(0,t.jsxs)(n.p,{children:["We now assemble the prompt template, using the ",(0,t.jsx)(n.code,{children:"exampleSelector"})," created\nabove."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import {\n  ChatPromptTemplate,\n  FewShotChatMessagePromptTemplate,\n} from "@langchain/core/prompts";\n\n// Define the few-shot prompt.\nconst fewShotPrompt = new FewShotChatMessagePromptTemplate({\n  // The input variables select the values to pass to the example_selector\n  inputVariables: ["input"],\n  exampleSelector,\n  // Define how ech example will be formatted.\n  // In this case, each example will become 2 messages:\n  // 1 human, and 1 AI\n  examplePrompt: ChatPromptTemplate.fromMessages([\n    ["human", "{input}"],\n    ["ai", "{output}"],\n  ]),\n});\n\nconst results = await fewShotPrompt.invoke({ input: "What\'s 3+3?" });\nconsole.log(results.toChatMessages());\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'[\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "2+3", additional_kwargs: {}, response_metadata: {} },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "2+3",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "5",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "5",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {},\n    tool_calls: [],\n    invalid_tool_calls: []\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "2+2", additional_kwargs: {}, response_metadata: {} },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "2+2",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "4",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "4",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {},\n    tool_calls: [],\n    invalid_tool_calls: []\n  }\n]\n'})}),"\n",(0,t.jsx)(n.p,{children:"And we can pass this few-shot chat message prompt template into another\nchat prompt template:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const finalPrompt = ChatPromptTemplate.fromMessages([\n  ["system", "You are a wondrous wizard of math."],\n  fewShotPrompt,\n  ["human", "{input}"],\n]);\n\nconst result = await fewShotPrompt.invoke({ input: "What\'s 3+3?" });\nconsole.log(result);\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'ChatPromptValue {\n  lc_serializable: true,\n  lc_kwargs: {\n    messages: [\n      HumanMessage {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "2+3",\n          additional_kwargs: {},\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "2+3",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      AIMessage {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "5",\n          tool_calls: [],\n          invalid_tool_calls: [],\n          additional_kwargs: {},\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "5",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: {},\n        tool_calls: [],\n        invalid_tool_calls: []\n      },\n      HumanMessage {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "2+2",\n          additional_kwargs: {},\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "2+2",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      AIMessage {\n        lc_serializable: true,\n        lc_kwargs: {\n          content: "4",\n          tool_calls: [],\n          invalid_tool_calls: [],\n          additional_kwargs: {},\n          response_metadata: {}\n        },\n        lc_namespace: [ "langchain_core", "messages" ],\n        content: "4",\n        name: undefined,\n        additional_kwargs: {},\n        response_metadata: {},\n        tool_calls: [],\n        invalid_tool_calls: []\n      }\n    ]\n  },\n  lc_namespace: [ "langchain_core", "prompt_values" ],\n  messages: [\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: { content: "2+3", additional_kwargs: {}, response_metadata: {} },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "2+3",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    AIMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "5",\n        tool_calls: [],\n        invalid_tool_calls: [],\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "5",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {},\n      tool_calls: [],\n      invalid_tool_calls: []\n    },\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: { content: "2+2", additional_kwargs: {}, response_metadata: {} },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "2+2",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    AIMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "4",\n        tool_calls: [],\n        invalid_tool_calls: [],\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "4",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {},\n      tool_calls: [],\n      invalid_tool_calls: []\n    }\n  ]\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"use-with-an-chat-model",children:"Use with an chat model"}),"\n",(0,t.jsx)(n.p,{children:"Finally, you can connect your model to the few-shot prompt."}),"\n",(0,t.jsx)(o.A,{customVarName:"model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const chain = finalPrompt.pipe(model);\n\nawait chain.invoke({ input: "What\'s 3+3?" });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "6",\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "6",\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 1, promptTokens: 51, totalTokens: 52 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,t.jsx)(n.p,{children:"You\u2019ve now learned how to add few-shot examples to your chat prompts."}),"\n",(0,t.jsxs)(n.p,{children:["Next, check out the other how-to guides on prompt templates in this\nsection, the related how-to guide on ",(0,t.jsx)(n.a,{href:"../../docs/how_to/few_shot_examples",children:"few shotting with text completion\nmodels"}),", or the other ",(0,t.jsx)(n.a,{href:"../../docs/how_to/example_selectors/",children:"example\nselector how-to guides"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},63142:(e,n,a)=>{a.d(n,{A:()=>p});a(96540);var t=a(11470),s=a(19365),o=a(21432),l=a(27846),i=a(27293),r=a(74848);function c(e){let{children:n}=e;return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.A,{type:"tip",children:(0,r.jsxs)("p",{children:["See"," ",(0,r.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,r.jsx)(l.A,{children:n})]})}const m={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},d=["openai","anthropic","mistral","groq","vertex"];function p(e){const{customVarName:n,additionalDependencies:a}=e,l=n??"model",i=e.openaiParams??m.openaiParams,p=e.anthropicParams??m.anthropicParams,h=e.fireworksParams??m.fireworksParams,u=e.mistralParams??m.mistralParams,g=e.groqParams??m.groqParams,_=e.vertexParams??m.vertexParams,x=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],w={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${l} = new ChatOpenAI(${i});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${l} = new ChatAnthropic(${p});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${l} = new ChatFireworks(${h});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${l} = new ChatMistralAI(${u});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${l} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${l} = new ChatVertexAI(${_});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},f=(e.onlyWso?d:x).map((e=>w[e]));return(0,r.jsxs)("div",{children:[(0,r.jsx)("h3",{children:"\uc0ac\uc6a9\ud560 \ucc44\ud305 \ubaa8\ub378 \uc120\ud0dd:"}),(0,r.jsx)(t.A,{groupId:"modelTabs",children:f.map((e=>(0,r.jsxs)(s.A,{value:e.value,label:e.label,children:[(0,r.jsx)("h4",{children:"\uc758\uc874\uc131 \ucd94\uac00"}),(0,r.jsx)(c,{children:[e.dependencies,a].join(" ")}),(0,r.jsx)("h4",{children:"\ud658\uacbd\ubcc0\uc218 \ucd94\uac00"}),(0,r.jsx)(o.A,{language:"bash",children:e.envs}),(0,r.jsx)("h4",{children:"\ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654"}),(0,r.jsx)(o.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,n,a)=>{a.d(n,{A:()=>i});a(96540);var t=a(11470),s=a(19365),o=a(21432),l=a(74848);function i(e){let{children:n}=e;return(0,l.jsxs)(t.A,{groupId:"npm2yarn",children:[(0,l.jsx)(s.A,{value:"npm",label:"npm",children:(0,l.jsxs)(o.A,{language:"bash",children:["npm i ",n]})}),(0,l.jsx)(s.A,{value:"yarn",label:"yarn",default:!0,children:(0,l.jsxs)(o.A,{language:"bash",children:["yarn add ",n]})}),(0,l.jsx)(s.A,{value:"pnpm",label:"pnpm",children:(0,l.jsxs)(o.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);