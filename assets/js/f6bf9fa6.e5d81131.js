(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8639],{4490:(e,n,s)=>{"use strict";s.r(n),s.d(n,{assets:()=>u,contentTitle:()=>l,default:()=>g,frontMatter:()=>c,metadata:()=>h,toc:()=>p});var t=s(74848),a=s(28453),o=s(64428),r=s(51285),i=s.n(r);const c={},l="Fake LLM",h={id:"integrations/chat/fake",title:"Fake LLM",description:"LangChain provides a fake LLM chat model for testing purposes. This allows you to mock out calls to the LLM and and simulate what would happen if the LLM responded in a certain way.",source:"@site/docs/integrations/chat/fake.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/fake",permalink:"/docs/integrations/chat/fake",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/fake.mdx",tags:[],version:"current",frontMatter:{},sidebar:"integrations",previous:{title:"Cohere",permalink:"/docs/integrations/chat/cohere"},next:{title:"Fireworks",permalink:"/docs/integrations/chat/fireworks"}},u={},p=[{value:"Usage",id:"usage",level:2}];function d(e){const n={h1:"h1",h2:"h2",p:"p",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"fake-llm",children:"Fake LLM"}),"\n",(0,t.jsx)(n.p,{children:"LangChain provides a fake LLM chat model for testing purposes. This allows you to mock out calls to the LLM and and simulate what would happen if the LLM responded in a certain way."}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n","\n",(0,t.jsx)(o.A,{language:"typescript",children:i()})]})}function g(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},51285:e=>{e.exports={content:'import { FakeListChatModel } from "@langchain/core/utils/testing";\nimport { HumanMessage } from "@langchain/core/messages";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\n\n/**\n * The FakeListChatModel can be used to simulate ordered predefined responses.\n */\n\nconst chat = new FakeListChatModel({\n  responses: ["I\'ll callback later.", "You \'console\' them!"],\n});\n\nconst firstMessage = new HumanMessage("You want to hear a JavasSript joke?");\nconst secondMessage = new HumanMessage(\n  "How do you cheer up a JavaScript developer?"\n);\nconst firstResponse = await chat.invoke([firstMessage]);\nconst secondResponse = await chat.invoke([secondMessage]);\n\nconsole.log({ firstResponse });\nconsole.log({ secondResponse });\n\n/**\n * The FakeListChatModel can also be used to simulate streamed responses.\n */\n\nconst stream = await chat\n  .pipe(new StringOutputParser())\n  .stream(`You want to hear a JavasSript joke?`);\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks.join(""));\n\n/**\n * The FakeListChatModel can also be used to simulate delays in either either synchronous or streamed responses.\n */\n\nconst slowChat = new FakeListChatModel({\n  responses: ["Because Oct 31 equals Dec 25", "You \'console\' them!"],\n  sleep: 1000,\n});\n\nconst thirdMessage = new HumanMessage(\n  "Why do programmers always mix up Halloween and Christmas?"\n);\nconst slowResponse = await slowChat.invoke([thirdMessage]);\nconsole.log({ slowResponse });\n\nconst slowStream = await slowChat\n  .pipe(new StringOutputParser())\n  .stream("How do you cheer up a JavaScript developer?");\nconst slowChunks = [];\nfor await (const chunk of slowStream) {\n  slowChunks.push(chunk);\n}\n\nconsole.log(slowChunks.join(""));\n',imports:[{local:"FakeListChatModel",imported:"FakeListChatModel",source:"@langchain/core/utils/testing"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"},{local:"StringOutputParser",imported:"StringOutputParser",source:"@langchain/core/output_parsers"}]}}}]);