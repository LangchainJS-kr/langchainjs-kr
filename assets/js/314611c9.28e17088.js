"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5987],{85432:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>u,contentTitle:()=>c,default:()=>p,frontMatter:()=>l,metadata:()=>d,toc:()=>h});var s=a(74848),r=a(28453),t=a(78847),i=a(27846),o=a(63142);const l={sidebar_class_name:"hidden",title:"How to handle cases where no queries are generated"},c=void 0,d={id:"how_to/query_no_queries",title:"How to handle cases where no queries are generated",description:"This guide assumes familiarity with the following:",source:"@site/docs/how_to/query_no_queries.mdx",sourceDirName:"how_to",slug:"/how_to/query_no_queries",permalink:"/docs/how_to/query_no_queries",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/query_no_queries.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to handle cases where no queries are generated"},sidebar:"tutorialSidebar",previous:{title:"How to handle multiple retrievers",permalink:"/docs/how_to/query_multiple_retrievers"},next:{title:"How to recursively split text by characters",permalink:"/docs/how_to/recursive_text_splitter"}},u={},h=[{value:"Setup",id:"setup",level:2},{value:"Install dependencies",id:"install-dependencies",level:3},...t.toc,{value:"Set environment variables",id:"set-environment-variables",level:3},{value:"Create Index",id:"create-index",level:3},{value:"Query analysis",id:"query-analysis",level:2},{value:"Retrieval with query analysis",id:"retrieval-with-query-analysis",level:2},{value:"Next steps",id:"next-steps",level:2}];function m(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,s.jsx)(n.p,{children:"This guide assumes familiarity with the following:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"../../docs/tutorials/query_analysis",children:"Query analysis"})}),"\n"]})]}),"\n",(0,s.jsx)(n.p,{children:"Sometimes, a query analysis technique may allow for any number of\nqueries to be generated - including no queries! In this case, our\noverall chain will need to inspect the result of the query analysis\nbefore deciding whether to call the retriever or not."}),"\n",(0,s.jsx)(n.p,{children:"We will use mock data for this example."}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsx)(n.h3,{id:"install-dependencies",children:"Install dependencies"}),"\n","\n",(0,s.jsx)(t.default,{}),"\n",(0,s.jsx)(i.A,{children:(0,s.jsx)(n.p,{children:"@langchain/community @langchain/openai zod chromadb"})}),"\n",(0,s.jsx)(n.h3,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"OPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGCHAIN_TRACING_V2=true\n"})}),"\n",(0,s.jsx)(n.h3,{id:"create-index",children:"Create Index"}),"\n",(0,s.jsx)(n.p,{children:"We will create a vectorstore over fake information."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { Chroma } from "@langchain/community/vectorstores/chroma";\nimport { OpenAIEmbeddings } from "@langchain/openai";\nimport "chromadb";\n\nconst texts = ["Harrison worked at Kensho"];\nconst embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });\nconst vectorstore = await Chroma.fromTexts(texts, {}, embeddings, {\n  collectionName: "harrison",\n});\nconst retriever = vectorstore.asRetriever(1);\n'})}),"\n",(0,s.jsx)(n.h2,{id:"query-analysis",children:"Query analysis"}),"\n",(0,s.jsx)(n.p,{children:"We will use function calling to structure the output. However, we will\nconfigure the LLM such that is doesn\u2019t NEED to call the function\nrepresenting a search query (should it decide not to). We will also then\nuse a prompt to do query analysis that explicitly lays when it should\nand shouldn\u2019t make a search."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { z } from "zod";\n\nconst searchSchema = z.object({\n  query: z.string().describe("Similarity search query applied to job record."),\n});\n'})}),"\n","\n",(0,s.jsx)(o.A,{customVarName:"llm"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { zodToJsonSchema } from "zod-to-json-schema";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport {\n  RunnableSequence,\n  RunnablePassthrough,\n} from "@langchain/core/runnables";\n\nconst system = `You have the ability to issue search queries to get information to help answer user information.\n\nYou do not NEED to look things up. If you don\'t need to, then just respond normally.`;\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", system],\n  ["human", "{question}"],\n]);\nconst llmWithTools = llm.bind({\n  tools: [\n    {\n      type: "function" as const,\n      function: {\n        name: "search",\n        description: "Search over a database of job records.",\n        parameters: zodToJsonSchema(searchSchema),\n      },\n    },\n  ],\n});\nconst queryAnalyzer = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llmWithTools,\n]);\n'})}),"\n",(0,s.jsx)(n.p,{children:"We can see that by invoking this we get an message that sometimes - but\nnot always - returns a tool call."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await queryAnalyzer.invoke("where did Harrison work");\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "",\n    additional_kwargs: {\n      function_call: undefined,\n      tool_calls: [\n        {\n          id: "call_uqHm5OMbXBkmqDr7Xzj8EMmd",\n          type: "function",\n          function: [Object]\n        }\n      ]\n    }\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "",\n  name: undefined,\n  additional_kwargs: {\n    function_call: undefined,\n    tool_calls: [\n      {\n        id: "call_uqHm5OMbXBkmqDr7Xzj8EMmd",\n        type: "function",\n        function: { name: "search", arguments: \'{"query":"Harrison"}\' }\n      }\n    ]\n  }\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await queryAnalyzer.invoke("hi!");\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "Hello! How can I assist you today?",\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "Hello! How can I assist you today?",\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"retrieval-with-query-analysis",children:"Retrieval with query analysis"}),"\n",(0,s.jsx)(n.p,{children:"So how would we include this in a chain? Let\u2019s look at an example below."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { JsonOutputKeyToolsParser } from "@langchain/core/output_parsers/openai_tools";\n\nconst outputParser = new JsonOutputKeyToolsParser({\n  keyName: "search",\n});\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables";\n\nconst chain = async (question: string, config?: RunnableConfig) => {\n  const response = await queryAnalyzer.invoke(question, config);\n  if (\n    "tool_calls" in response.additional_kwargs &&\n    response.additional_kwargs.tool_calls !== undefined\n  ) {\n    const query = await outputParser.invoke(response, config);\n    return retriever.invoke(query[0].query, config);\n  } else {\n    return response;\n  }\n};\n\nconst customChain = new RunnableLambda({ func: chain });\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await customChain.invoke("where did Harrison Work");\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'[ Document { pageContent: "Harrison worked at Kensho", metadata: {} } ]\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await customChain.invoke("hi!");\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "Hello! How can I assist you today?",\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "Hello! How can I assist you today?",\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,s.jsx)(n.p,{children:"You\u2019ve now learned some techniques for handling irrelevant questions in\nquery analysis systems."}),"\n",(0,s.jsxs)(n.p,{children:["Next, check out some of the other query analysis guides in this section,\nlike ",(0,s.jsx)(n.a,{href:"../../docs/how_to/query_few_shot",children:"how to use few-shot examples"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},63142:(e,n,a)=>{a.d(n,{A:()=>h});a(96540);var s=a(11470),r=a(19365),t=a(21432),i=a(27846),o=a(27293),l=a(74848);function c(e){let{children:n}=e;return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(o.A,{type:"tip",children:(0,l.jsxs)("p",{children:["See"," ",(0,l.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,l.jsx)(i.A,{children:n})]})}const d={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},u=["openai","anthropic","mistral","groq","vertex"];function h(e){const{customVarName:n,additionalDependencies:a}=e,i=n??"model",o=e.openaiParams??d.openaiParams,h=e.anthropicParams??d.anthropicParams,m=e.fireworksParams??d.fireworksParams,p=e.mistralParams??d.mistralParams,g=e.groqParams??d.groqParams,x=e.vertexParams??d.vertexParams,y=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],f={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${i} = new ChatOpenAI(${o});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${i} = new ChatAnthropic(${h});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${i} = new ChatFireworks(${m});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${i} = new ChatMistralAI(${p});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${i} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${i} = new ChatVertexAI(${x});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},w=(e.onlyWso?u:y).map((e=>f[e]));return(0,l.jsxs)("div",{children:[(0,l.jsx)("h3",{children:"\uc0ac\uc6a9\ud560 \ucc44\ud305 \ubaa8\ub378 \uc120\ud0dd:"}),(0,l.jsx)(s.A,{groupId:"modelTabs",children:w.map((e=>(0,l.jsxs)(r.A,{value:e.value,label:e.label,children:[(0,l.jsx)("h4",{children:"\uc758\uc874\uc131 \ucd94\uac00"}),(0,l.jsx)(c,{children:[e.dependencies,a].join(" ")}),(0,l.jsx)("h4",{children:"\ud658\uacbd\ubcc0\uc218 \ucd94\uac00"}),(0,l.jsx)(t.A,{language:"bash",children:e.envs}),(0,l.jsx)("h4",{children:"\ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654"}),(0,l.jsx)(t.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,n,a)=>{a.d(n,{A:()=>o});a(96540);var s=a(11470),r=a(19365),t=a(21432),i=a(74848);function o(e){let{children:n}=e;return(0,i.jsxs)(s.A,{groupId:"npm2yarn",children:[(0,i.jsx)(r.A,{value:"npm",label:"npm",children:(0,i.jsxs)(t.A,{language:"bash",children:["npm i ",n]})}),(0,i.jsx)(r.A,{value:"yarn",label:"yarn",default:!0,children:(0,i.jsxs)(t.A,{language:"bash",children:["yarn add ",n]})}),(0,i.jsx)(r.A,{value:"pnpm",label:"pnpm",children:(0,i.jsxs)(t.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);