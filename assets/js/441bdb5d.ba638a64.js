"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2283],{76999:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>u});var r=n(74848),s=n(28453),a=n(63142);const o={sidebar_class_name:"hidden",title:"How to parse JSON output"},i=void 0,l={id:"how_to/output_parser_json",title:"How to parse JSON output",description:"While some model providers support [built-in ways to return structured",source:"@site/docs/how_to/output_parser_json.mdx",sourceDirName:"how_to",slug:"/how_to/output_parser_json",permalink:"/docs/how_to/output_parser_json",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/output_parser_json.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to parse JSON output"},sidebar:"tutorialSidebar",previous:{title:"How to try to fix errors in output parsing",permalink:"/docs/how_to/output_parser_fixing"},next:{title:"How to parse XML output",permalink:"/docs/how_to/output_parser_xml"}},p={},u=[{value:"Streaming",id:"streaming",level:2},{value:"Next steps",id:"next-steps",level:2}];function c(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(t.p,{children:["While some model providers support ",(0,r.jsx)(t.a,{href:"../../docs/how_to/structured_output",children:"built-in ways to return structured\noutput"}),", not all do. We can use an\noutput parser to help users to specify an arbitrary JSON schema via the\nprompt, query a model for outputs that conform to that schema, and\nfinally parse that schema as JSON."]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsx)(t.p,{children:"Keep in mind that large language models are leaky abstractions! You\u2019ll\nhave to use an LLM with sufficient capacity to generate well-formed\nJSON."})}),"\n",(0,r.jsxs)(t.admonition,{title:"Prerequisites",type:"info",children:[(0,r.jsx)(t.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"../../docs/concepts/#chat-models",children:"Chat models"})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"../../docs/concepts/#output-parsers",children:"Output parsers"})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"../../docs/concepts/#prompt-templates",children:"Prompt templates"})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"../../docs/how_to/structured_output",children:"Structured output"})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"../../docs/how_to/sequence/",children:"Chaining runnables together"})}),"\n"]})]}),"\n",(0,r.jsxs)(t.p,{children:["The\n",(0,r.jsx)(t.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_output_parsers.JsonOutputParser.html",children:(0,r.jsx)(t.code,{children:"JsonOutputParser"})}),"\nis one built-in option for prompting for and then parsing JSON output."]}),"\n","\n",(0,r.jsx)(a.A,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:'import { ChatOpenAI } from "@langchain/openai";\nconst model = new ChatOpenAI({\n  model: "gpt-4o",\n  temperature: 0,\n});\n\nimport { JsonOutputParser } from "@langchain/core/output_parsers";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\n// Define your desired data structure. Only used for typing the parser output.\ninterface Joke {\n  setup: string;\n  punchline: string;\n}\n\n// A query and format instructions used to prompt a language model.\nconst jokeQuery = "Tell me a joke.";\nconst formatInstructions =\n  "Respond with a valid JSON object, containing two fields: \'setup\' and \'punchline\'.";\n\n// Set up a parser + inject instructions into the prompt template.\nconst parser = new JsonOutputParser<Joke>();\n\nconst prompt = ChatPromptTemplate.fromTemplate(\n  "Answer the user query.\\n{format_instructions}\\n{query}\\n"\n);\n\nconst partialedPrompt = await prompt.partial({\n  format_instructions: formatInstructions,\n});\n\nconst chain = partialedPrompt.pipe(model).pipe(parser);\n\nawait chain.invoke({ query: jokeQuery });\n'})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-text",children:'{\n  setup: "Why don\'t scientists trust atoms?",\n  punchline: "Because they make up everything!"\n}\n'})}),"\n",(0,r.jsx)(t.h2,{id:"streaming",children:"Streaming"}),"\n",(0,r.jsxs)(t.p,{children:["The ",(0,r.jsx)(t.code,{children:"JsonOutputParser"})," also supports streaming partial chunks. This is\nuseful when the model returns partial JSON output in multiple chunks.\nThe parser will keep track of the partial chunks and return the final\nJSON output when the model finishes generating the output."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:"for await (const s of await chain.stream({ query: jokeQuery })) {\n  console.log(s);\n}\n"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-text",children:'{}\n{ setup: "" }\n{ setup: "Why" }\n{ setup: "Why don\'t" }\n{ setup: "Why don\'t scientists" }\n{ setup: "Why don\'t scientists trust" }\n{ setup: "Why don\'t scientists trust atoms" }\n{ setup: "Why don\'t scientists trust atoms?", punchline: "" }\n{ setup: "Why don\'t scientists trust atoms?", punchline: "Because" }\n{\n  setup: "Why don\'t scientists trust atoms?",\n  punchline: "Because they"\n}\n{\n  setup: "Why don\'t scientists trust atoms?",\n  punchline: "Because they make"\n}\n{\n  setup: "Why don\'t scientists trust atoms?",\n  punchline: "Because they make up"\n}\n{\n  setup: "Why don\'t scientists trust atoms?",\n  punchline: "Because they make up everything"\n}\n{\n  setup: "Why don\'t scientists trust atoms?",\n  punchline: "Because they make up everything!"\n}\n'})}),"\n",(0,r.jsx)(t.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,r.jsxs)(t.p,{children:["You\u2019ve now learned one way to prompt a model to return structured JSON.\nNext, check out the ",(0,r.jsx)(t.a,{href:"../../docs/how_to/structured_output",children:"broader guide on obtaining structured\noutput"})," for other techniques."]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},63142:(e,t,n)=>{n.d(t,{A:()=>h});n(96540);var r=n(11470),s=n(19365),a=n(21432),o=n(27846),i=n(27293),l=n(74848);function p(e){let{children:t}=e;return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(i.A,{type:"tip",children:(0,l.jsxs)("p",{children:["See"," ",(0,l.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,l.jsx)(o.A,{children:t})]})}const u={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},c=["openai","anthropic","mistral","groq","vertex"];function h(e){const{customVarName:t,additionalDependencies:n}=e,o=t??"model",i=e.openaiParams??u.openaiParams,h=e.anthropicParams??u.anthropicParams,d=e.fireworksParams??u.fireworksParams,m=e.mistralParams??u.mistralParams,g=e.groqParams??u.groqParams,x=e.vertexParams??u.vertexParams,f=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],j={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${o} = new ChatOpenAI(${i});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${o} = new ChatAnthropic(${h});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${o} = new ChatFireworks(${d});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${o} = new ChatMistralAI(${m});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${o} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${o} = new ChatVertexAI(${x});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},y=(e.onlyWso?c:f).map((e=>j[e]));return(0,l.jsxs)("div",{children:[(0,l.jsx)("h3",{children:"\uc0ac\uc6a9\ud560 \ucc44\ud305 \ubaa8\ub378 \uc120\ud0dd:"}),(0,l.jsx)(r.A,{groupId:"modelTabs",children:y.map((e=>(0,l.jsxs)(s.A,{value:e.value,label:e.label,children:[(0,l.jsx)("h4",{children:"\uc758\uc874\uc131 \ucd94\uac00"}),(0,l.jsx)(p,{children:[e.dependencies,n].join(" ")}),(0,l.jsx)("h4",{children:"\ud658\uacbd\ubcc0\uc218 \ucd94\uac00"}),(0,l.jsx)(a.A,{language:"bash",children:e.envs}),(0,l.jsx)("h4",{children:"\ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654"}),(0,l.jsx)(a.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,t,n)=>{n.d(t,{A:()=>i});n(96540);var r=n(11470),s=n(19365),a=n(21432),o=n(74848);function i(e){let{children:t}=e;return(0,o.jsxs)(r.A,{groupId:"npm2yarn",children:[(0,o.jsx)(s.A,{value:"npm",label:"npm",children:(0,o.jsxs)(a.A,{language:"bash",children:["npm i ",t]})}),(0,o.jsx)(s.A,{value:"yarn",label:"yarn",default:!0,children:(0,o.jsxs)(a.A,{language:"bash",children:["yarn add ",t]})}),(0,o.jsx)(s.A,{value:"pnpm",label:"pnpm",children:(0,o.jsxs)(a.A,{language:"bash",children:["pnpm add ",t]})})]})}}}]);