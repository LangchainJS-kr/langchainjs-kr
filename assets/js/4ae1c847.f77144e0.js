"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4602],{98813:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>r,toc:()=>d});var s=a(74848),t=a(28453),o=a(63142);const i={sidebar_class_name:"hidden",title:"How to manage memory"},l=void 0,r={id:"how_to/chatbots_memory",title:"How to manage memory",description:"This guide assumes familiarity with the following:",source:"@site/docs/how_to/chatbots_memory.mdx",sourceDirName:"how_to",slug:"/how_to/chatbots_memory",permalink:"/docs/how_to/chatbots_memory",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/chatbots_memory.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to manage memory"},sidebar:"tutorialSidebar",previous:{title:"How to split by character",permalink:"/docs/how_to/character_text_splitter"},next:{title:"How to do retrieval",permalink:"/docs/how_to/chatbots_retrieval"}},c={},d=[{value:"Setup",id:"setup",level:2},{value:"Message passing",id:"message-passing",level:2},{value:"Chat history",id:"chat-history",level:2},{value:"Automatic history management",id:"automatic-history-management",level:2},{value:"Modifying chat history",id:"modifying-chat-history",level:2},{value:"Trimming messages",id:"trimming-messages",level:3},{value:"Summary memory",id:"summary-memory",level:3},{value:"Next steps",id:"next-steps",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,s.jsx)(n.p,{children:"This guide assumes familiarity with the following:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"../../docs/tutorials/chatbot",children:"Chatbots"})}),"\n"]})]}),"\n",(0,s.jsx)(n.p,{children:"A key feature of chatbots is their ability to use content of previous\nconversation turns as context. This state management can take several\nforms, including:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Simply stuffing previous messages into a chat model prompt."}),"\n",(0,s.jsx)(n.li,{children:"The above, but trimming old messages to reduce the amount of\ndistracting information the model has to deal with."}),"\n",(0,s.jsx)(n.li,{children:"More complex modifications like synthesizing summaries for long\nrunning conversations."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"We\u2019ll go into more detail on a few techniques below!"}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsx)(n.p,{children:"You\u2019ll need to install a few packages, and set any LLM API keys:"}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s also set up a chat model that we\u2019ll use for the below examples:"}),"\n","\n",(0,s.jsx)(o.A,{}),"\n",(0,s.jsx)(n.h2,{id:"message-passing",children:"Message passing"}),"\n",(0,s.jsx)(n.p,{children:"The simplest form of memory is simply passing chat history messages into\na chain. Here\u2019s an example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { HumanMessage, AIMessage } from "@langchain/core/messages";\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from "@langchain/core/prompts";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    "You are a helpful assistant. Answer all questions to the best of your ability.",\n  ],\n  new MessagesPlaceholder("messages"),\n]);\n\nconst chain = prompt.pipe(llm);\n\nawait chain.invoke({\n  messages: [\n    new HumanMessage(\n      "Translate this sentence from English to French: I love programming."\n    ),\n    new AIMessage("J\'adore la programmation."),\n    new HumanMessage("What did you just say?"),\n  ],\n});\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: `I said "J\'adore la programmation," which means "I love programming" in French.`,\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: `I said "J\'adore la programmation," which means "I love programming" in French.`,\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 21, promptTokens: 61, totalTokens: 82 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"We can see that by passing the previous conversation into a chain, it\ncan use it as context to answer questions. This is the basic concept\nunderpinning chatbot memory - the rest of the guide will demonstrate\nconvenient techniques for passing or reformatting messages."}),"\n",(0,s.jsx)(n.h2,{id:"chat-history",children:"Chat history"}),"\n",(0,s.jsx)(n.p,{children:"It\u2019s perfectly fine to store and pass messages directly as an array, but\nwe can use LangChain\u2019s built-in message history class to store and load\nmessages as well. Instances of this class are responsible for storing\nand loading chat messages from persistent storage. LangChain integrates\nwith many providers but for this demo we will use an ephemeral demo\nclass."}),"\n",(0,s.jsx)(n.p,{children:"Here\u2019s an example of the API:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { ChatMessageHistory } from "langchain/stores/message/in_memory";\n\nconst demoEphemeralChatMessageHistory = new ChatMessageHistory();\n\nawait demoEphemeralChatMessageHistory.addMessage(\n  new HumanMessage(\n    "Translate this sentence from English to French: I love programming."\n  )\n);\n\nawait demoEphemeralChatMessageHistory.addMessage(\n  new AIMessage("J\'adore la programmation.")\n);\n\nawait demoEphemeralChatMessageHistory.getMessages();\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'[\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Translate this sentence from English to French: I love programming.",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Translate this sentence from English to French: I love programming.",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "J\'adore la programmation.",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "J\'adore la programmation.",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {},\n    tool_calls: [],\n    invalid_tool_calls: []\n  }\n]\n'})}),"\n",(0,s.jsx)(n.p,{children:"We can use it directly to store conversation turns for our chain:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await demoEphemeralChatMessageHistory.clear();\n\nconst input1 =\n  "Translate this sentence from English to French: I love programming.";\n\nawait demoEphemeralChatMessageHistory.addMessage(new HumanMessage(input1));\n\nconst response = await chain.invoke({\n  messages: await demoEphemeralChatMessageHistory.getMessages(),\n});\n\nawait demoEphemeralChatMessageHistory.addMessage(response);\n\nconst input2 = "What did I just ask you?";\n\nawait demoEphemeralChatMessageHistory.addMessage(new HumanMessage(input2));\n\nawait chain.invoke({\n  messages: await demoEphemeralChatMessageHistory.getMessages(),\n});\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'You just asked me to translate the sentence "I love programming" from English to French.\',\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: \'You just asked me to translate the sentence "I love programming" from English to French.\',\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 18, promptTokens: 73, totalTokens: 91 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"automatic-history-management",children:"Automatic history management"}),"\n",(0,s.jsxs)(n.p,{children:["The previous examples pass messages to the chain explicitly. This is a\ncompletely acceptable approach, but it does require external management\nof new messages. LangChain also includes an wrapper for LCEL chains that\ncan handle this process automatically called\n",(0,s.jsx)(n.code,{children:"RunnableWithMessageHistory"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["To show how it works, let\u2019s slightly modify the above prompt to take a\nfinal ",(0,s.jsx)(n.code,{children:"input"})," variable that populates a ",(0,s.jsx)(n.code,{children:"HumanMessage"})," template after\nthe chat history. This means that we will expect a ",(0,s.jsx)(n.code,{children:"chat_history"}),"\nparameter that contains all messages BEFORE the current messages instead\nof all messages:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const runnableWithMessageHistoryPrompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    "You are a helpful assistant. Answer all questions to the best of your ability.",\n  ],\n  new MessagesPlaceholder("chat_history"),\n  ["human", "{input}"],\n]);\n\nconst chain2 = runnableWithMessageHistoryPrompt.pipe(llm);\n'})}),"\n",(0,s.jsxs)(n.p,{children:["We\u2019ll pass the latest input to the conversation here and let the\n",(0,s.jsx)(n.code,{children:"RunnableWithMessageHistory"})," class wrap our chain and do the work of\nappending that ",(0,s.jsx)(n.code,{children:"input"})," variable to the chat history."]}),"\n",(0,s.jsx)(n.p,{children:"Next, let\u2019s declare our wrapped chain:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import { RunnableWithMessageHistory } from "@langchain/core/runnables";\n\nconst demoEphemeralChatMessageHistoryForChain = new ChatMessageHistory();\n\nconst chainWithMessageHistory = new RunnableWithMessageHistory({\n  runnable: chain2,\n  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistoryForChain,\n  inputMessagesKey: "input",\n  historyMessagesKey: "chat_history",\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"This class takes a few parameters in addition to the chain that we want\nto wrap:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A factory function that returns a message history for a given\nsession id. This allows your chain to handle multiple users at once\nby loading different messages for different conversations."}),"\n",(0,s.jsxs)(n.li,{children:["An ",(0,s.jsx)(n.code,{children:"inputMessagesKey"})," that specifies which part of the input should\nbe tracked and stored in the chat history. In this example, we want\nto track the string passed in as input."]}),"\n",(0,s.jsxs)(n.li,{children:["A ",(0,s.jsx)(n.code,{children:"historyMessagesKey"})," that specifies what the previous messages\nshould be injected into the prompt as. Our prompt has a\n",(0,s.jsx)(n.code,{children:"MessagesPlaceholder"})," named ",(0,s.jsx)(n.code,{children:"chat_history"}),", so we specify this\nproperty to match. (For chains with multiple outputs) an\n",(0,s.jsx)(n.code,{children:"outputMessagesKey"})," which specifies which output to store as\nhistory. This is the inverse of ",(0,s.jsx)(n.code,{children:"inputMessagesKey"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["We can invoke this new chain as normal, with an additional\n",(0,s.jsx)(n.code,{children:"configurable"})," field that specifies the particular ",(0,s.jsx)(n.code,{children:"sessionId"})," to pass\nto the factory function. This is unused for the demo, but in real-world\nchains, you\u2019ll want to return a chat history corresponding to the passed\nsession:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await chainWithMessageHistory.invoke(\n  {\n    input:\n      "Translate this sentence from English to French: I love programming.",\n  },\n  { configurable: { sessionId: "unused" } }\n);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: `The translation of "I love programming" in French is "J\'adore la programmation."`,\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: `The translation of "I love programming" in French is "J\'adore la programmation."`,\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 20, promptTokens: 39, totalTokens: 59 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await chainWithMessageHistory.invoke(\n  {\n    input: "What did I just ask you?",\n  },\n  { configurable: { sessionId: "unused" } }\n);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'You just asked for the translation of the sentence "I love programming" from English to French.\',\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: \'You just asked for the translation of the sentence "I love programming" from English to French.\',\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 19, promptTokens: 74, totalTokens: 93 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"modifying-chat-history",children:"Modifying chat history"}),"\n",(0,s.jsx)(n.p,{children:"Modifying stored chat messages can help your chatbot handle a variety of\nsituations. Here are some examples:"}),"\n",(0,s.jsx)(n.h3,{id:"trimming-messages",children:"Trimming messages"}),"\n",(0,s.jsxs)(n.p,{children:["LLMs and chat models have limited context windows, and even if you\u2019re\nnot directly hitting limits, you may want to limit the amount of\ndistraction the model has to deal with. One solution is to only load and\nstore the most recent ",(0,s.jsx)(n.code,{children:"n"})," messages. Let\u2019s use an example history with\nsome preloaded messages:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await demoEphemeralChatMessageHistory.clear();\n\nawait demoEphemeralChatMessageHistory.addMessage(\n  new HumanMessage("Hey there! I\'m Nemo.")\n);\n\nawait demoEphemeralChatMessageHistory.addMessage(new AIMessage("Hello!"));\n\nawait demoEphemeralChatMessageHistory.addMessage(\n  new HumanMessage("How are you today?")\n);\n\nawait demoEphemeralChatMessageHistory.addMessage(new AIMessage("Fine thanks!"));\n\nawait demoEphemeralChatMessageHistory.getMessages();\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'[\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Hey there! I\'m Nemo.",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Hey there! I\'m Nemo.",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Hello!",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Hello!",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {},\n    tool_calls: [],\n    invalid_tool_calls: []\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "How are you today?",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "How are you today?",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Fine thanks!",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Fine thanks!",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {},\n    tool_calls: [],\n    invalid_tool_calls: []\n  }\n]\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Let\u2019s use this message history with the ",(0,s.jsx)(n.code,{children:"RunnableWithMessageHistory"}),"\nchain we declared above:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const chainWithMessageHistory2 = new RunnableWithMessageHistory({\n  runnable: chain2,\n  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistory,\n  inputMessagesKey: "input",\n  historyMessagesKey: "chat_history",\n});\n\nawait chainWithMessageHistory2.invoke(\n  {\n    input: "What\'s my name?",\n  },\n  { configurable: { sessionId: "unused" } }\n);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "Your name is Nemo!",\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "Your name is Nemo!",\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 6, promptTokens: 66, totalTokens: 72 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"We can see the chain remembers the preloaded name."}),"\n",(0,s.jsxs)(n.p,{children:["But let\u2019s say we have a very small context window, and we want to trim\nthe number of messages passed to the chain to only the 2 most recent\nones. We can use the ",(0,s.jsx)(n.code,{children:"clear"})," method to remove messages and re-add them\nto the history. We don\u2019t have to, but let\u2019s put this method at the front\nof our chain to ensure it\u2019s always called:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'import {\n  RunnablePassthrough,\n  RunnableSequence,\n} from "@langchain/core/runnables";\n\nconst trimMessages = async (_chainInput: Record<string, any>) => {\n  const storedMessages = await demoEphemeralChatMessageHistory.getMessages();\n  if (storedMessages.length <= 2) {\n    return false;\n  }\n  await demoEphemeralChatMessageHistory.clear();\n  for (const message of storedMessages.slice(-2)) {\n    demoEphemeralChatMessageHistory.addMessage(message);\n  }\n  return true;\n};\n\nconst chainWithTrimming = RunnableSequence.from([\n  RunnablePassthrough.assign({ messages_trimmed: trimMessages }),\n  chainWithMessageHistory2,\n]);\n'})}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s call this new chain and check the messages afterwards:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await chainWithTrimming.invoke(\n  {\n    input: "Where does P. Sherman live?",\n  },\n  { configurable: { sessionId: "unused" } }\n);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie "Finding Nem\'... 3 more characters,\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: \'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie "Finding Nem\'... 3 more characters,\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 26, promptTokens: 53, totalTokens: 79 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"await demoEphemeralChatMessageHistory.getMessages();\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'[\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "What\'s my name?",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "What\'s my name?",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Your name is Nemo!",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: { function_call: undefined, tool_calls: undefined },\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Your name is Nemo!",\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {\n      tokenUsage: { completionTokens: 6, promptTokens: 66, totalTokens: 72 },\n      finish_reason: "stop"\n    },\n    tool_calls: [],\n    invalid_tool_calls: []\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Where does P. Sherman live?",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Where does P. Sherman live?",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie "Finding Nem\'... 3 more characters,\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: { function_call: undefined, tool_calls: undefined },\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: \'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie "Finding Nem\'... 3 more characters,\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {\n      tokenUsage: { completionTokens: 26, promptTokens: 53, totalTokens: 79 },\n      finish_reason: "stop"\n    },\n    tool_calls: [],\n    invalid_tool_calls: []\n  }\n]\n'})}),"\n",(0,s.jsxs)(n.p,{children:["And we can see that our history has removed the two oldest messages\nwhile still adding the most recent conversation at the end. The next\ntime the chain is called, ",(0,s.jsx)(n.code,{children:"trimMessages"})," will be called again, and only\nthe two most recent messages will be passed to the model. In this case,\nthis means that the model will forget the name we gave it the next time\nwe invoke it:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await chainWithTrimming.invoke(\n  {\n    input: "What is my name?",\n  },\n  { configurable: { sessionId: "unused" } }\n);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: "I\'m sorry, I don\'t have access to your personal information. Can I help you with anything else?",\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: "I\'m sorry, I don\'t have access to your personal information. Can I help you with anything else?",\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 22, promptTokens: 73, totalTokens: 95 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"await demoEphemeralChatMessageHistory.getMessages();\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'[\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Where does P. Sherman live?",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Where does P. Sherman live?",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie "Finding Nem\'... 3 more characters,\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: { function_call: undefined, tool_calls: undefined },\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: \'P. Sherman is a fictional character who lives at 42 Wallaby Way, Sydney, from the movie "Finding Nem\'... 3 more characters,\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {\n      tokenUsage: { completionTokens: 26, promptTokens: 53, totalTokens: 79 },\n      finish_reason: "stop"\n    },\n    tool_calls: [],\n    invalid_tool_calls: []\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "What is my name?",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "What is my name?",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "I\'m sorry, I don\'t have access to your personal information. Can I help you with anything else?",\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: { function_call: undefined, tool_calls: undefined },\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "I\'m sorry, I don\'t have access to your personal information. Can I help you with anything else?",\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {\n      tokenUsage: { completionTokens: 22, promptTokens: 73, totalTokens: 95 },\n      finish_reason: "stop"\n    },\n    tool_calls: [],\n    invalid_tool_calls: []\n  }\n]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"summary-memory",children:"Summary memory"}),"\n",(0,s.jsx)(n.p,{children:"We can use this same pattern in other ways too. For example, we could\nuse an additional LLM call to generate a summary of the conversation\nbefore calling our chain. Let\u2019s recreate our chat history and chatbot\nchain:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await demoEphemeralChatMessageHistory.clear();\n\nawait demoEphemeralChatMessageHistory.addMessage(\n  new HumanMessage("Hey there! I\'m Nemo.")\n);\n\nawait demoEphemeralChatMessageHistory.addMessage(new AIMessage("Hello!"));\n\nawait demoEphemeralChatMessageHistory.addMessage(\n  new HumanMessage("How are you today?")\n);\n\nawait demoEphemeralChatMessageHistory.addMessage(new AIMessage("Fine thanks!"));\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const runnableWithSummaryMemoryPrompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    "You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.",\n  ],\n  new MessagesPlaceholder("chat_history"),\n  ["human", "{input}"],\n]);\n\nconst summaryMemoryChain = runnableWithSummaryMemoryPrompt.pipe(llm);\n\nconst chainWithMessageHistory3 = new RunnableWithMessageHistory({\n  runnable: summaryMemoryChain,\n  getMessageHistory: (_sessionId) => demoEphemeralChatMessageHistory,\n  inputMessagesKey: "input",\n  historyMessagesKey: "chat_history",\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"And now, let\u2019s create a function that will distill previous interactions\ninto a summary. We can add this one to the front of the chain too:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'const summarizeMessages = async (_chainInput: Record<string, any>) => {\n  const storedMessages = await demoEphemeralChatMessageHistory.getMessages();\n  if (storedMessages.length === 0) {\n    return false;\n  }\n  const summarizationPrompt = ChatPromptTemplate.fromMessages([\n    new MessagesPlaceholder("chat_history"),\n    [\n      "user",\n      "Distill the above chat messages into a single summary message. Include as many specific details as you can.",\n    ],\n  ]);\n  const summarizationChain = summarizationPrompt.pipe(llm);\n  const summaryMessage = await summarizationChain.invoke({\n    chat_history: storedMessages,\n  });\n  await demoEphemeralChatMessageHistory.clear();\n  demoEphemeralChatMessageHistory.addMessage(summaryMessage);\n  return true;\n};\n\nconst chainWithSummarization = RunnableSequence.from([\n  RunnablePassthrough.assign({\n    messages_summarized: summarizeMessages,\n  }),\n  chainWithMessageHistory3,\n]);\n'})}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s see if it remembers the name we gave it:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'await chainWithSummarization.invoke(\n  {\n    input: "What did I say my name was?",\n  },\n  {\n    configurable: { sessionId: "unused" },\n  }\n);\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'You introduced yourself as "Nemo."\',\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: \'You introduced yourself as "Nemo."\',\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 8, promptTokens: 87, totalTokens: 95 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"await demoEphemeralChatMessageHistory.getMessages();\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:'[\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "The conversation consists of a greeting from someone named Nemo and a general inquiry about their we"... 86 more characters,\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: { function_call: undefined, tool_calls: undefined },\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "The conversation consists of a greeting from someone named Nemo and a general inquiry about their we"... 86 more characters,\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {\n      tokenUsage: { completionTokens: 34, promptTokens: 62, totalTokens: 96 },\n      finish_reason: "stop"\n    },\n    tool_calls: [],\n    invalid_tool_calls: []\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "What did I say my name was?",\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "What did I say my name was?",\n    name: undefined,\n    additional_kwargs: {},\n    response_metadata: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \'You introduced yourself as "Nemo."\',\n      tool_calls: [],\n      invalid_tool_calls: [],\n      additional_kwargs: { function_call: undefined, tool_calls: undefined },\n      response_metadata: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: \'You introduced yourself as "Nemo."\',\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {\n      tokenUsage: { completionTokens: 8, promptTokens: 87, totalTokens: 95 },\n      finish_reason: "stop"\n    },\n    tool_calls: [],\n    invalid_tool_calls: []\n  }\n]\n'})}),"\n",(0,s.jsx)(n.p,{children:"Note that invoking the chain again will generate another summary\ngenerated from the initial summary plus new messages and so on. You\ncould also design a hybrid approach where a certain number of messages\nare retained in chat history while others are summarized."}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,s.jsx)(n.p,{children:"You\u2019ve now learned how to manage memory in your chatbots"}),"\n",(0,s.jsxs)(n.p,{children:["Next, check out some of the other guides in this section, such as ",(0,s.jsx)(n.a,{href:"../../docs/how_to/chatbots_retrieval",children:"how\nto add retrieval to your chatbot"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},63142:(e,n,a)=>{a.d(n,{A:()=>m});a(96540);var s=a(11470),t=a(19365),o=a(21432),i=a(27846),l=a(27293),r=a(74848);function c(e){let{children:n}=e;return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(l.A,{type:"tip",children:(0,r.jsxs)("p",{children:["See"," ",(0,r.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,r.jsx)(i.A,{children:n})]})}const d={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},h=["openai","anthropic","mistral","groq","vertex"];function m(e){const{customVarName:n,additionalDependencies:a}=e,i=n??"model",l=e.openaiParams??d.openaiParams,m=e.anthropicParams??d.anthropicParams,g=e.fireworksParams??d.fireworksParams,p=e.mistralParams??d.mistralParams,u=e.groqParams??d.groqParams,_=e.vertexParams??d.vertexParams,y=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],f={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${i} = new ChatOpenAI(${l});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${i} = new ChatAnthropic(${m});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${i} = new ChatFireworks(${g});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${i} = new ChatMistralAI(${p});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${i} = new ChatGroq(${u});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${i} = new ChatVertexAI(${_});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},w=(e.onlyWso?h:y).map((e=>f[e]));return(0,r.jsxs)("div",{children:[(0,r.jsx)("h3",{children:"\uc0ac\uc6a9\ud560 \ucc44\ud305 \ubaa8\ub378 \uc120\ud0dd:"}),(0,r.jsx)(s.A,{groupId:"modelTabs",children:w.map((e=>(0,r.jsxs)(t.A,{value:e.value,label:e.label,children:[(0,r.jsx)("h4",{children:"\uc758\uc874\uc131 \ucd94\uac00"}),(0,r.jsx)(c,{children:[e.dependencies,a].join(" ")}),(0,r.jsx)("h4",{children:"\ud658\uacbd\ubcc0\uc218 \ucd94\uac00"}),(0,r.jsx)(o.A,{language:"bash",children:e.envs}),(0,r.jsx)("h4",{children:"\ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654"}),(0,r.jsx)(o.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,n,a)=>{a.d(n,{A:()=>l});a(96540);var s=a(11470),t=a(19365),o=a(21432),i=a(74848);function l(e){let{children:n}=e;return(0,i.jsxs)(s.A,{groupId:"npm2yarn",children:[(0,i.jsx)(t.A,{value:"npm",label:"npm",children:(0,i.jsxs)(o.A,{language:"bash",children:["npm i ",n]})}),(0,i.jsx)(t.A,{value:"yarn",label:"yarn",default:!0,children:(0,i.jsxs)(o.A,{language:"bash",children:["yarn add ",n]})}),(0,i.jsx)(t.A,{value:"pnpm",label:"pnpm",children:(0,i.jsxs)(o.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);