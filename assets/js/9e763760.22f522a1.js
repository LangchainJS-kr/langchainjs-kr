(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4658],{48965:(e,n,a)=>{"use strict";a.r(n),a.d(n,{assets:()=>u,contentTitle:()=>p,default:()=>x,frontMatter:()=>r,metadata:()=>h,toc:()=>g});var t=a(74848),i=a(28453),l=a(78847),o=a(64428),s=a(48558),d=a.n(s),c=a(34137),m=a.n(c);const r={sidebar_class_name:"node-only"},p="Llama CPP",h={id:"integrations/text_embedding/llama_cpp",title:"Llama CPP",description:"Only available on Node.js.",source:"@site/docs/integrations/text_embedding/llama_cpp.mdx",sourceDirName:"integrations/text_embedding",slug:"/integrations/text_embedding/llama_cpp",permalink:"/docs/integrations/text_embedding/llama_cpp",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/text_embedding/llama_cpp.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"node-only"},sidebar:"integrations",previous:{title:"HuggingFace Inference",permalink:"/docs/integrations/text_embedding/hugging_face_inference"},next:{title:"Minimax",permalink:"/docs/integrations/text_embedding/minimax"}},u={},g=[{value:"Setup",id:"setup",level:2},...l.toc,{value:"Usage",id:"usage",level:2},{value:"Basic use",id:"basic-use",level:3},{value:"Document embedding",id:"document-embedding",level:3}];function b(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"llama-cpp",children:"Llama CPP"}),"\n",(0,t.jsx)(n.admonition,{title:"Compatibility",type:"tip",children:(0,t.jsx)(n.p,{children:"Only available on Node.js."})}),"\n",(0,t.jsxs)(n.p,{children:["This module is based on the ",(0,t.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," Node.js bindings for ",(0,t.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp"}),", allowing you to work with a locally running LLM. This allows you to work with a much smaller quantized model capable of running on a laptop environment, ideal for testing and scratch padding ideas without running up a bill!"]}),"\n",(0,t.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsxs)(n.p,{children:["You'll need to install the ",(0,t.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," module to communicate with your local model."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install -S node-llama-cpp\n"})}),"\n","\n",(0,t.jsx)(l.default,{}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/community\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You will also need a local Llama 2 model (or a model supported by ",(0,t.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"}),"). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example)."]}),"\n",(0,t.jsxs)(n.p,{children:["Out-of-the-box ",(0,t.jsx)(n.code,{children:"node-llama-cpp"})," is tuned for running on a MacOS platform with support for the Metal GPU of Apple M-series of processors. If you need to turn this off or need support for the CUDA architecture then refer to the documentation at ",(0,t.jsx)(n.a,{href:"https://withcatai.github.io/node-llama-cpp/",children:"node-llama-cpp"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["For advice on getting and preparing ",(0,t.jsx)(n.code,{children:"llama2"})," see the documentation for the LLM version of this module."]}),"\n",(0,t.jsxs)(n.p,{children:["A note to LangChain.js contributors: if you want to run the tests associated with this module you will need to put the path to your local model in the environment variable ",(0,t.jsx)(n.code,{children:"LLAMA_PATH"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,t.jsx)(n.h3,{id:"basic-use",children:"Basic use"}),"\n",(0,t.jsxs)(n.p,{children:["We need to provide a path to our local Llama2 model, also the ",(0,t.jsx)(n.code,{children:"embeddings"})," property is always set to ",(0,t.jsx)(n.code,{children:"true"})," in this module."]}),"\n","\n",(0,t.jsx)(o.A,{language:"typescript",children:d()}),"\n",(0,t.jsx)(n.h3,{id:"document-embedding",children:"Document embedding"}),"\n","\n",(0,t.jsx)(o.A,{language:"typescript",children:m()})]})}function x(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(b,{...e})}):b(e)}},48558:e=>{e.exports={content:'import { LlamaCppEmbeddings } from "@langchain/community/embeddings/llama_cpp";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst embeddings = new LlamaCppEmbeddings({\n  modelPath: llamaPath,\n});\n\nconst res = embeddings.embedQuery("Hello Llama!");\n\nconsole.log(res);\n\n/*\n\t[ 15043, 365, 29880, 3304, 29991 ]\n*/\n',imports:[{local:"LlamaCppEmbeddings",imported:"LlamaCppEmbeddings",source:"@langchain/community/embeddings/llama_cpp"}]}},34137:e=>{e.exports={content:'import { LlamaCppEmbeddings } from "@langchain/community/embeddings/llama_cpp";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst documents = ["Hello World!", "Bye Bye!"];\n\nconst embeddings = new LlamaCppEmbeddings({\n  modelPath: llamaPath,\n});\n\nconst res = await embeddings.embedDocuments(documents);\n\nconsole.log(res);\n\n/*\n\t[ [ 15043, 2787, 29991 ], [ 2648, 29872, 2648, 29872, 29991 ] ]\n*/\n',imports:[{local:"LlamaCppEmbeddings",imported:"LlamaCppEmbeddings",source:"@langchain/community/embeddings/llama_cpp"}]}}}]);