"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7976],{73607:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>d});var r=s(74848),n=s(28453);const o={sidebar_class_name:"node-only",hide_table_of_contents:!0},a="Recursive URL Loader",i={id:"integrations/document_loaders/web_loaders/recursive_url_loader",title:"Recursive URL Loader",description:"When loading content from a website, we may want to process load all URLs on a page.",source:"@site/docs/integrations/document_loaders/web_loaders/recursive_url_loader.mdx",sourceDirName:"integrations/document_loaders/web_loaders",slug:"/integrations/document_loaders/web_loaders/recursive_url_loader",permalink:"/docs/integrations/document_loaders/web_loaders/recursive_url_loader",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/document_loaders/web_loaders/recursive_url_loader.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"node-only",hide_table_of_contents:!0},sidebar:"integrations",previous:{title:"PDF files",permalink:"/docs/integrations/document_loaders/web_loaders/pdf"},next:{title:"S3 File",permalink:"/docs/integrations/document_loaders/web_loaders/s3"}},l={},d=[{value:"Setup",id:"setup",level:2},{value:"Usage",id:"usage",level:2},{value:"Options",id:"options",level:2}];function c(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h1,{id:"recursive-url-loader",children:"Recursive URL Loader"}),"\n",(0,r.jsx)(t.p,{children:"When loading content from a website, we may want to process load all URLs on a page."}),"\n",(0,r.jsxs)(t.p,{children:["For example, let's look at the ",(0,r.jsx)(t.a,{href:"/docs/introduction",children:"LangChain.js introduction"})," docs."]}),"\n",(0,r.jsx)(t.p,{children:"This has many interesting child pages that we may want to load, split, and later retrieve in bulk."}),"\n",(0,r.jsx)(t.p,{children:"The challenge is traversing the tree of child pages and assembling a list!"}),"\n",(0,r.jsx)(t.p,{children:"We do this using the RecursiveUrlLoader."}),"\n",(0,r.jsx)(t.p,{children:"This also gives us the flexibility to exclude some children, customize the extractor, and more."}),"\n",(0,r.jsx)(t.h2,{id:"setup",children:"Setup"}),"\n",(0,r.jsxs)(t.p,{children:["To get started, you'll need to install the ",(0,r.jsx)(t.a,{href:"https://www.npmjs.com/package/jsdom",children:(0,r.jsx)(t.code,{children:"jsdom"})})," package:"]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",metastring:"npm2yarn",children:"npm i jsdom\n"})}),"\n",(0,r.jsxs)(t.p,{children:["We also suggest adding a package like ",(0,r.jsx)(t.a,{href:"https://www.npmjs.com/package/html-to-text",children:(0,r.jsx)(t.code,{children:"html-to-text"})})," or\n",(0,r.jsx)(t.a,{href:"https://www.npmjs.com/package/@mozilla/readability",children:(0,r.jsx)(t.code,{children:"@mozilla/readability"})})," for extracting the raw text from the page."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",metastring:"npm2yarn",children:"npm i html-to-text\n"})}),"\n",(0,r.jsx)(t.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:'import { compile } from "html-to-text";\nimport { RecursiveUrlLoader } from "langchain/document_loaders/web/recursive_url";\n\nconst url = "/docs/introduction";\n\nconst compiledConvert = compile({ wordwrap: 130 }); // returns (text: string) => string;\n\nconst loader = new RecursiveUrlLoader(url, {\n  extractor: compiledConvert,\n  maxDepth: 1,\n  excludeDirs: ["/docs/api/"],\n});\n\nconst docs = await loader.load();\n'})}),"\n",(0,r.jsx)(t.h2,{id:"options",children:"Options"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-typescript",children:"interface Options {\n  excludeDirs?: string[]; // webpage directories to exclude.\n  extractor?: (text: string) => string; // a function to extract the text of the document from the webpage, by default it returns the page as it is. It is recommended to use tools like html-to-text to extract the text. By default, it just returns the page as it is.\n  maxDepth?: number; // the maximum depth to crawl. By default, it is set to 2. If you need to crawl the whole website, set it to a number that is large enough would simply do the job.\n  timeout?: number; // the timeout for each request, in the unit of seconds. By default, it is set to 10000 (10 seconds).\n  preventOutside?: boolean; // whether to prevent crawling outside the root url. By default, it is set to true.\n  callerOptions?: AsyncCallerConstructorParams; // the options to call the AsyncCaller for example setting max concurrency (default is 64)\n}\n"})}),"\n",(0,r.jsx)(t.p,{children:"However, since it's hard to perform a perfect filter, you may still see some irrelevant results in the results. You can perform a filter on the returned documents by yourself, if it's needed. Most of the time, the returned results are good enough."})]})}function u(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);