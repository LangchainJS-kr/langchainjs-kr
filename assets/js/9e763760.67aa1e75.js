(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4658,65],{48965:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>g,contentTitle:()=>p,default:()=>b,frontMatter:()=>m,metadata:()=>h,toc:()=>u});var a=t(74848),i=t(28453),o=t(78847),l=t(64428),s=t(48558),d=t.n(s),r=t(34137),c=t.n(r);const m={sidebar_class_name:"node-only"},p="Llama CPP",h={id:"integrations/text_embedding/llama_cpp",title:"Llama CPP",description:"Only available on Node.js.",source:"@site/docs/integrations/text_embedding/llama_cpp.mdx",sourceDirName:"integrations/text_embedding",slug:"/integrations/text_embedding/llama_cpp",permalink:"/docs/integrations/text_embedding/llama_cpp",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/text_embedding/llama_cpp.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"node-only"},sidebar:"integrations",previous:{title:"HuggingFace Inference",permalink:"/docs/integrations/text_embedding/hugging_face_inference"},next:{title:"Minimax",permalink:"/docs/integrations/text_embedding/minimax"}},g={},u=[{value:"Setup",id:"setup",level:2},...o.toc,{value:"Usage",id:"usage",level:2},{value:"Basic use",id:"basic-use",level:3},{value:"Document embedding",id:"document-embedding",level:3}];function x(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"llama-cpp",children:"Llama CPP"}),"\n",(0,a.jsx)(n.admonition,{title:"Compatibility",type:"tip",children:(0,a.jsx)(n.p,{children:"Only available on Node.js."})}),"\n",(0,a.jsxs)(n.p,{children:["This module is based on the ",(0,a.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," Node.js bindings for ",(0,a.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp"}),", allowing you to work with a locally running LLM. This allows you to work with a much smaller quantized model capable of running on a laptop environment, ideal for testing and scratch padding ideas without running up a bill!"]}),"\n",(0,a.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsxs)(n.p,{children:["You'll need to install the ",(0,a.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," module to communicate with your local model."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install -S node-llama-cpp\n"})}),"\n","\n",(0,a.jsx)(o.default,{}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/community\n"})}),"\n",(0,a.jsxs)(n.p,{children:["You will also need a local Llama 2 model (or a model supported by ",(0,a.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"}),"). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example)."]}),"\n",(0,a.jsxs)(n.p,{children:["Out-of-the-box ",(0,a.jsx)(n.code,{children:"node-llama-cpp"})," is tuned for running on a MacOS platform with support for the Metal GPU of Apple M-series of processors. If you need to turn this off or need support for the CUDA architecture then refer to the documentation at ",(0,a.jsx)(n.a,{href:"https://withcatai.github.io/node-llama-cpp/",children:"node-llama-cpp"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["For advice on getting and preparing ",(0,a.jsx)(n.code,{children:"llama2"})," see the documentation for the LLM version of this module."]}),"\n",(0,a.jsxs)(n.p,{children:["A note to LangChain.js contributors: if you want to run the tests associated with this module you will need to put the path to your local model in the environment variable ",(0,a.jsx)(n.code,{children:"LLAMA_PATH"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(n.h3,{id:"basic-use",children:"Basic use"}),"\n",(0,a.jsxs)(n.p,{children:["We need to provide a path to our local Llama2 model, also the ",(0,a.jsx)(n.code,{children:"embeddings"})," property is always set to ",(0,a.jsx)(n.code,{children:"true"})," in this module."]}),"\n","\n",(0,a.jsx)(l.A,{language:"typescript",children:d()}),"\n",(0,a.jsx)(n.h3,{id:"document-embedding",children:"Document embedding"}),"\n","\n",(0,a.jsx)(l.A,{language:"typescript",children:c()})]})}function b(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(x,{...e})}):x(e)}},78847:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>r});var a=t(74848),i=t(28453);const o={},l=void 0,s={id:"mdx_components/integration_install_tooltip",title:"integration_install_tooltip",description:"See this section for general instructions on installing integration packages.",source:"@site/docs/mdx_components/integration_install_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/integration_install_tooltip",permalink:"/docs/mdx_components/integration_install_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/integration_install_tooltip.mdx",tags:[],version:"current",frontMatter:{}},d={},r=[];function c(e){const n={a:"a",admonition:"admonition",p:"p",...(0,i.R)(),...e.components};return(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"/docs/how_to/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},48558:e=>{e.exports={content:'import { LlamaCppEmbeddings } from "@langchain/community/embeddings/llama_cpp";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst embeddings = new LlamaCppEmbeddings({\n  modelPath: llamaPath,\n});\n\nconst res = embeddings.embedQuery("Hello Llama!");\n\nconsole.log(res);\n\n/*\n\t[ 15043, 365, 29880, 3304, 29991 ]\n*/\n',imports:[{local:"LlamaCppEmbeddings",imported:"LlamaCppEmbeddings",source:"@langchain/community/embeddings/llama_cpp"}]}},34137:e=>{e.exports={content:'import { LlamaCppEmbeddings } from "@langchain/community/embeddings/llama_cpp";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst documents = ["Hello World!", "Bye Bye!"];\n\nconst embeddings = new LlamaCppEmbeddings({\n  modelPath: llamaPath,\n});\n\nconst res = await embeddings.embedDocuments(documents);\n\nconsole.log(res);\n\n/*\n\t[ [ 15043, 2787, 29991 ], [ 2648, 29872, 2648, 29872, 29991 ] ]\n*/\n',imports:[{local:"LlamaCppEmbeddings",imported:"LlamaCppEmbeddings",source:"@langchain/community/embeddings/llama_cpp"}]}}}]);