"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7220],{89889:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>u,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>d,toc:()=>h});var t=a(74848),s=a(28453),r=a(78847),i=a(27846),o=a(63142);const l={sidebar_class_name:"hidden",title:"How to add examples to the prompt"},c=void 0,d={id:"how_to/query_few_shot",title:"How to add examples to the prompt",description:"This guide assumes familiarity with the following:",source:"@site/docs/how_to/query_few_shot.mdx",sourceDirName:"how_to",slug:"/how_to/query_few_shot",permalink:"/docs/how_to/query_few_shot",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/query_few_shot.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to add examples to the prompt"},sidebar:"tutorialSidebar",previous:{title:"How to construct filters",permalink:"/docs/how_to/query_constructing_filters"},next:{title:"How to deal with high cardinality categorical variables",permalink:"/docs/how_to/query_high_cardinality"}},u={},h=[{value:"Setup",id:"setup",level:2},{value:"Install dependencies",id:"install-dependencies",level:3},...r.toc,{value:"Set environment variables",id:"set-environment-variables",level:3},{value:"Query schema",id:"query-schema",level:2},{value:"Query generation",id:"query-generation",level:2},{value:"Adding examples and tuning the prompt",id:"adding-examples-and-tuning-the-prompt",level:2},{value:"Next steps",id:"next-steps",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,t.jsx)(n.p,{children:"This guide assumes familiarity with the following:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/tutorials/query_analysis",children:"Query analysis"})}),"\n"]})]}),"\n",(0,t.jsx)(n.p,{children:"As our query analysis becomes more complex, the LLM may struggle to\nunderstand how exactly it should respond in certain scenarios. In order\nto improve performance here, we can add examples to the prompt to guide\nthe LLM."}),"\n",(0,t.jsxs)(n.p,{children:["Let\u2019s take a look at how we can add examples for the LangChain YouTube\nvideo query analyzer we built in the ",(0,t.jsx)(n.a,{href:"../../docs/tutorials/query_analysis",children:"query analysis\ntutorial"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsx)(n.h3,{id:"install-dependencies",children:"Install dependencies"}),"\n","\n",(0,t.jsx)(r.default,{}),"\n",(0,t.jsx)(i.A,{children:(0,t.jsx)(n.p,{children:"zod uuid"})}),"\n",(0,t.jsx)(n.h3,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGCHAIN_TRACING_V2=true\n"})}),"\n",(0,t.jsx)(n.h2,{id:"query-schema",children:"Query schema"}),"\n",(0,t.jsxs)(n.p,{children:["We\u2019ll define a query schema that we want our model to output. To make\nour query analysis a bit more interesting, we\u2019ll add a ",(0,t.jsx)(n.code,{children:"subQueries"}),"\nfield that contains more narrow questions derived from the top level\nquestion."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { z } from "zod";\n\nconst subQueriesDescription = `\nIf the original question contains multiple distinct sub-questions,\nor if there are more generic questions that would be helpful to answer in\norder to answer the original question, write a list of all relevant sub-questions.\nMake sure this list is comprehensive and covers all parts of the original question.\nIt\'s ok if there\'s redundancy in the sub-questions, it\'s better to cover all the bases than to miss some.\nMake sure the sub-questions are as narrowly focused as possible in order to get the most relevant results.`;\n\nconst searchSchema = z.object({\n  query: z\n    .string()\n    .describe("Primary similarity search query applied to video transcripts."),\n  subQueries: z.array(z.string()).optional().describe(subQueriesDescription),\n  publishYear: z.number().optional().describe("Year video was published"),\n});\n'})}),"\n",(0,t.jsx)(n.h2,{id:"query-generation",children:"Query generation"}),"\n","\n",(0,t.jsx)(o.A,{customVarName:"llm"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { ChatPromptTemplate } from "@langchain/core/prompts";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from "@langchain/core/runnables";\n\nconst system = `You are an expert at converting user questions into database queries.\nYou have access to a database of tutorial videos about a software library for building LLM-powered applications.\nGiven a question, return a list of database queries optimized to retrieve the most relevant results.\n\nIf there are acronyms or words you are not familiar with, do not try to rephrase them.`;\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", system],\n  ["placeholder", "{examples}"],\n  ["human", "{question}"],\n]);\nconst llmWithTools = llm.withStructuredOutput(searchSchema, {\n  name: "Search",\n});\nconst queryAnalyzer = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llmWithTools,\n]);\n'})}),"\n",(0,t.jsx)(n.p,{children:"Let\u2019s try out our query analyzer without any examples in the prompt:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'await queryAnalyzer.invoke(\n  "what\'s the difference between web voyager and reflection agents? do both use langgraph?"\n);\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'{\n  query: "difference between Web Voyager and Reflection Agents",\n  subQueries: [ "Do Web Voyager and Reflection Agents use LangGraph?" ]\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"adding-examples-and-tuning-the-prompt",children:"Adding examples and tuning the prompt"}),"\n",(0,t.jsx)(n.p,{children:"This works pretty well, but we probably want it to decompose the\nquestion even further to separate the queries about Web Voyager and\nReflection Agents."}),"\n",(0,t.jsx)(n.p,{children:"To tune our query generation results, we can add some examples of inputs\nquestions and gold standard output queries to our prompt."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:"const examples = [];\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const question = "What\'s chat langchain, is it a langchain template?";\nconst query = {\n  query: "What is chat langchain and is it a langchain template?",\n  subQueries: ["What is chat langchain", "What is a langchain template"],\n};\nexamples.push({ input: question, toolCalls: [query] });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"1\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const question =\n  "How to build multi-agent system and stream intermediate steps from it";\nconst query = {\n  query:\n    "How to build multi-agent system and stream intermediate steps from it",\n  subQueries: [\n    "How to build multi-agent system",\n    "How to stream intermediate steps from multi-agent system",\n    "How to stream intermediate steps",\n  ],\n};\n\nexamples.push({ input: question, toolCalls: [query] });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"2\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'const question = "LangChain agents vs LangGraph?";\nconst query = {\n  query:\n    "What\'s the difference between LangChain agents and LangGraph? How do you deploy them?",\n  subQueries: [\n    "What are LangChain agents",\n    "What is LangGraph",\n    "How do you deploy LangChain agents",\n    "How do you deploy LangGraph",\n  ],\n};\nexamples.push({ input: question, toolCalls: [query] });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"3\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Now we need to update our prompt template and chain so that the examples\nare included in each prompt. Since we\u2019re working with LLM model\nfunction-calling, we\u2019ll need to do a bit of extra structuring to send\nexample inputs and outputs to the model. We\u2019ll create a\n",(0,t.jsx)(n.code,{children:"toolExampleToMessages"})," helper function to handle this for us:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import {\n  AIMessage,\n  BaseMessage,\n  HumanMessage,\n  SystemMessage,\n  ToolMessage,\n} from "@langchain/core/messages";\nimport { v4 as uuidV4 } from "uuid";\n\nconst toolExampleToMessages = (\n  example: Example | Record<string, any>\n): Array<BaseMessage> => {\n  const messages: Array<BaseMessage> = [\n    new HumanMessage({ content: example.input }),\n  ];\n  const openaiToolCalls = example.toolCalls.map((toolCall) => {\n    return {\n      id: uuidV4(),\n      type: "function" as const,\n      function: {\n        name: "search",\n        arguments: JSON.stringify(toolCall),\n      },\n    };\n  });\n\n  messages.push(\n    new AIMessage({\n      content: "",\n      additional_kwargs: { tool_calls: openaiToolCalls },\n    })\n  );\n\n  const toolOutputs =\n    "toolOutputs" in example\n      ? example.toolOutputs\n      : Array(openaiToolCalls.length).fill(\n          "You have correctly called this tool."\n        );\n  toolOutputs.forEach((output, index) => {\n    messages.push(\n      new ToolMessage({\n        content: output,\n        tool_call_id: openaiToolCalls[index].id,\n      })\n    );\n  });\n\n  return messages;\n};\n\nconst exampleMessages = examples.map((ex) => toolExampleToMessages(ex)).flat();\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from "@langchain/core/prompts";\nimport { RunnableSequence } from "@langchain/core/runnables";\n\nconst queryAnalyzerWithExamples = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n    examples: () => exampleMessages,\n  },\n  prompt,\n  llmWithTools,\n]);\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'await queryAnalyzerWithExamples.invoke(\n  "what\'s the difference between web voyager and reflection agents? do both use langgraph?"\n);\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'{\n  query: "Difference between Web Voyager and Reflection agents, do they both use LangGraph?",\n  subQueries: [\n    "Difference between Web Voyager and Reflection agents",\n    "Do Web Voyager and Reflection agents use LangGraph"\n  ]\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:"Thanks to our examples we get a slightly more decomposed search query.\nWith some more prompt engineering and tuning of our examples we could\nimprove query generation even more."}),"\n",(0,t.jsxs)(n.p,{children:["You can see that the examples are passed to the model as messages in the\n",(0,t.jsx)(n.a,{href:"https://smith.langchain.com/public/102829c3-69fc-4cb7-b28b-399ae2c9c008/r",children:"LangSmith\ntrace"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,t.jsx)(n.p,{children:"You\u2019ve now learned some techniques for combining few-shotting with query\nanalysis."}),"\n",(0,t.jsxs)(n.p,{children:["Next, check out some of the other query analysis guides in this section,\nlike ",(0,t.jsx)(n.a,{href:"../../docs/how_to/query_high_cardinality",children:"how to deal with high cardinality\ndata"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},63142:(e,n,a)=>{a.d(n,{A:()=>h});a(96540);var t=a(11470),s=a(19365),r=a(21432),i=a(27846),o=a(27293),l=a(74848);function c(e){let{children:n}=e;return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(o.A,{type:"tip",children:(0,l.jsxs)("p",{children:["See"," ",(0,l.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,l.jsx)(i.A,{children:n})]})}const d={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},u=["openai","anthropic","mistral","groq","vertex"];function h(e){const{customVarName:n,additionalDependencies:a}=e,i=n??"model",o=e.openaiParams??d.openaiParams,h=e.anthropicParams??d.anthropicParams,p=e.fireworksParams??d.fireworksParams,m=e.mistralParams??d.mistralParams,g=e.groqParams??d.groqParams,x=e.vertexParams??d.vertexParams,y=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],f={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${i} = new ChatOpenAI(${o});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${i} = new ChatAnthropic(${h});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${i} = new ChatFireworks(${p});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${i} = new ChatMistralAI(${m});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${i} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${i} = new ChatVertexAI(${x});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},w=(e.onlyWso?u:y).map((e=>f[e]));return(0,l.jsxs)("div",{children:[(0,l.jsx)("h3",{children:"\uc0ac\uc6a9\ud560 \ucc44\ud305 \ubaa8\ub378 \uc120\ud0dd:"}),(0,l.jsx)(t.A,{groupId:"modelTabs",children:w.map((e=>(0,l.jsxs)(s.A,{value:e.value,label:e.label,children:[(0,l.jsx)("h4",{children:"\uc758\uc874\uc131 \ucd94\uac00"}),(0,l.jsx)(c,{children:[e.dependencies,a].join(" ")}),(0,l.jsx)("h4",{children:"\ud658\uacbd\ubcc0\uc218 \ucd94\uac00"}),(0,l.jsx)(r.A,{language:"bash",children:e.envs}),(0,l.jsx)("h4",{children:"\ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654"}),(0,l.jsx)(r.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,n,a)=>{a.d(n,{A:()=>o});a(96540);var t=a(11470),s=a(19365),r=a(21432),i=a(74848);function o(e){let{children:n}=e;return(0,i.jsxs)(t.A,{groupId:"npm2yarn",children:[(0,i.jsx)(s.A,{value:"npm",label:"npm",children:(0,i.jsxs)(r.A,{language:"bash",children:["npm i ",n]})}),(0,i.jsx)(s.A,{value:"yarn",label:"yarn",default:!0,children:(0,i.jsxs)(r.A,{language:"bash",children:["yarn add ",n]})}),(0,i.jsx)(s.A,{value:"pnpm",label:"pnpm",children:(0,i.jsxs)(r.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);