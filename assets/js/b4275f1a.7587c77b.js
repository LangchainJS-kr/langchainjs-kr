(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2699,7817],{85941:(e,n,o)=>{"use strict";o.r(n),o.d(n,{assets:()=>u,contentTitle:()=>h,default:()=>v,frontMatter:()=>m,metadata:()=>p,toc:()=>f});var a=o(74848),t=o(28453),i=o(64428),s=o(78847),r=o(2280),l=o(45376),c=o.n(l),d=o(60741),g=o.n(d);const m={sidebar_label:"Google GenAI",keywords:["gemini","gemini-pro","ChatGoogleGenerativeAI"]},h="ChatGoogleGenerativeAI",p={id:"integrations/chat/google_generativeai",title:"ChatGoogleGenerativeAI",description:"You can access Google's gemini and gemini-vision models, as well as other",source:"@site/docs/integrations/chat/google_generativeai.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/google_generativeai",permalink:"/docs/integrations/chat/google_generativeai",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/google_generativeai.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"Google GenAI",keywords:["gemini","gemini-pro","ChatGoogleGenerativeAI"]},sidebar:"integrations",previous:{title:"Friendli",permalink:"/docs/integrations/chat/friendli"},next:{title:"(Legacy) Google PaLM/VertexAI",permalink:"/docs/integrations/chat/google_palm"}},u={},f=[...s.toc,{value:"Usage",id:"usage",level:2},...r.toc,{value:"Multimodal support",id:"multimodal-support",level:2},{value:"Gemini Prompting FAQs",id:"gemini-prompting-faqs",level:2}];function x(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"chatgooglegenerativeai",children:"ChatGoogleGenerativeAI"}),"\n",(0,a.jsxs)(n.p,{children:["You can access Google's ",(0,a.jsx)(n.code,{children:"gemini"})," and ",(0,a.jsx)(n.code,{children:"gemini-vision"})," models, as well as other\ngenerative models in LangChain through ",(0,a.jsx)(n.code,{children:"ChatGoogleGenerativeAI"})," class in the\n",(0,a.jsx)(n.code,{children:"@langchain/google-genai"})," integration package."]}),"\n",(0,a.jsxs)(n.admonition,{type:"tip",children:[(0,a.jsxs)(n.p,{children:["You can also access Google's ",(0,a.jsx)(n.code,{children:"gemini"})," family of models via the LangChain VertexAI and VertexAI-web integrations."]}),(0,a.jsxs)(n.p,{children:["Click ",(0,a.jsx)(n.a,{href:"/docs/integrations/chat/google_vertex_ai",children:"here"})," to read the docs."]})]}),"\n",(0,a.jsxs)(n.p,{children:["Get an API key here: ",(0,a.jsx)(n.a,{href:"https://ai.google.dev/tutorials/setup",children:"https://ai.google.dev/tutorials/setup"})]}),"\n",(0,a.jsxs)(n.p,{children:["You'll first need to install the ",(0,a.jsx)(n.code,{children:"@langchain/google-genai"})," package:"]}),"\n","\n",(0,a.jsx)(s.default,{}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/google-genai\n"})}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n","\n",(0,a.jsx)(r.default,{}),"\n","\n",(0,a.jsx)(i.A,{language:"typescript",children:c()}),"\n",(0,a.jsx)(n.h2,{id:"multimodal-support",children:"Multimodal support"}),"\n",(0,a.jsxs)(n.p,{children:["To provide an image, pass a human message with a ",(0,a.jsx)(n.code,{children:"content"})," field set to an array of content objects. Each content object\nwhere each dict contains either an image value (type of image_url) or a text (type of text) value. The value of image_url must be a base64\nencoded image (e.g., data",":image","/png;base64,abcd124):"]}),"\n","\n",(0,a.jsx)(i.A,{language:"typescript",children:g()}),"\n",(0,a.jsx)(n.h2,{id:"gemini-prompting-faqs",children:"Gemini Prompting FAQs"}),"\n",(0,a.jsx)(n.p,{children:"As of the time this doc was written (2023/12/12), Gemini has some restrictions on the types and structure of prompts it accepts. Specifically:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:'When providing multimodal (image) inputs, you are restricted to at most 1 message of "human" (user) type. You cannot pass multiple messages (though the single human message may have multiple content entries)'}),"\n",(0,a.jsx)(n.li,{children:"System messages are not natively supported, and will be merged with the first human message if present."}),"\n",(0,a.jsx)(n.li,{children:"For regular chat conversations, messages must follow the human/ai/human/ai alternating pattern. You may not provide 2 AI or human messages in sequence."}),"\n",(0,a.jsx)(n.li,{children:"Message may be blocked if they violate the safety checks of the LLM. In this case, the model will return an empty response."}),"\n"]})]})}function v(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(x,{...e})}):x(e)}},2280:(e,n,o)=>{"use strict";o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>g,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var a=o(74848),t=o(28453);const i={},s=void 0,r={id:"mdx_components/unified_model_params_tooltip",title:"unified_model_params_tooltip",description:"We're unifying model params across all packages. We now suggest using model instead of modelName, and apiKey for API keys.",source:"@site/docs/mdx_components/unified_model_params_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/unified_model_params_tooltip",permalink:"/docs/mdx_components/unified_model_params_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/unified_model_params_tooltip.mdx",tags:[],version:"current",frontMatter:{}},l={},c=[];function d(e){const n={admonition:"admonition",code:"code",p:"p",...(0,t.R)(),...e.components};return(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["We're unifying model params across all packages. We now suggest using ",(0,a.jsx)(n.code,{children:"model"})," instead of ",(0,a.jsx)(n.code,{children:"modelName"}),", and ",(0,a.jsx)(n.code,{children:"apiKey"})," for API keys."]})})}function g(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},45376:e=>{e.exports={content:"import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport { HarmBlockThreshold, HarmCategory } from \"@google/generative-ai\";\n\n/*\n * Before running this, you should make sure you have created a\n * Google Cloud Project that has `generativelanguage` API enabled.\n *\n * You will also need to generate an API key and set\n * an environment variable GOOGLE_API_KEY\n *\n */\n\n// Text\nconst model = new ChatGoogleGenerativeAI({\n  model: \"gemini-pro\",\n  maxOutputTokens: 2048,\n  safetySettings: [\n    {\n      category: HarmCategory.HARM_CATEGORY_HARASSMENT,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    },\n  ],\n});\n\n// Batch and stream are also supported\nconst res = await model.invoke([\n  [\n    \"human\",\n    \"What would be a good company name for a company that makes colorful socks?\",\n  ],\n]);\n\nconsole.log(res);\n\n/*\n  AIMessage {\n    content: '1. Rainbow Soles\\n' +\n      '2. Toe-tally Colorful\\n' +\n      '3. Bright Sock Creations\\n' +\n      '4. Hue Knew Socks\\n' +\n      '5. The Happy Sock Factory\\n' +\n      '6. Color Pop Hosiery\\n' +\n      '7. Sock It to Me!\\n' +\n      '8. Mismatched Masterpieces\\n' +\n      '9. Threads of Joy\\n' +\n      '10. Funky Feet Emporium\\n' +\n      '11. Colorful Threads\\n' +\n      '12. Sole Mates\\n' +\n      '13. Colorful Soles\\n' +\n      '14. Sock Appeal\\n' +\n      '15. Happy Feet Unlimited\\n' +\n      '16. The Sock Stop\\n' +\n      '17. The Sock Drawer\\n' +\n      '18. Sole-diers\\n' +\n      '19. Footloose Footwear\\n' +\n      '20. Step into Color',\n    name: 'model',\n    additional_kwargs: {}\n  }\n*/\n",imports:[{local:"ChatGoogleGenerativeAI",imported:"ChatGoogleGenerativeAI",source:"@langchain/google-genai"}]}},60741:e=>{e.exports={content:'import fs from "fs";\nimport { ChatGoogleGenerativeAI } from "@langchain/google-genai";\nimport { HumanMessage } from "@langchain/core/messages";\n\n// Multi-modal\nconst vision = new ChatGoogleGenerativeAI({\n  model: "gemini-pro-vision",\n  maxOutputTokens: 2048,\n});\nconst image = fs.readFileSync("./hotdog.jpg").toString("base64");\nconst input2 = [\n  new HumanMessage({\n    content: [\n      {\n        type: "text",\n        text: "Describe the following image.",\n      },\n      {\n        type: "image_url",\n        image_url: `data:image/png;base64,${image}`,\n      },\n    ],\n  }),\n];\n\nconst res2 = await vision.invoke(input2);\n\nconsole.log(res2);\n\n/*\n  AIMessage {\n    content: \' The image shows a hot dog in a bun. The hot dog is grilled and has a dark brown color. The bun is toasted and has a light brown color. The hot dog is in the center of the bun.\',\n    name: \'model\',\n    additional_kwargs: {}\n  }\n*/\n\n// Multi-modal streaming\nconst res3 = await vision.stream(input2);\n\nfor await (const chunk of res3) {\n  console.log(chunk);\n}\n\n/*\n  AIMessageChunk {\n    content: \' The image shows a hot dog in a bun. The hot dog is grilled and has grill marks on it. The bun is toasted and has a light golden\',\n    name: \'model\',\n    additional_kwargs: {}\n  }\n  AIMessageChunk {\n    content: \' brown color. The hot dog is in the center of the bun.\',\n    name: \'model\',\n    additional_kwargs: {}\n  }\n*/\n',imports:[{local:"ChatGoogleGenerativeAI",imported:"ChatGoogleGenerativeAI",source:"@langchain/google-genai"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}}}]);