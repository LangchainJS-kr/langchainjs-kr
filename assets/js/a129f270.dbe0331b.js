(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9359,65],{77016:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>u,contentTitle:()=>p,default:()=>d,frontMatter:()=>l,metadata:()=>h,toc:()=>g});var o=t(74848),a=t(28453),s=t(64428),i=t(60141),r=t.n(i),c=t(78847);const l={hide_table_of_contents:!0},p="SearchApi tool",h={id:"integrations/tools/searchapi",title:"SearchApi tool",description:"The SearchApi tool connects your agents and chains to the internet.",source:"@site/docs/integrations/tools/searchapi.mdx",sourceDirName:"integrations/tools",slug:"/integrations/tools/searchapi",permalink:"/docs/integrations/tools/searchapi",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/tools/searchapi.mdx",tags:[],version:"current",frontMatter:{hide_table_of_contents:!0},sidebar:"integrations",previous:{title:"Python interpreter tool",permalink:"/docs/integrations/tools/pyinterpreter"},next:{title:"Searxng Search tool",permalink:"/docs/integrations/tools/searxng"}},u={},g=[{value:"Usage",id:"usage",level:2},...c.toc];function m(n){const e={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"searchapi-tool",children:"SearchApi tool"}),"\n",(0,o.jsxs)(e.p,{children:["The ",(0,o.jsx)(e.code,{children:"SearchApi"})," tool connects your agents and chains to the internet."]}),"\n",(0,o.jsx)(e.p,{children:"A wrapper around the Search API. This tool is handy when you need to answer questions about current events."}),"\n",(0,o.jsx)(e.h2,{id:"usage",children:"Usage"}),"\n",(0,o.jsx)(e.p,{children:"Input should be a search query."}),"\n","\n","\n",(0,o.jsx)(c.default,{}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai\n"})}),"\n",(0,o.jsx)(s.A,{language:"typescript",children:r()})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(m,{...n})}):m(n)}},78847:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var o=t(74848),a=t(28453);const s={},i=void 0,r={id:"mdx_components/integration_install_tooltip",title:"integration_install_tooltip",description:"See this section for general instructions on installing integration packages.",source:"@site/docs/mdx_components/integration_install_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/integration_install_tooltip",permalink:"/docs/mdx_components/integration_install_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/integration_install_tooltip.mdx",tags:[],version:"current",frontMatter:{}},c={},l=[];function p(n){const e={a:"a",admonition:"admonition",p:"p",...(0,a.R)(),...n.components};return(0,o.jsx)(e.admonition,{type:"tip",children:(0,o.jsxs)(e.p,{children:["See ",(0,o.jsx)(e.a,{href:"/docs/how_to/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},60141:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\nimport { AgentExecutor } from "langchain/agents";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { RunnableSequence } from "@langchain/core/runnables";\nimport { AgentFinish, AgentAction } from "@langchain/core/agents";\nimport { BaseMessageChunk } from "@langchain/core/messages";\nimport { SearchApi } from "@langchain/community/tools/searchapi";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n});\nconst tools = [\n  new SearchApi(process.env.SEARCHAPI_API_KEY, {\n    engine: "google_news",\n  }),\n];\nconst prefix = ChatPromptTemplate.fromMessages([\n  [\n    "ai",\n    "Answer the following questions as best you can. In your final answer, use a bulleted list markdown format.",\n  ],\n  ["human", "{input}"],\n]);\n// Replace this with your actual output parser.\nconst customOutputParser = (\n  input: BaseMessageChunk\n): AgentAction | AgentFinish => ({\n  log: "test",\n  returnValues: {\n    output: input,\n  },\n});\n// Replace this placeholder agent with your actual implementation.\nconst agent = RunnableSequence.from([prefix, model, customOutputParser]);\nconst executor = AgentExecutor.fromAgentAndTools({\n  agent,\n  tools,\n});\nconst res = await executor.invoke({\n  input: "What\'s happening in Ukraine today?",\n});\nconsole.log(res);\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"AgentExecutor",imported:"AgentExecutor",source:"langchain/agents"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"RunnableSequence",imported:"RunnableSequence",source:"@langchain/core/runnables"},{local:"AgentFinish",imported:"AgentFinish",source:"@langchain/core/agents"},{local:"AgentAction",imported:"AgentAction",source:"@langchain/core/agents"},{local:"BaseMessageChunk",imported:"BaseMessageChunk",source:"@langchain/core/messages"},{local:"SearchApi",imported:"SearchApi",source:"@langchain/community/tools/searchapi"}]}}}]);