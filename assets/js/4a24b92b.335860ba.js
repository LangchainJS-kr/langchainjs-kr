"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2943],{719:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});var t=o(74848),a=o(28453);const s={sidebar_class_name:"hidden",title:"How to call tools with multi-modal data"},l=void 0,i={id:"how_to/tool_calls_multi_modal",title:"How to call tools with multi-modal data",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/tool_calls_multi_modal.mdx",sourceDirName:"how_to",slug:"/how_to/tool_calls_multi_modal",permalink:"/docs/how_to/tool_calls_multi_modal",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/tool_calls_multi_modal.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to call tools with multi-modal data"},sidebar:"tutorialSidebar",previous:{title:"How to use a chat model to call tools",permalink:"/docs/how_to/tool_calling"},next:{title:"How to use LangChain tools",permalink:"/docs/how_to/tools_builtin"}},r={},c=[{value:"OpenAI",id:"openai",level:2},{value:"Anthropic",id:"anthropic",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,t.jsx)(n.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#chat-models",children:"Chat models"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#tools",children:"LangChain Tools"})}),"\n"]})]}),"\n",(0,t.jsx)(n.p,{children:"Here we demonstrate how to call tools with multi-modal data, such as\nimages."}),"\n",(0,t.jsxs)(n.p,{children:["Some multi-modal models, such as those that can reason over images or\naudio, support ",(0,t.jsx)(n.a,{href:"../../docs/concepts/#tool-calling",children:"tool calling"}),"\nfeatures as well."]}),"\n",(0,t.jsxs)(n.p,{children:["To call tools using such models, simply bind tools to them in the ",(0,t.jsx)(n.a,{href:"../../docs/how_to/tool_calling",children:"usual\nway"}),", and invoke the model using content\nblocks of the desired type (e.g., containing image data)."]}),"\n",(0,t.jsxs)(n.p,{children:["Below, we demonstrate examples using\n",(0,t.jsx)(n.a,{href:"../../docs/integrations/platforms/openai",children:"OpenAI"})," and\n",(0,t.jsx)(n.a,{href:"../../docs/integrations/platforms/anthropic",children:"Anthropic"}),". We will use\nthe same image and tool in all cases. Let\u2019s first select an image, and\nbuild a placeholder tool that expects as input the string \u201csunny\u201d,\n\u201ccloudy\u201d, or \u201crainy\u201d. We will ask the models to describe the weather in\nthe image."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { DynamicStructuredTool } from "@langchain/core/tools";\nimport { z } from "zod";\n\nconst imageUrl =\n  "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg";\n\nconst weatherTool = new DynamicStructuredTool({\n  name: "multiply",\n  description: "Describe the weather",\n  schema: z.object({\n    weather: z.enum(["sunny", "cloudy", "rainy"]),\n  }),\n  func: async ({ weather }) => {\n    console.log(weather);\n    return weather;\n  },\n});\n'})}),"\n",(0,t.jsx)(n.h2,{id:"openai",children:"OpenAI"}),"\n",(0,t.jsx)(n.p,{children:"For OpenAI, we can feed the image URL directly in a content block of\ntype \u201cimage_url\u201d:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { HumanMessage } from "@langchain/core/messages";\nimport { ChatOpenAI } from "@langchain/openai";\n\nconst model = new ChatOpenAI({\n  model: "gpt-4o",\n}).bindTools([weatherTool]);\n\nconst message = new HumanMessage({\n  content: [\n    {\n      type: "text",\n      text: "describe the weather in this image",\n    },\n    {\n      type: "image_url",\n      image_url: {\n        url: imageUrl,\n      },\n    },\n  ],\n});\n\nconst response = await model.invoke([message]);\n\nconsole.log(response.tool_calls);\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'[\n  {\n    name: "multiply",\n    args: { weather: "sunny" },\n    id: "call_MbIAYS9ESBG1EWNM2sMlinjR"\n  }\n]\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Note that we recover tool calls with parsed arguments in LangChain\u2019s\n",(0,t.jsx)(n.a,{href:"../../docs/how_to/tool_calling",children:"standard format"})," in the model response."]}),"\n",(0,t.jsx)(n.h2,{id:"anthropic",children:"Anthropic"}),"\n",(0,t.jsx)(n.p,{children:"For Anthropic, we can format a base64-encoded image into a content block\nof type \u201cimage\u201d, as below:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import * as fs from "node:fs/promises";\n\nimport { ChatAnthropic } from "@langchain/anthropic";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst imageData = await fs.readFile("../../data/sunny_day.jpeg");\n\nconst model = new ChatAnthropic({\n  model: "claude-3-sonnet-20240229",\n}).bindTools([weatherTool]);\n\nconst message = new HumanMessage({\n  content: [\n    {\n      type: "text",\n      text: "describe the weather in this image",\n    },\n    {\n      type: "image_url",\n      image_url: {\n        url: `data:image/jpeg;base64,${imageData.toString("base64")}`,\n      },\n    },\n  ],\n});\n\nconst response = await model.invoke([message]);\n\nconsole.log(response.tool_calls);\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'[\n  {\n    name: "multiply",\n    args: { weather: "sunny" },\n    id: "toolu_01KnRZWQkgWYSzL2x28crXFm"\n  }\n]\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);