(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3878,65],{67858:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>m,default:()=>j,frontMatter:()=>h,metadata:()=>p,toc:()=>g});var a=t(74848),l=t(28453),o=t(78847),i=t(64428),s=t(6855),r=t.n(s),d=t(15026),c=t.n(d);const h={sidebar_class_name:"node-only"},m="Llama CPP",p={id:"integrations/llms/llama_cpp",title:"Llama CPP",description:"Only available on Node.js.",source:"@site/docs/integrations/llms/llama_cpp.mdx",sourceDirName:"integrations/llms",slug:"/integrations/llms/llama_cpp",permalink:"/docs/integrations/llms/llama_cpp",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/llms/llama_cpp.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"node-only"},sidebar:"integrations",previous:{title:"Layerup Security",permalink:"/docs/integrations/llms/layerup_security"},next:{title:"NIBittensor",permalink:"/docs/integrations/llms/ni_bittensor"}},u={},g=[{value:"Setup",id:"setup",level:2},...o.toc,{value:"Guide to installing Llama2",id:"guide-to-installing-llama2",level:2},{value:"Getting the Llama2 models",id:"getting-the-llama2-models",level:3},{value:"Converting and quantizing the model",id:"converting-and-quantizing-the-model",level:3},{value:"Usage",id:"usage",level:2},{value:"Streaming",id:"streaming",level:2}];function x(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"llama-cpp",children:"Llama CPP"}),"\n",(0,a.jsx)(n.admonition,{title:"Compatibility",type:"tip",children:(0,a.jsx)(n.p,{children:"Only available on Node.js."})}),"\n",(0,a.jsxs)(n.p,{children:["This module is based on the ",(0,a.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," Node.js bindings for ",(0,a.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp"}),", allowing you to work with a locally running LLM. This allows you to work with a much smaller quantized model capable of running on a laptop environment, ideal for testing and scratch padding ideas without running up a bill!"]}),"\n",(0,a.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsxs)(n.p,{children:["You'll need to install the ",(0,a.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," module to communicate with your local model."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install -S node-llama-cpp\n"})}),"\n","\n",(0,a.jsx)(o.default,{}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/community\n"})}),"\n",(0,a.jsxs)(n.p,{children:["You will also need a local Llama 2 model (or a model supported by ",(0,a.jsx)(n.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"}),"). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example)."]}),"\n",(0,a.jsxs)(n.p,{children:["Out-of-the-box ",(0,a.jsx)(n.code,{children:"node-llama-cpp"})," is tuned for running on a MacOS platform with support for the Metal GPU of Apple M-series of processors. If you need to turn this off or need support for the CUDA architecture then refer to the documentation at ",(0,a.jsx)(n.a,{href:"https://withcatai.github.io/node-llama-cpp/",children:"node-llama-cpp"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["A note to LangChain.js contributors: if you want to run the tests associated with this module you will need to put the path to your local model in the environment variable ",(0,a.jsx)(n.code,{children:"LLAMA_PATH"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"guide-to-installing-llama2",children:"Guide to installing Llama2"}),"\n",(0,a.jsxs)(n.p,{children:["Getting a local Llama2 model running on your machine is a pre-req so this is a quick guide to getting and building Llama 7B (the smallest) and then quantizing it so that it will run comfortably on a laptop. To do this you will need ",(0,a.jsx)(n.code,{children:"python3"})," on your machine (3.11 is recommended), also ",(0,a.jsx)(n.code,{children:"gcc"})," and ",(0,a.jsx)(n.code,{children:"make"})," so that ",(0,a.jsx)(n.code,{children:"llama.cpp"})," can be built."]}),"\n",(0,a.jsx)(n.h3,{id:"getting-the-llama2-models",children:"Getting the Llama2 models"}),"\n",(0,a.jsxs)(n.p,{children:["To get a copy of Llama2 you need to visit ",(0,a.jsx)(n.a,{href:"https://ai.meta.com/resources/models-and-libraries/llama-downloads/",children:"Meta AI"})," and request access to their models. Once Meta AI grant you access, you will receive an email containing a unique URL to access the files, this will be needed in the next steps.\nNow create a directory to work in, for example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"mkdir llama2\ncd llama2\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now we need to get the Meta AI ",(0,a.jsx)(n.code,{children:"llama"})," repo in place so we can download the model."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"git clone https://github.com/facebookresearch/llama.git\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Once we have this in place we can change into this directory and run the downloader script to get the model we will be working with. Note: From here on its assumed that the model in use is ",(0,a.jsx)(n.code,{children:"llama-2\u20137b"}),", if you select a different model don't forget to change the references to the model accordingly."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"cd llama\n/bin/bash ./download.sh\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This script will ask you for the URL that Meta AI sent to you (see above), you will also select the model to download, in this case we used ",(0,a.jsx)(n.code,{children:"llama-2\u20137b"}),". Once this step has completed successfully (this can take some time, the ",(0,a.jsx)(n.code,{children:"llama-2\u20137b"})," model is around 13.5Gb) there should be a new ",(0,a.jsx)(n.code,{children:"llama-2\u20137b"})," directory containing the model and other files."]}),"\n",(0,a.jsx)(n.h3,{id:"converting-and-quantizing-the-model",children:"Converting and quantizing the model"}),"\n",(0,a.jsxs)(n.p,{children:["In this step we need to use ",(0,a.jsx)(n.code,{children:"llama.cpp"})," so we need to download that repo."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"cd ..\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Now we need to build the ",(0,a.jsx)(n.code,{children:"llama.cpp"})," tools and set up our ",(0,a.jsx)(n.code,{children:"python"})," environment. In these steps it's assumed that your install of python can be run using ",(0,a.jsx)(n.code,{children:"python3"})," and that the virtual environment can be called ",(0,a.jsx)(n.code,{children:"llama2"}),", adjust accordingly for your own situation."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"make\npython3 -m venv llama2\nsource llama2/bin/activate\n"})}),"\n",(0,a.jsxs)(n.p,{children:["After activating your llama2 environment you should see ",(0,a.jsx)(n.code,{children:"(llama2)"})," prefixing your command prompt to let you know this is the active environment. Note: if you need to come back to build another model or re-quantize the model don't forget to activate the environment again also if you update ",(0,a.jsx)(n.code,{children:"llama.cpp"})," you will need to rebuild the tools and possibly install new or updated dependencies! Now that we have an active python environment, we need to install the python dependencies."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"python3 -m pip install -r requirements.txt\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Having done this, we can start converting and quantizing the Llama2 model ready for use locally via ",(0,a.jsx)(n.code,{children:"llama.cpp"}),".\nFirst, we need to convert the model, prior to the conversion let's create a directory to store it in."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"mkdir models/7B\npython3 convert.py --outfile models/7B/gguf-llama2-f16.bin --outtype f16 ../../llama2/llama/llama-2-7b --vocab-dir ../../llama2/llama/llama-2-7b\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This should create a converted model called ",(0,a.jsx)(n.code,{children:"gguf-llama2-f16.bin"})," in the directory we just created. Note that this is just a converted model so it is also around 13.5Gb in size, in the next step we will quantize it down to around 4Gb."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"./quantize ./models/7B/gguf-llama2-f16.bin ./models/7B/gguf-llama2-q4_0.bin q4_0\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Running this should result in a new model being created in the ",(0,a.jsx)(n.code,{children:"models\\7B"})," directory, this one called ",(0,a.jsx)(n.code,{children:"gguf-llama2-q4_0.bin"}),", this is the model we can use with langchain. You can validate this model is working by testing it using the ",(0,a.jsx)(n.code,{children:"llama.cpp"})," tools."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'./main -m ./models/7B/gguf-llama2-q4_0.bin -n 1024 --repeat_penalty 1.0 --color -i -r "User:" -f ./prompts/chat-with-bob.txt\n'})}),"\n",(0,a.jsx)(n.p,{children:"Running this command fires up the model for a chat session. BTW if you are running out of disk space this small model is the only one we need, so you can backup and/or delete the original and converted 13.5Gb models."}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n","\n",(0,a.jsx)(i.A,{language:"typescript",children:r()}),"\n",(0,a.jsx)(n.h2,{id:"streaming",children:"Streaming"}),"\n","\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(i.A,{language:"typescript",children:c()}),";"]})]})}function j(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(x,{...e})}):x(e)}},78847:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var a=t(74848),l=t(28453);const o={},i=void 0,s={id:"mdx_components/integration_install_tooltip",title:"integration_install_tooltip",description:"See this section for general instructions on installing integration packages.",source:"@site/docs/mdx_components/integration_install_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/integration_install_tooltip",permalink:"/docs/mdx_components/integration_install_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/integration_install_tooltip.mdx",tags:[],version:"current",frontMatter:{}},r={},d=[];function c(e){const n={a:"a",admonition:"admonition",p:"p",...(0,l.R)(),...e.components};return(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"/docs/how_to/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},6855:e=>{e.exports={content:'import { LlamaCpp } from "@langchain/community/llms/llama_cpp";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\nconst question = "Where do Llamas come from?";\n\nconst model = new LlamaCpp({ modelPath: llamaPath });\n\nconsole.log(`You: ${question}`);\nconst response = await model.invoke(question);\nconsole.log(`AI : ${response}`);\n',imports:[{local:"LlamaCpp",imported:"LlamaCpp",source:"@langchain/community/llms/llama_cpp"}]}},15026:e=>{e.exports={content:'import { LlamaCpp } from "@langchain/community/llms/llama_cpp";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst model = new LlamaCpp({ modelPath: llamaPath, temperature: 0.7 });\n\nconst prompt = "Tell me a short story about a happy Llama.";\n\nconst stream = await model.stream(prompt);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\n\n\n Once\n  upon\n  a\n  time\n ,\n  in\n  the\n  rolling\n  hills\n  of\n  Peru\n ...\n */\n',imports:[{local:"LlamaCpp",imported:"LlamaCpp",source:"@langchain/community/llms/llama_cpp"}]}}}]);