"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6690],{93309:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>h});var r=t(74848),a=t(28453),o=t(78847),s=t(27846);const i={sidebar_class_name:"hidden",title:"How to handle long text"},c=void 0,l={id:"how_to/extraction_long_text",title:"How to handle long text",description:"This guide assumes familiarity with the following:",source:"@site/docs/how_to/extraction_long_text.mdx",sourceDirName:"how_to",slug:"/how_to/extraction_long_text",permalink:"/docs/how_to/extraction_long_text",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/extraction_long_text.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to handle long text"},sidebar:"tutorialSidebar",previous:{title:"How to use reference examples",permalink:"/docs/how_to/extraction_examples"},next:{title:"How to do extraction without using function calling",permalink:"/docs/how_to/extraction_parse"}},d={},h=[{value:"Set up",id:"set-up",level:2},...o.toc,{value:"Define the schema",id:"define-the-schema",level:2},{value:"Brute force approach",id:"brute-force-approach",level:2},{value:"Merge results",id:"merge-results",level:3},{value:"RAG based approach",id:"rag-based-approach",level:2},{value:"Common issues",id:"common-issues",level:2},{value:"Next steps",id:"next-steps",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,r.jsx)(n.p,{children:"This guide assumes familiarity with the following:"}),(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../../docs/tutorials/extraction",children:"Extraction"})}),"\n"]})]}),"\n",(0,r.jsx)(n.p,{children:"When working with files, like PDFs, you\u2019re likely to encounter text that\nexceeds your language model\u2019s context window. To process this text,\nconsider these strategies:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Change LLM"})," Choose a different LLM that supports a larger context\nwindow."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Brute Force"})," Chunk the document, and extract content from each\nchunk."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RAG"})," Chunk the document, index the chunks, and only extract\ncontent from a subset of chunks that look \u201crelevant\u201d."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Keep in mind that these strategies have different trade offs and the\nbest strategy likely depends on the application that you\u2019re designing!"}),"\n",(0,r.jsx)(n.h2,{id:"set-up",children:"Set up"}),"\n",(0,r.jsx)(n.p,{children:"First, let\u2019s install some required dependencies:"}),"\n","\n",(0,r.jsx)(o.default,{}),"\n",(0,r.jsx)(s.A,{children:(0,r.jsx)(n.p,{children:"@langchain/openai zod cheerio"})}),"\n",(0,r.jsxs)(n.p,{children:["Next, we need some example data! Let\u2019s download an article about ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Car",children:"cars\nfrom Wikipedia"})," and load it as a\nLangChain ",(0,r.jsx)(n.code,{children:"Document"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'import { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";\n// Only required in a Deno notebook environment to load the peer dep.\nimport "cheerio";\n\nconst loader = new CheerioWebBaseLoader("https://en.wikipedia.org/wiki/Car");\n\nconst docs = await loader.load();\n\ndocs[0].pageContent.length;\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"97336\n"})}),"\n",(0,r.jsx)(n.h2,{id:"define-the-schema",children:"Define the schema"}),"\n",(0,r.jsx)(n.p,{children:"Here, we\u2019ll define schema to extract key developments from the text."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'import { z } from "zod";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { ChatOpenAI } from "@langchain/openai";\n\nconst keyDevelopmentSchema = z\n  .object({\n    year: z\n      .number()\n      .describe("The year when there was an important historic development."),\n    description: z\n      .string()\n      .describe("What happened in this year? What was the development?"),\n    evidence: z\n      .string()\n      .describe(\n        "Repeat verbatim the sentence(s) from which the year and description information were extracted"\n      ),\n  })\n  .describe("Information about a development in the history of cars.");\n\nconst extractionDataSchema = z\n  .object({\n    key_developments: z.array(keyDevelopmentSchema),\n  })\n  .describe(\n    "Extracted information about key developments in the history of cars"\n  );\n\nconst SYSTEM_PROMPT_TEMPLATE = [\n  "You are an expert at identifying key historic development in text.",\n  "Only extract important historic developments. Extract nothing if no important information can be found in the text.",\n].join("\\n");\n\n// Define a custom prompt to provide instructions and any additional context.\n// 1) You can add examples into the prompt template to improve extraction quality\n// 2) Introduce additional parameters to take context into account (e.g., include metadata\n//    about the document from which the text was extracted.)\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", SYSTEM_PROMPT_TEMPLATE],\n  // Keep on reading through this use case to see how to use examples to improve performance\n  // MessagesPlaceholder(\'examples\'),\n  ["human", "{text}"],\n]);\n\n// We will be using tool calling mode, which\n// requires a tool calling capable model.\nconst llm = new ChatOpenAI({\n  model: "gpt-4-0125-preview",\n  temperature: 0,\n});\n\nconst extractionChain = prompt.pipe(\n  llm.withStructuredOutput(extractionDataSchema)\n);\n'})}),"\n",(0,r.jsx)(n.h2,{id:"brute-force-approach",children:"Brute force approach"}),"\n",(0,r.jsx)(n.p,{children:"Split the documents into chunks such that each chunk fits into the\ncontext window of the LLMs."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'import { TokenTextSplitter } from "langchain/text_splitter";\n\nconst textSplitter = new TokenTextSplitter({\n  chunkSize: 2000,\n  chunkOverlap: 20,\n});\n\n// Note that this method takes an array of docs\nconst splitDocs = await textSplitter.splitDocuments(docs);\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Use the ",(0,r.jsx)(n.code,{children:".batch"})," method present on all runnables to run the extraction\nin ",(0,r.jsx)(n.strong,{children:"parallel"})," across each chunk!"]}),"\n",(0,r.jsxs)(n.admonition,{type:"tip",children:[(0,r.jsxs)(n.p,{children:["You can often use ",(0,r.jsx)(n.code,{children:".batch()"})," to parallelize the extractions!"]}),(0,r.jsx)(n.p,{children:"If your model is exposed via an API, this will likely speed up your\nextraction flow."})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"// Limit just to the first 3 chunks\n// so the code can be re-run quickly\nconst firstFewTexts = splitDocs.slice(0, 3).map((doc) => doc.pageContent);\n\nconst extractionChainParams = firstFewTexts.map((text) => {\n  return { text };\n});\n\nconst results = await extractionChain.batch(extractionChainParams, {\n  maxConcurrency: 5,\n});\n"})}),"\n",(0,r.jsx)(n.h3,{id:"merge-results",children:"Merge results"}),"\n",(0,r.jsx)(n.p,{children:"After extracting data from across the chunks, we\u2019ll want to merge the\nextractions together."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"const keyDevelopments = results.flatMap((result) => result.key_developments);\n\nkeyDevelopments.slice(0, 20);\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:'[\n  { year: 0, description: "", evidence: "" },\n  {\n    year: 1769,\n    description: "French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle.",\n    evidence: "French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769."\n  },\n  {\n    year: 1808,\n    description: "French-born Swiss inventor Fran\xe7ois Isaac de Rivaz designed and constructed the first internal combu"... 25 more characters,\n    evidence: "French-born Swiss inventor Fran\xe7ois Isaac de Rivaz designed and constructed the first internal combu"... 33 more characters\n  },\n  {\n    year: 1886,\n    description: "German inventor Carl Benz patented his Benz Patent-Motorwagen, inventing the modern car\u2014a practical,"... 40 more characters,\n    evidence: "The modern car\u2014a practical, marketable automobile for everyday use\u2014was invented in 1886, when German"... 56 more characters\n  },\n  {\n    year: 1908,\n    description: "The 1908 Model T, an American car manufactured by the Ford Motor Company, became one of the first ca"... 28 more characters,\n    evidence: "One of the first cars affordable by the masses was the 1908 Model T, an American car manufactured by"... 24 more characters\n  }\n]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"rag-based-approach",children:"RAG based approach"}),"\n",(0,r.jsx)(n.p,{children:"Another simple idea is to chunk up the text, but instead of extracting\ninformation from every chunk, just focus on the the most relevant\nchunks."}),"\n",(0,r.jsxs)(n.admonition,{type:"caution",children:[(0,r.jsx)(n.p,{children:"It can be difficult to identify which chunks are relevant."}),(0,r.jsxs)(n.p,{children:["For example, in the ",(0,r.jsx)(n.code,{children:"car"})," article we\u2019re using here, most of the article\ncontains key development information. So by using ",(0,r.jsx)(n.strong,{children:"RAG"}),", we\u2019ll likely\nbe throwing out a lot of relevant information."]}),(0,r.jsx)(n.p,{children:"We suggest experimenting with your use case and determining whether this\napproach works or not."})]}),"\n",(0,r.jsxs)(n.p,{children:["Here\u2019s a simple example that relies on an in-memory demo\n",(0,r.jsx)(n.code,{children:"MemoryVectorStore"})," vectorstore."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'import { MemoryVectorStore } from "langchain/vectorstores/memory";\nimport { OpenAIEmbeddings } from "@langchain/openai";\n\n// Only load the first 10 docs for speed in this demo use-case\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  splitDocs.slice(0, 10),\n  new OpenAIEmbeddings()\n);\n\n// Only extract from top document\nconst retriever = vectorstore.asRetriever({ k: 1 });\n'})}),"\n",(0,r.jsx)(n.p,{children:"In this case the RAG extractor is only looking at the top document."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'import { RunnableSequence } from "@langchain/core/runnables";\n\nconst ragExtractor = RunnableSequence.from([\n  {\n    text: retriever.pipe((docs) => docs[0].pageContent),\n  },\n  extractionChain,\n]);\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const results = await ragExtractor.invoke(\n  "Key developments associated with cars"\n);\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"results.key_developments;\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:'[\n  {\n    year: 2020,\n    description: "The lifetime of a car built in the 2020s is expected to be about 16 years, or about 2 million km (1."... 33 more characters,\n    evidence: "The lifetime of a car built in the 2020s is expected to be about 16 years, or about 2 millionkm (1.2"... 31 more characters\n  },\n  {\n    year: 2030,\n    description: "All fossil fuel vehicles will be banned in Amsterdam from 2030.",\n    evidence: "all fossil fuel vehicles will be banned in Amsterdam from 2030."\n  },\n  {\n    year: 2020,\n    description: "In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year.",\n    evidence: "In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year."\n  }\n]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"common-issues",children:"Common issues"}),"\n",(0,r.jsx)(n.p,{children:"Different methods have their own pros and cons related to cost, speed,\nand accuracy."}),"\n",(0,r.jsx)(n.p,{children:"Watch out for these issues:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Chunking content means that the LLM can fail to extract information\nif the information is spread across multiple chunks."}),"\n",(0,r.jsx)(n.li,{children:"Large chunk overlap may cause the same information to be extracted\ntwice, so be prepared to de-duplicate!"}),"\n",(0,r.jsx)(n.li,{children:"LLMs can make up data. If looking for a single fact across a large\ntext and using a brute force approach, you may end up getting more\nmade up data."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,r.jsx)(n.p,{children:"You\u2019ve now learned how to improve extraction quality using few-shot\nexamples."}),"\n",(0,r.jsxs)(n.p,{children:["Next, check out some of the other guides in this section, such as ",(0,r.jsx)(n.a,{href:"../../docs/how_to/extraction_examples",children:"some\ntips on how to improve extraction quality with\nexamples"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},19365:(e,n,t)=>{t.d(n,{A:()=>s});t(96540);var r=t(34164);const a={tabItem:"tabItem_Ymn6"};var o=t(74848);function s(e){let{children:n,hidden:t,className:s}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,r.A)(a.tabItem,s),hidden:t,children:n})}},11470:(e,n,t)=>{t.d(n,{A:()=>j});var r=t(96540),a=t(34164),o=t(23104),s=t(56347),i=t(205),c=t(57485),l=t(31682),d=t(89466);function h(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,r.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:r,default:a}}=e;return{value:n,label:t,attributes:r,default:a}}))}(t);return function(e){const n=(0,l.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const a=(0,s.W6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,c.aZ)(o),(0,r.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(a.location.search);n.set(o,e),a.replace({...a.location,search:n.toString()})}),[o,a])]}function x(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,o=u(e),[s,c]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const r=t.find((e=>e.default))??t[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:o}))),[l,h]=m({queryString:t,groupId:a}),[x,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,o]=(0,d.Dv)(t);return[a,(0,r.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:a}),g=(()=>{const e=l??x;return p({value:e,tabValues:o})?e:null})();(0,i.A)((()=>{g&&c(g)}),[g]);return{selectedValue:s,selectValue:(0,r.useCallback)((e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);c(e),h(e),f(e)}),[h,f,o]),tabValues:o}}var f=t(92303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function v(e){let{className:n,block:t,selectedValue:r,selectValue:s,tabValues:i}=e;const c=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),d=e=>{const n=e.currentTarget,t=c.indexOf(n),a=i[t].value;a!==r&&(l(n),s(a))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},n),children:i.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:r===n?0:-1,"aria-selected":r===n,ref:e=>c.push(e),onKeyDown:h,onClick:d,...o,className:(0,a.A)("tabs__item",g.tabItem,o?.className,{"tabs__item--active":r===n}),children:t??n},n)}))})}function w(e){let{lazy:n,children:t,selectedValue:a}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===a));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==a})))})}function y(e){const n=x(e);return(0,b.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,b.jsx)(v,{...n,...e}),(0,b.jsx)(w,{...n,...e})]})}function j(e){const n=(0,f.A)();return(0,b.jsx)(y,{...e,children:h(e.children)},String(n))}},27846:(e,n,t)=>{t.d(n,{A:()=>i});t(96540);var r=t(11470),a=t(19365),o=t(21432),s=t(74848);function i(e){let{children:n}=e;return(0,s.jsxs)(r.A,{groupId:"npm2yarn",children:[(0,s.jsx)(a.A,{value:"npm",label:"npm",children:(0,s.jsxs)(o.A,{language:"bash",children:["npm i ",n]})}),(0,s.jsx)(a.A,{value:"yarn",label:"yarn",default:!0,children:(0,s.jsxs)(o.A,{language:"bash",children:["yarn add ",n]})}),(0,s.jsx)(a.A,{value:"pnpm",label:"pnpm",children:(0,s.jsxs)(o.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);