(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[506],{89394:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>b,contentTitle:()=>f,default:()=>x,frontMatter:()=>g,metadata:()=>w,toc:()=>k});var a=t(74848),o=t(28453),r=t(64428),i=t(234),s=t.n(i),l=t(78847),c=t(22058),p=t.n(c),h=t(3137),m=t.n(h),d=t(34563),u=t.n(d);const g={sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},f="Fallbacks",w={id:"how_to/fallbacks",title:"Fallbacks",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/fallbacks.mdx",sourceDirName:"how_to",slug:"/how_to/fallbacks",permalink:"/docs/how_to/fallbacks",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/fallbacks.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},sidebar:"tutorialSidebar"},b={},k=[{value:"Handling LLM API errors",id:"handling-llm-api-errors",level:2},...l.toc,{value:"Fallbacks for RunnableSequences",id:"fallbacks-for-runnablesequences",level:2},{value:"Handling long inputs",id:"handling-long-inputs",level:2},{value:"Fallback to a better model",id:"fallback-to-a-better-model",level:2}];function A(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"fallbacks",children:"Fallbacks"}),"\n",(0,a.jsxs)(e.admonition,{title:"Prerequisites",type:"info",children:[(0,a.jsx)(e.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"/docs/concepts/#langchain-expression-language",children:"LangChain Expression Language (LCEL)"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"/docs/how_to/sequence/",children:"Chaining runnables"})}),"\n"]})]}),"\n",(0,a.jsx)(e.p,{children:"When working with language models, you may encounter issues from the underlying APIs, e.g. rate limits or downtime.\nAs you move your LLM applications into production it becomes more and more important to have contingencies for errors.\nThat's why we've introduced the concept of fallbacks."}),"\n",(0,a.jsx)(e.p,{children:"Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level.\nThis is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want want to use e.g. a different prompt template."}),"\n",(0,a.jsx)(e.h2,{id:"handling-llm-api-errors",children:"Handling LLM API errors"}),"\n",(0,a.jsx)(e.p,{children:"This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down,\nyou could have hit a rate limit, or any number of things."}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"IMPORTANT:"})," By default, many of LangChain's LLM wrappers catch errors and retry.\nYou will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying rather than failing."]}),"\n","\n","\n",(0,a.jsx)(l.default,{}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/anthropic @langchain/openai\n"})}),"\n",(0,a.jsx)(r.A,{language:"typescript",children:s()}),"\n",(0,a.jsx)(e.h2,{id:"fallbacks-for-runnablesequences",children:"Fallbacks for RunnableSequences"}),"\n",(0,a.jsx)(e.p,{children:"We can also create fallbacks for sequences, that are sequences themselves.\nHere we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model).\nBecause OpenAI is NOT a chat model, you likely want a different prompt."}),"\n","\n",(0,a.jsx)(r.A,{language:"typescript",children:p()}),"\n",(0,a.jsx)(e.h2,{id:"handling-long-inputs",children:"Handling long inputs"}),"\n",(0,a.jsx)(e.p,{children:"One of the big limiting factors of LLMs in their context window.\nSometimes you can count and track the length of prompts before sending them to an LLM,\nbut in situations where that is hard/complicated you can fallback to a model with longer context length."}),"\n","\n",(0,a.jsx)(r.A,{language:"typescript",children:m()}),"\n",(0,a.jsx)(e.h2,{id:"fallback-to-a-better-model",children:"Fallback to a better model"}),"\n",(0,a.jsx)(e.p,{children:"Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle.\nThis naturally points to fallbacks - we can try with a faster and cheaper model, but then if parsing fails we can use GPT-4."}),"\n","\n",(0,a.jsx)(r.A,{language:"typescript",children:u()})]})}function x(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(A,{...n})}):A(n)}},34563:n=>{n.exports={content:'import { z } from "zod";\nimport { OpenAI, ChatOpenAI } from "@langchain/openai";\nimport { PromptTemplate } from "@langchain/core/prompts";\nimport { StructuredOutputParser } from "@langchain/core/output_parsers";\n\nconst prompt = PromptTemplate.fromTemplate(\n  `Return a JSON object containing the following value wrapped in an "input" key. Do not return anything else:\\n{input}`\n);\n\nconst badModel = new OpenAI({\n  maxRetries: 0,\n  model: "gpt-3.5-turbo-instruct",\n});\n\nconst normalModel = new ChatOpenAI({\n  model: "gpt-4",\n});\n\nconst outputParser = StructuredOutputParser.fromZodSchema(\n  z.object({\n    input: z.string(),\n  })\n);\n\nconst badChain = prompt.pipe(badModel).pipe(outputParser);\n\nconst goodChain = prompt.pipe(normalModel).pipe(outputParser);\n\ntry {\n  const result = await badChain.invoke({\n    input: "testing0",\n  });\n} catch (e) {\n  console.log(e);\n  /*\n  OutputParserException [Error]: Failed to parse. Text: "\n\n  { "name" : " Testing0 ", "lastname" : " testing ", "fullname" : " testing ", "role" : " test ", "telephone" : "+1-555-555-555 ", "email" : " testing@gmail.com ", "role" : " test ", "text" : " testing0 is different than testing ", "role" : " test ", "immediate_affected_version" : " 0.0.1 ", "immediate_version" : " 1.0.0 ", "leading_version" : " 1.0.0 ", "version" : " 1.0.0 ", "finger prick" : " no ", "finger prick" : " s ", "text" : " testing0 is different than testing ", "role" : " test ", "immediate_affected_version" : " 0.0.1 ", "immediate_version" : " 1.0.0 ", "leading_version" : " 1.0.0 ", "version" : " 1.0.0 ", "finger prick" :". Error: SyntaxError: Unexpected end of JSON input\n*/\n}\n\nconst chain = badChain.withFallbacks({\n  fallbacks: [goodChain],\n});\n\nconst result = await chain.invoke({\n  input: "testing",\n});\n\nconsole.log(result);\n\n/*\n  { input: \'testing\' }\n*/\n',imports:[{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"},{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"PromptTemplate",imported:"PromptTemplate",source:"@langchain/core/prompts"},{local:"StructuredOutputParser",imported:"StructuredOutputParser",source:"@langchain/core/output_parsers"}]}},22058:n=>{n.exports={content:'import { ChatOpenAI, OpenAI } from "@langchain/openai";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\nimport { ChatPromptTemplate, PromptTemplate } from "@langchain/core/prompts";\n\nconst chatPrompt = ChatPromptTemplate.fromMessages<{ animal: string }>([\n  [\n    "system",\n    "You\'re a nice assistant who always includes a compliment in your response",\n  ],\n  ["human", "Why did the {animal} cross the road?"],\n]);\n\n// Use a fake model name that will always throw an error\nconst fakeOpenAIChatModel = new ChatOpenAI({\n  model: "potato!",\n  maxRetries: 0,\n});\n\nconst prompt =\n  PromptTemplate.fromTemplate(`Instructions: You should always include a compliment in your response.\n\nQuestion: Why did the {animal} cross the road?\n\nAnswer:`);\n\nconst openAILLM = new OpenAI({});\n\nconst outputParser = new StringOutputParser();\n\nconst badChain = chatPrompt.pipe(fakeOpenAIChatModel).pipe(outputParser);\n\nconst goodChain = prompt.pipe(openAILLM).pipe(outputParser);\n\nconst chain = badChain.withFallbacks({\n  fallbacks: [goodChain],\n});\n\nconst result = await chain.invoke({\n  animal: "dragon",\n});\n\nconsole.log(result);\n\n/*\n  I don\'t know, but I\'m sure it was an impressive sight. You must have a great imagination to come up with such an interesting question!\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"},{local:"StringOutputParser",imported:"StringOutputParser",source:"@langchain/core/output_parsers"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"PromptTemplate",imported:"PromptTemplate",source:"@langchain/core/prompts"}]}},3137:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\n\n// Use a model with a shorter context window\nconst shorterLlm = new ChatOpenAI({\n  model: "gpt-3.5-turbo",\n  maxRetries: 0,\n});\n\nconst longerLlm = new ChatOpenAI({\n  model: "gpt-3.5-turbo-16k",\n});\n\nconst modelWithFallback = shorterLlm.withFallbacks({\n  fallbacks: [longerLlm],\n});\n\nconst input = `What is the next number: ${"one, two, ".repeat(3000)}`;\n\ntry {\n  await shorterLlm.invoke(input);\n} catch (e) {\n  // Length error\n  console.log(e);\n}\n\nconst result = await modelWithFallback.invoke(input);\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: \'The next number is one.\',\n    name: undefined,\n    additional_kwargs: { function_call: undefined }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}},234:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\nimport { ChatAnthropic } from "@langchain/anthropic";\n\n// Use a fake model name that will always throw an error\nconst fakeOpenAIModel = new ChatOpenAI({\n  model: "potato!",\n  maxRetries: 0,\n});\n\nconst anthropicModel = new ChatAnthropic({});\n\nconst modelWithFallback = fakeOpenAIModel.withFallbacks({\n  fallbacks: [anthropicModel],\n});\n\nconst result = await modelWithFallback.invoke("What is your name?");\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: \' My name is Claude. I was created by Anthropic.\',\n    additional_kwargs: {}\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"ChatAnthropic",imported:"ChatAnthropic",source:"@langchain/anthropic"}]}}}]);