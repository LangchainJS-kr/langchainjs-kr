(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9359],{77016:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>p,default:()=>d,frontMatter:()=>l,metadata:()=>h,toc:()=>g});var o=t(74848),a=t(28453),s=t(64428),r=t(60141),i=t.n(r),c=t(78847);const l={hide_table_of_contents:!0},p="SearchApi tool",h={id:"integrations/tools/searchapi",title:"SearchApi tool",description:"The SearchApi tool connects your agents and chains to the internet.",source:"@site/docs/integrations/tools/searchapi.mdx",sourceDirName:"integrations/tools",slug:"/integrations/tools/searchapi",permalink:"/docs/integrations/tools/searchapi",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/tools/searchapi.mdx",tags:[],version:"current",frontMatter:{hide_table_of_contents:!0},sidebar:"integrations",previous:{title:"Python interpreter tool",permalink:"/docs/integrations/tools/pyinterpreter"},next:{title:"Searxng Search tool",permalink:"/docs/integrations/tools/searxng"}},u={},g=[{value:"Usage",id:"usage",level:2},...c.toc];function m(e){const n={code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"searchapi-tool",children:"SearchApi tool"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"SearchApi"})," tool connects your agents and chains to the internet."]}),"\n",(0,o.jsx)(n.p,{children:"A wrapper around the Search API. This tool is handy when you need to answer questions about current events."}),"\n",(0,o.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,o.jsx)(n.p,{children:"Input should be a search query."}),"\n","\n","\n",(0,o.jsx)(c.default,{}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai\n"})}),"\n",(0,o.jsx)(s.A,{language:"typescript",children:i()})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(m,{...e})}):m(e)}},60141:e=>{e.exports={content:'import { ChatOpenAI } from "@langchain/openai";\nimport { AgentExecutor } from "langchain/agents";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { RunnableSequence } from "@langchain/core/runnables";\nimport { AgentFinish, AgentAction } from "@langchain/core/agents";\nimport { BaseMessageChunk } from "@langchain/core/messages";\nimport { SearchApi } from "@langchain/community/tools/searchapi";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n});\nconst tools = [\n  new SearchApi(process.env.SEARCHAPI_API_KEY, {\n    engine: "google_news",\n  }),\n];\nconst prefix = ChatPromptTemplate.fromMessages([\n  [\n    "ai",\n    "Answer the following questions as best you can. In your final answer, use a bulleted list markdown format.",\n  ],\n  ["human", "{input}"],\n]);\n// Replace this with your actual output parser.\nconst customOutputParser = (\n  input: BaseMessageChunk\n): AgentAction | AgentFinish => ({\n  log: "test",\n  returnValues: {\n    output: input,\n  },\n});\n// Replace this placeholder agent with your actual implementation.\nconst agent = RunnableSequence.from([prefix, model, customOutputParser]);\nconst executor = AgentExecutor.fromAgentAndTools({\n  agent,\n  tools,\n});\nconst res = await executor.invoke({\n  input: "What\'s happening in Ukraine today?",\n});\nconsole.log(res);\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"AgentExecutor",imported:"AgentExecutor",source:"langchain/agents"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"RunnableSequence",imported:"RunnableSequence",source:"@langchain/core/runnables"},{local:"AgentFinish",imported:"AgentFinish",source:"@langchain/core/agents"},{local:"AgentAction",imported:"AgentAction",source:"@langchain/core/agents"},{local:"BaseMessageChunk",imported:"BaseMessageChunk",source:"@langchain/core/messages"},{local:"SearchApi",imported:"SearchApi",source:"@langchain/community/tools/searchapi"}]}}}]);