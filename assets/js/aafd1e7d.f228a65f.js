(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2288],{3572:(e,t,n)=>{"use strict";n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>h,default:()=>g,frontMatter:()=>l,metadata:()=>d,toc:()=>p});var r=n(74848),o=n(28453),s=n(64428),i=n(78847),a=n(50429),c=n.n(a);const l={sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},h="How use a vector store to retrieve data",d={id:"how_to/vectorstore_retriever",title:"How use a vector store to retrieve data",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/vectorstore_retriever.mdx",sourceDirName:"how_to",slug:"/how_to/vectorstore_retriever",permalink:"/docs/how_to/vectorstore_retriever",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/vectorstore_retriever.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},sidebar:"tutorialSidebar"},u={},p=[...i.toc,{value:"Next steps",id:"next-steps",level:2}];function m(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h1,{id:"how-use-a-vector-store-to-retrieve-data",children:"How use a vector store to retrieve data"}),"\n",(0,r.jsxs)(t.admonition,{title:"Prerequisites",type:"info",children:[(0,r.jsx)(t.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"/docs/concepts/#vectorstores",children:"Vector stores"})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"/docs/concepts/#retrievers",children:"Retrievers"})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"/docs/concepts#text-splitters",children:"Text splitters"})}),"\n",(0,r.jsx)(t.li,{children:(0,r.jsx)(t.a,{href:"/docs/how_to/sequence/",children:"Chaining runnables"})}),"\n"]})]}),"\n",(0,r.jsxs)(t.p,{children:["Vector stores can be converted into retrievers using the ",(0,r.jsx)(t.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_vectorstores.VectorStore.html#asRetriever",children:(0,r.jsx)(t.code,{children:".asRetriever()"})})," method, which allows you to more easily compose them in chains."]}),"\n",(0,r.jsx)(t.p,{children:"Below, we show a retrieval-augmented generation (RAG) chain that performs question answering over documents using the following steps:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsx)(t.li,{children:"Initialize an vector store"}),"\n",(0,r.jsx)(t.li,{children:"Create a retriever from that vector store"}),"\n",(0,r.jsx)(t.li,{children:"Compose a question answering chain"}),"\n",(0,r.jsx)(t.li,{children:"Ask questions!"}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"Each of the steps has multiple sub steps and potential configurations, but we'll go through one common flow.\nFirst, install the required dependency:"}),"\n","\n","\n",(0,r.jsx)(i.default,{}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai\n"})}),"\n",(0,r.jsxs)(t.p,{children:["You can download the ",(0,r.jsx)(t.code,{children:"state_of_the_union.txt"})," file ",(0,r.jsx)(t.a,{href:"https://github.com/langchain-ai/langchain/blob/master/docs/docs/modules/state_of_the_union.txt",children:"here"}),"."]}),"\n","\n",(0,r.jsx)(s.A,{language:"typescript",children:c()}),"\n",(0,r.jsx)(t.p,{children:"Let's walk through what's happening here."}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsxs)(t.p,{children:["We first load a long text and split it into smaller documents using a text splitter.\nWe then load those documents (which also embeds the documents using the passed ",(0,r.jsx)(t.code,{children:"OpenAIEmbeddings"})," instance) into HNSWLib, our vector store, creating our index."]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"Though we can query the vector store directly, we convert the vector store into a retriever to return retrieved documents in the right format for the question answering chain."}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"We initialize a retrieval chain, which we'll call later in step 4."}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["\n",(0,r.jsx)(t.p,{children:"We ask questions!"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,r.jsx)(t.p,{children:"You've now learned how to convert a vector store as a retriever."}),"\n",(0,r.jsxs)(t.p,{children:["See the individual sections for deeper dives on specific retrievers, the ",(0,r.jsx)(t.a,{href:"/docs/tutorials/rag",children:"broader tutorial on RAG"}),", or this section to learn how to\n",(0,r.jsx)(t.a,{href:"/docs/how_to/custom_retriever/",children:"create your own custom retriever over any data source"}),"."]})]})}function g(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},50429:e=>{e.exports={content:'import * as fs from "node:fs";\n\nimport { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";\nimport { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";\nimport { MemoryVectorStore } from "langchain/vectorstores/memory";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from "@langchain/core/runnables";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\nimport type { Document } from "@langchain/core/documents";\n\nconst formatDocumentsAsString = (documents: Document[]) => {\n  return documents.map((document) => document.pageContent).join("\\n\\n");\n};\n\n// Initialize the LLM to use to answer the question.\nconst model = new ChatOpenAI({\n  model: "gpt-4o",\n});\nconst text = fs.readFileSync("state_of_the_union.txt", "utf8");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n// Create a vector store from the documents.\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings()\n);\n\n// Initialize a retriever wrapper around the vector store\nconst vectorStoreRetriever = vectorStore.asRetriever();\n\n// Create a system & human prompt for the chat model\nconst SYSTEM_TEMPLATE = `Use the following pieces of context to answer the question at the end.\nIf you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.\n----------------\n{context}`;\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", SYSTEM_TEMPLATE],\n  ["human", "{question}"],\n]);\n\nconst chain = RunnableSequence.from([\n  {\n    context: vectorStoreRetriever.pipe(formatDocumentsAsString),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst answer = await chain.invoke(\n  "What did the president say about Justice Breyer?"\n);\n\nconsole.log({ answer });\n\n/*\n  {\n    answer: \'The president honored Justice Stephen Breyer by recognizing his dedication to serving the country as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. He thanked Justice Breyer for his service.\'\n  }\n*/\n',imports:[{local:"OpenAIEmbeddings",imported:"OpenAIEmbeddings",source:"@langchain/openai"},{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"RecursiveCharacterTextSplitter",imported:"RecursiveCharacterTextSplitter",source:"@langchain/textsplitters"},{local:"MemoryVectorStore",imported:"MemoryVectorStore",source:"langchain/vectorstores/memory"},{local:"RunnablePassthrough",imported:"RunnablePassthrough",source:"@langchain/core/runnables"},{local:"RunnableSequence",imported:"RunnableSequence",source:"@langchain/core/runnables"},{local:"StringOutputParser",imported:"StringOutputParser",source:"@langchain/core/output_parsers"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"Document",imported:"Document",source:"@langchain/core/documents"}]}}}]);