(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4291,65],{93896:(n,e,a)=>{"use strict";a.r(e),a.d(e,{assets:()=>b,contentTitle:()=>_,default:()=>M,frontMatter:()=>x,metadata:()=>C,toc:()=>j});var t=a(74848),o=a(28453),s=a(78847),l=a(64428),i=a(72297),r=a.n(i),m=a(79437),c=a.n(m),p=a(5103),h=a.n(p),d=a(508),u=a.n(d),g=a(91406),y=a.n(g),f=a(48231),w=a.n(f);const x={sidebar_class_name:"node-only"},_="Llama CPP",C={id:"integrations/chat/llama_cpp",title:"Llama CPP",description:"Only available on Node.js.",source:"@site/docs/integrations/chat/llama_cpp.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/llama_cpp",permalink:"/docs/integrations/chat/llama_cpp",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/llama_cpp.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"node-only"},sidebar:"integrations",previous:{title:"Groq",permalink:"/docs/integrations/chat/groq"},next:{title:"Minimax",permalink:"/docs/integrations/chat/minimax"}},b={},j=[{value:"Setup",id:"setup",level:2},...s.toc,{value:"Usage",id:"usage",level:2},{value:"Basic use",id:"basic-use",level:3},{value:"System messages",id:"system-messages",level:3},{value:"Chains",id:"chains",level:3},{value:"Streaming",id:"streaming",level:3}];function L(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"llama-cpp",children:"Llama CPP"}),"\n",(0,t.jsx)(e.admonition,{title:"Compatibility",type:"tip",children:(0,t.jsx)(e.p,{children:"Only available on Node.js."})}),"\n",(0,t.jsxs)(e.p,{children:["This module is based on the ",(0,t.jsx)(e.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," Node.js bindings for ",(0,t.jsx)(e.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp"}),", allowing you to work with a locally running LLM. This allows you to work with a much smaller quantized model capable of running on a laptop environment, ideal for testing and scratch padding ideas without running up a bill!"]}),"\n",(0,t.jsx)(e.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsxs)(e.p,{children:["You'll need to install the ",(0,t.jsx)(e.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"})," module to communicate with your local model."]}),"\n","\n",(0,t.jsx)(s.default,{}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install -S node-llama-cpp @langchain/community\n"})}),"\n",(0,t.jsxs)(e.p,{children:["You will also need a local Llama 2 model (or a model supported by ",(0,t.jsx)(e.a,{href:"https://github.com/withcatai/node-llama-cpp",children:"node-llama-cpp"}),"). You will need to pass the path to this model to the LlamaCpp module as a part of the parameters (see example)."]}),"\n",(0,t.jsxs)(e.p,{children:["Out-of-the-box ",(0,t.jsx)(e.code,{children:"node-llama-cpp"})," is tuned for running on a MacOS platform with support for the Metal GPU of Apple M-series of processors. If you need to turn this off or need support for the CUDA architecture then refer to the documentation at ",(0,t.jsx)(e.a,{href:"https://withcatai.github.io/node-llama-cpp/",children:"node-llama-cpp"}),"."]}),"\n",(0,t.jsxs)(e.p,{children:["For advice on getting and preparing ",(0,t.jsx)(e.code,{children:"llama2"})," see the documentation for the LLM version of this module."]}),"\n",(0,t.jsxs)(e.p,{children:["A note to LangChain.js contributors: if you want to run the tests associated with this module you will need to put the path to your local model in the environment variable ",(0,t.jsx)(e.code,{children:"LLAMA_PATH"}),"."]}),"\n",(0,t.jsx)(e.h2,{id:"usage",children:"Usage"}),"\n",(0,t.jsx)(e.h3,{id:"basic-use",children:"Basic use"}),"\n",(0,t.jsx)(e.p,{children:"In this case we pass in a prompt wrapped as a message and expect a response."}),"\n","\n",(0,t.jsx)(l.A,{language:"typescript",children:r()}),"\n",(0,t.jsx)(e.h3,{id:"system-messages",children:"System messages"}),"\n",(0,t.jsxs)(e.p,{children:["We can also provide a system message, note that with the ",(0,t.jsx)(e.code,{children:"llama_cpp"})," module a system message will cause the creation of a new session."]}),"\n","\n",(0,t.jsx)(l.A,{language:"typescript",children:c()}),"\n",(0,t.jsx)(e.h3,{id:"chains",children:"Chains"}),"\n",(0,t.jsxs)(e.p,{children:["This module can also be used with chains, note that using more complex chains will require suitably powerful version of ",(0,t.jsx)(e.code,{children:"llama2"})," such as the 70B version."]}),"\n","\n",(0,t.jsx)(l.A,{language:"typescript",children:h()}),"\n",(0,t.jsx)(e.h3,{id:"streaming",children:"Streaming"}),"\n",(0,t.jsx)(e.p,{children:"We can also stream with Llama CPP, this can be using a raw 'single prompt' string:"}),"\n","\n",(0,t.jsx)(l.A,{language:"typescript",children:u()}),"\n",(0,t.jsx)(e.p,{children:"Or you can provide multiple messages, note that this takes the input and then submits a Llama2 formatted prompt to the model."}),"\n","\n",(0,t.jsx)(l.A,{language:"typescript",children:y()}),"\n",(0,t.jsxs)(e.p,{children:["Using the ",(0,t.jsx)(e.code,{children:"invoke"})," method, we can also achieve stream generation, and use ",(0,t.jsx)(e.code,{children:"signal"})," to abort the generation."]}),"\n","\n",(0,t.jsx)(l.A,{language:"typescript",children:w()})]})}function M(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(L,{...n})}):L(n)}},78847:(n,e,a)=>{"use strict";a.r(e),a.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>m});var t=a(74848),o=a(28453);const s={},l=void 0,i={id:"mdx_components/integration_install_tooltip",title:"integration_install_tooltip",description:"See this section for general instructions on installing integration packages.",source:"@site/docs/mdx_components/integration_install_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/integration_install_tooltip",permalink:"/docs/mdx_components/integration_install_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/integration_install_tooltip.mdx",tags:[],version:"current",frontMatter:{}},r={},m=[];function c(n){const e={a:"a",admonition:"admonition",p:"p",...(0,o.R)(),...n.components};return(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsxs)(e.p,{children:["See ",(0,t.jsx)(e.a,{href:"/docs/how_to/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},72297:n=>{n.exports={content:"import { ChatLlamaCpp } from \"@langchain/community/chat_models/llama_cpp\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst llamaPath = \"/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin\";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath });\n\nconst response = await model.invoke([\n  new HumanMessage({ content: \"My name is John.\" }),\n]);\nconsole.log({ response });\n\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: 'Hello John.',\n      additional_kwargs: {}\n    },\n    lc_namespace: [ 'langchain', 'schema' ],\n    content: 'Hello John.',\n    name: undefined,\n    additional_kwargs: {}\n  }\n*/\n",imports:[{local:"ChatLlamaCpp",imported:"ChatLlamaCpp",source:"@langchain/community/chat_models/llama_cpp"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},5103:n=>{n.exports={content:'import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";\nimport { LLMChain } from "langchain/chains";\nimport { PromptTemplate } from "@langchain/core/prompts";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath, temperature: 0.5 });\n\nconst prompt = PromptTemplate.fromTemplate(\n  "What is a good name for a company that makes {product}?"\n);\nconst chain = new LLMChain({ llm: model, prompt });\n\nconst response = await chain.invoke({ product: "colorful socks" });\n\nconsole.log({ response });\n\n/*\n  {\n  text: `I\'m not sure what you mean by "colorful socks" but here are some ideas:\\n` +\n    \'\\n\' +\n    \'- Sock-it to me!\\n\' +\n    \'- Socks Away\\n\' +\n    \'- Fancy Footwear\'\n  }\n*/\n',imports:[{local:"ChatLlamaCpp",imported:"ChatLlamaCpp",source:"@langchain/community/chat_models/llama_cpp"},{local:"LLMChain",imported:"LLMChain",source:"langchain/chains"},{local:"PromptTemplate",imported:"PromptTemplate",source:"@langchain/core/prompts"}]}},508:n=>{n.exports={content:'import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath, temperature: 0.7 });\n\nconst stream = await model.stream("Tell me a short story about a happy Llama.");\n\nfor await (const chunk of stream) {\n  console.log(chunk.content);\n}\n\n/*\n\n  Once\n   upon\n   a\n   time\n  ,\n   in\n   a\n   green\n   and\n   sunny\n   field\n  ...\n*/\n',imports:[{local:"ChatLlamaCpp",imported:"ChatLlamaCpp",source:"@langchain/community/chat_models/llama_cpp"}]}},48231:n=>{n.exports={content:'import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";\nimport { SystemMessage, HumanMessage } from "@langchain/core/messages";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath, temperature: 0.7 });\n\nconst controller = new AbortController();\n\nsetTimeout(() => {\n  controller.abort();\n  console.log("Aborted");\n}, 5000);\n\nawait model.invoke(\n  [\n    new SystemMessage(\n      "You are a pirate, responses must be very verbose and in pirate dialect."\n    ),\n    new HumanMessage("Tell me about Llamas?"),\n  ],\n  {\n    signal: controller.signal,\n    callbacks: [\n      {\n        handleLLMNewToken(token) {\n          console.log(token);\n        },\n      },\n    ],\n  }\n);\n/*\n\n  Once\n   upon\n   a\n   time\n  ,\n   in\n   a\n   green\n   and\n   sunny\n   field\n  ...\n  Aborted\n\n  AbortError\n\n*/\n',imports:[{local:"ChatLlamaCpp",imported:"ChatLlamaCpp",source:"@langchain/community/chat_models/llama_cpp"},{local:"SystemMessage",imported:"SystemMessage",source:"@langchain/core/messages"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},91406:n=>{n.exports={content:'import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";\nimport { SystemMessage, HumanMessage } from "@langchain/core/messages";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst llamaCpp = new ChatLlamaCpp({ modelPath: llamaPath, temperature: 0.7 });\n\nconst stream = await llamaCpp.stream([\n  new SystemMessage(\n    "You are a pirate, responses must be very verbose and in pirate dialect."\n  ),\n  new HumanMessage("Tell me about Llamas?"),\n]);\n\nfor await (const chunk of stream) {\n  console.log(chunk.content);\n}\n\n/*\n\n  Ar\n  rr\n  r\n  ,\n   me\n   heart\n  y\n  !\n\n   Ye\n   be\n   ask\n  in\n  \'\n   about\n   llam\n  as\n  ,\n   e\n  h\n  ?\n  ...\n*/\n',imports:[{local:"ChatLlamaCpp",imported:"ChatLlamaCpp",source:"@langchain/community/chat_models/llama_cpp"},{local:"SystemMessage",imported:"SystemMessage",source:"@langchain/core/messages"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},79437:n=>{n.exports={content:'import { ChatLlamaCpp } from "@langchain/community/chat_models/llama_cpp";\nimport { SystemMessage, HumanMessage } from "@langchain/core/messages";\n\nconst llamaPath = "/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath });\n\nconst response = await model.invoke([\n  new SystemMessage(\n    "You are a pirate, responses must be very verbose and in pirate dialect, add \'Arr, m\'hearty!\' to each sentence."\n  ),\n  new HumanMessage("Tell me where Llamas come from?"),\n]);\nconsole.log({ response });\n\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Arr, m\'hearty! Llamas come from the land of Peru.",\n      additional_kwargs: {}\n    },\n    lc_namespace: [ \'langchain\', \'schema\' ],\n    content: "Arr, m\'hearty! Llamas come from the land of Peru.",\n    name: undefined,\n    additional_kwargs: {}\n  }\n*/\n',imports:[{local:"ChatLlamaCpp",imported:"ChatLlamaCpp",source:"@langchain/community/chat_models/llama_cpp"},{local:"SystemMessage",imported:"SystemMessage",source:"@langchain/core/messages"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}}}]);