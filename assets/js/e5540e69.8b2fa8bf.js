"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8216],{27074:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>h,contentTitle:()=>c,default:()=>w,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var s=a(74848),t=a(28453),r=a(27846),o=a(63142);const i={sidebar_class_name:"hidden",title:"How to do retrieval"},c=void 0,l={id:"how_to/chatbots_retrieval",title:"How to do retrieval",description:"This guide assumes familiarity with the following:",source:"@site/docs/how_to/chatbots_retrieval.mdx",sourceDirName:"how_to",slug:"/how_to/chatbots_retrieval",permalink:"/docs/how_to/chatbots_retrieval",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/chatbots_retrieval.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to do retrieval"},sidebar:"tutorialSidebar",previous:{title:"How to manage memory",permalink:"/docs/how_to/chatbots_memory"},next:{title:"How to use tools",permalink:"/docs/how_to/chatbots_tools"}},h={},d=[{value:"Setup",id:"setup",level:2},{value:"Creating a retriever",id:"creating-a-retriever",level:2},{value:"Document chains",id:"document-chains",level:2},{value:"Retrieval chains",id:"retrieval-chains",level:2},{value:"Query transformation",id:"query-transformation",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Next steps",id:"next-steps",level:2}];function m(n){const e={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(e.admonition,{title:"Prerequisites",type:"info",children:[(0,s.jsx)(e.p,{children:"This guide assumes familiarity with the following:"}),(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"../../docs/tutorials/chatbot",children:"Chatbots"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"../../docs/tutorials/rag",children:"Retrieval-augmented generation"})}),"\n"]})]}),"\n",(0,s.jsx)(e.p,{children:"Retrieval is a common technique chatbots use to augment their responses\nwith data outside a chat model\u2019s training data. This section will cover\nhow to implement retrieval in the context of chatbots, but it\u2019s worth\nnoting that retrieval is a very subtle and deep topic."}),"\n",(0,s.jsx)(e.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsx)(e.p,{children:"You\u2019ll need to install a few packages, and set any LLM API keys:"}),"\n","\n",(0,s.jsx)(r.A,{children:(0,s.jsx)(e.p,{children:"@langchain/openai cheerio"})}),"\n",(0,s.jsx)(e.p,{children:"Let\u2019s also set up a chat model that we\u2019ll use for the below examples."}),"\n","\n",(0,s.jsx)(o.A,{customVarName:"llm"}),"\n",(0,s.jsx)(e.h2,{id:"creating-a-retriever",children:"Creating a retriever"}),"\n",(0,s.jsxs)(e.p,{children:["We\u2019ll use ",(0,s.jsx)(e.a,{href:"https://docs.smith.langchain.com",children:"the LangSmith\ndocumentation"})," as source material and\nstore the content in a vectorstore for later retrieval. Note that this\nexample will gloss over some of the specifics around parsing and storing\na data source - you can see more ",(0,s.jsx)(e.a,{href:"../../docs/how_to/#qa-with-rag",children:"in-depth documentation on creating\nretrieval systems here"}),"."]}),"\n",(0,s.jsx)(e.p,{children:"Let\u2019s use a document loader to pull text from the docs:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import "cheerio";\nimport { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";\n\nconst loader = new CheerioWebBaseLoader(\n  "https://docs.smith.langchain.com/user_guide"\n);\n\nconst rawDocs = await loader.load();\n\nrawDocs[0].pageContent.length;\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:"36687\n"})}),"\n",(0,s.jsx)(e.p,{children:"Next, we split it into smaller chunks that the LLM\u2019s context window can\nhandle and store it in a vector database:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 500,\n  chunkOverlap: 0,\n});\n\nconst allSplits = await textSplitter.splitDocuments(rawDocs);\n'})}),"\n",(0,s.jsx)(e.p,{children:"Then we embed and store those chunks in a vector database:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import { OpenAIEmbeddings } from "@langchain/openai";\nimport { MemoryVectorStore } from "langchain/vectorstores/memory";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  allSplits,\n  new OpenAIEmbeddings()\n);\n'})}),"\n",(0,s.jsx)(e.p,{children:"And finally, let\u2019s create a retriever from our initialized vectorstore:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'const retriever = vectorstore.asRetriever(4);\n\nconst docs = await retriever.invoke("how can langsmith help with testing?");\n\nconsole.log(docs);\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'[\n  Document {\n    pageContent: "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"... 294 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 7, to: 7 } }\n    }\n  },\n  Document {\n    pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\u200bWhi"... 347 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 6, to: 6 } }\n    }\n  },\n  Document {\n    pageContent: "will help in curation of test cases that can help track regressions/improvements and development of "... 393 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 11, to: 11 } }\n    }\n  },\n  Document {\n    pageContent: "that time period \u2014 this is especially handy for debugging production issues.LangSmith also allows fo"... 396 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 11, to: 11 } }\n    }\n  }\n]\n'})}),"\n",(0,s.jsx)(e.p,{children:"We can see that invoking the retriever above results in some parts of\nthe LangSmith docs that contain information about testing that our\nchatbot can use as context when answering questions. And now we\u2019ve got a\nretriever that can return related data from the LangSmith docs!"}),"\n",(0,s.jsx)(e.h2,{id:"document-chains",children:"Document chains"}),"\n",(0,s.jsxs)(e.p,{children:["Now that we have a retriever that can return LangChain docs, let\u2019s\ncreate a chain that can use them as context to answer questions. We\u2019ll\nuse a ",(0,s.jsx)(e.code,{children:"createStuffDocumentsChain"})," helper function to \u201cstuff\u201d all of the\ninput documents into the prompt. It will also handle formatting the docs\nas strings."]}),"\n",(0,s.jsxs)(e.p,{children:["In addition to a chat model, the function also expects a prompt that has\na ",(0,s.jsx)(e.code,{children:"context"})," variable, as well as a placeholder for chat history messages\nnamed ",(0,s.jsx)(e.code,{children:"messages"}),". We\u2019ll create an appropriate prompt and pass it as\nshown below:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import { createStuffDocumentsChain } from "langchain/chains/combine_documents";\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from "@langchain/core/prompts";\n\nconst SYSTEM_TEMPLATE = `Answer the user\'s questions based on the below context. \nIf the context doesn\'t contain any relevant information to the question, don\'t make something up and just say "I don\'t know":\n\n<context>\n{context}\n</context>\n`;\n\nconst questionAnsweringPrompt = ChatPromptTemplate.fromMessages([\n  ["system", SYSTEM_TEMPLATE],\n  new MessagesPlaceholder("messages"),\n]);\n\nconst documentChain = await createStuffDocumentsChain({\n  llm,\n  prompt: questionAnsweringPrompt,\n});\n'})}),"\n",(0,s.jsxs)(e.p,{children:["We can invoke this ",(0,s.jsx)(e.code,{children:"documentChain"})," by itself to answer questions. Let\u2019s\nuse the docs we retrieved above and the same question,\n",(0,s.jsx)(e.code,{children:"how can langsmith help with testing?"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import { HumanMessage, AIMessage } from "@langchain/core/messages";\n\nawait documentChain.invoke({\n  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],\n  context: docs,\n});\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'"Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a"... 229 more characters\n'})}),"\n",(0,s.jsx)(e.p,{children:"Looks good! For comparison, we can try it with no context docs and\ncompare the result:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'await documentChain.invoke({\n  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],\n  context: [],\n});\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'"I don\'t know."\n'})}),"\n",(0,s.jsx)(e.p,{children:"We can see that the LLM does not return any results."}),"\n",(0,s.jsx)(e.h2,{id:"retrieval-chains",children:"Retrieval chains"}),"\n",(0,s.jsx)(e.p,{children:"Let\u2019s combine this document chain with the retriever. Here\u2019s one way\nthis can look:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import type { BaseMessage } from "@langchain/core/messages";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from "@langchain/core/runnables";\n\nconst parseRetrieverInput = (params: { messages: BaseMessage[] }) => {\n  return params.messages[params.messages.length - 1].content;\n};\n\nconst retrievalChain = RunnablePassthrough.assign({\n  context: RunnableSequence.from([parseRetrieverInput, retriever]),\n}).assign({\n  answer: documentChain,\n});\n'})}),"\n",(0,s.jsx)(e.p,{children:"Given a list of input messages, we extract the content of the last\nmessage in the list and pass that to the retriever to fetch some\ndocuments. Then, we pass those documents as context to our document\nchain to generate a final response."}),"\n",(0,s.jsx)(e.p,{children:"Invoking this chain combines both steps outlined above:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'await retrievalChain.invoke({\n  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],\n});\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'{\n  messages: [\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Can LangSmith help test my LLM applications?",\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Can LangSmith help test my LLM applications?",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    }\n  ],\n  context: [\n    Document {\n      pageContent: "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"... 294 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "this guide, we\u2019ll highlight the breadth of workflows LangSmith supports and how they fit into each s"... 343 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\u200bWhi"... 347 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "The ability to rapidly understand how the model is performing \u2014 and debug where it is failing \u2014 is i"... 138 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    }\n  ],\n  answer: "Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a"... 297 more characters\n}\n'})}),"\n",(0,s.jsx)(e.p,{children:"Looks good!"}),"\n",(0,s.jsx)(e.h2,{id:"query-transformation",children:"Query transformation"}),"\n",(0,s.jsx)(e.p,{children:"Our retrieval chain is capable of answering questions about LangSmith,\nbut there\u2019s a problem - chatbots interact with users conversationally,\nand therefore have to deal with followup questions."}),"\n",(0,s.jsxs)(e.p,{children:["The chain in its current form will struggle with this. Consider a\nfollowup question to our original question like ",(0,s.jsx)(e.code,{children:"Tell me more!"}),". If we\ninvoke our retriever with that query directly, we get documents\nirrelevant to LLM application testing:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'await retriever.invoke("Tell me more!");\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'[\n  Document {\n    pageContent: "Oftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in"... 40 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 8, to: 8 } }\n    }\n  },\n  Document {\n    pageContent: "This allows you to quickly test out different prompts and models. You can open the playground from a"... 37 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 10, to: 10 } }\n    }\n  },\n  Document {\n    pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\u200bWhi"... 347 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 6, to: 6 } }\n    }\n  },\n  Document {\n    pageContent: "together, making it easier to track the performance of and annotate your application across multiple"... 244 more characters,\n    metadata: {\n      source: "https://docs.smith.langchain.com/user_guide",\n      loc: { lines: { from: 11, to: 11 } }\n    }\n  }\n]\n'})}),"\n",(0,s.jsx)(e.p,{children:"This is because the retriever has no innate concept of state, and will\nonly pull documents most similar to the query given. To solve this, we\ncan transform the query into a standalone query without any external\nreferences an LLM."}),"\n",(0,s.jsx)(e.p,{children:"Here\u2019s an example:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'const queryTransformPrompt = ChatPromptTemplate.fromMessages([\n  new MessagesPlaceholder("messages"),\n  [\n    "user",\n    "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.",\n  ],\n]);\n\nconst queryTransformationChain = queryTransformPrompt.pipe(llm);\n\nawait queryTransformationChain.invoke({\n  messages: [\n    new HumanMessage("Can LangSmith help test my LLM applications?"),\n    new AIMessage(\n      "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."\n    ),\n    new HumanMessage("Tell me more!"),\n  ],\n});\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'AIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'"LangSmith LLM application testing and evaluation features"\',\n    tool_calls: [],\n    invalid_tool_calls: [],\n    additional_kwargs: { function_call: undefined, tool_calls: undefined },\n    response_metadata: {}\n  },\n  lc_namespace: [ "langchain_core", "messages" ],\n  content: \'"LangSmith LLM application testing and evaluation features"\',\n  name: undefined,\n  additional_kwargs: { function_call: undefined, tool_calls: undefined },\n  response_metadata: {\n    tokenUsage: { completionTokens: 11, promptTokens: 144, totalTokens: 155 },\n    finish_reason: "stop"\n  },\n  tool_calls: [],\n  invalid_tool_calls: []\n}\n'})}),"\n",(0,s.jsx)(e.p,{children:"Awesome! That transformed query would pull up context documents related\nto LLM application testing."}),"\n",(0,s.jsx)(e.p,{children:"Let\u2019s add this to our retrieval chain. We can wrap our retriever as\nfollows:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import { RunnableBranch } from "@langchain/core/runnables";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\n\nconst queryTransformingRetrieverChain = RunnableBranch.from([\n  [\n    (params: { messages: BaseMessage[] }) => params.messages.length === 1,\n    RunnableSequence.from([parseRetrieverInput, retriever]),\n  ],\n  queryTransformPrompt.pipe(llm).pipe(new StringOutputParser()).pipe(retriever),\n]).withConfig({ runName: "chat_retriever_chain" });\n'})}),"\n",(0,s.jsx)(e.p,{children:"Then, we can use this query transformation chain to make our retrieval\nchain better able to handle such followup questions:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:"const conversationalRetrievalChain = RunnablePassthrough.assign({\n  context: queryTransformingRetrieverChain,\n}).assign({\n  answer: documentChain,\n});\n"})}),"\n",(0,s.jsx)(e.p,{children:"Awesome! Let\u2019s invoke this new chain with the same inputs as earlier:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'await conversationalRetrievalChain.invoke({\n  messages: [new HumanMessage("Can LangSmith help test my LLM applications?")],\n});\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'{\n  messages: [\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Can LangSmith help test my LLM applications?",\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Can LangSmith help test my LLM applications?",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    }\n  ],\n  context: [\n    Document {\n      pageContent: "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"... 294 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "this guide, we\u2019ll highlight the breadth of workflows LangSmith supports and how they fit into each s"... 343 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\u200bWhi"... 347 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "The ability to rapidly understand how the model is performing \u2014 and debug where it is failing \u2014 is i"... 138 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    }\n  ],\n  answer: "Yes, LangSmith can help test your LLM applications. It allows developers to create datasets, which a"... 297 more characters\n}\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'await conversationalRetrievalChain.invoke({\n  messages: [\n    new HumanMessage("Can LangSmith help test my LLM applications?"),\n    new AIMessage(\n      "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."\n    ),\n    new HumanMessage("Tell me more!"),\n  ],\n});\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'{\n  messages: [\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Can LangSmith help test my LLM applications?",\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Can LangSmith help test my LLM applications?",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    AIMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"... 317 more characters,\n        tool_calls: [],\n        invalid_tool_calls: [],\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"... 317 more characters,\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {},\n      tool_calls: [],\n      invalid_tool_calls: []\n    },\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Tell me more!",\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Tell me more!",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    }\n  ],\n  context: [\n    Document {\n      pageContent: "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"... 294 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\u200bWhi"... 347 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "this guide, we\u2019ll highlight the breadth of workflows LangSmith supports and how they fit into each s"... 343 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "will help in curation of test cases that can help track regressions/improvements and development of "... 393 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    }\n  ],\n  answer: "LangSmith supports a variety of workflows to aid in the development of your applications, from creat"... 607 more characters\n}\n'})}),"\n",(0,s.jsxs)(e.p,{children:["You can check out ",(0,s.jsx)(e.a,{href:"https://smith.langchain.com/public/dc4d6bd4-fea5-45df-be94-06ad18882ae9/r",children:"this LangSmith\ntrace"}),"\nto see the internal query transformation step for yourself."]}),"\n",(0,s.jsx)(e.h2,{id:"streaming",children:"Streaming"}),"\n",(0,s.jsxs)(e.p,{children:["Because this chain is constructed with LCEL, you can use familiar\nmethods like ",(0,s.jsx)(e.code,{children:".stream()"})," with it:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'const stream = await conversationalRetrievalChain.stream({\n  messages: [\n    new HumanMessage("Can LangSmith help test my LLM applications?"),\n    new AIMessage(\n      "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."\n    ),\n    new HumanMessage("Tell me more!"),\n  ],\n});\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-text",children:'{\n  messages: [\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Can LangSmith help test my LLM applications?",\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Can LangSmith help test my LLM applications?",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    },\n    AIMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"... 317 more characters,\n        tool_calls: [],\n        invalid_tool_calls: [],\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examp"... 317 more characters,\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {},\n      tool_calls: [],\n      invalid_tool_calls: []\n    },\n    HumanMessage {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Tell me more!",\n        additional_kwargs: {},\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Tell me more!",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {}\n    }\n  ]\n}\n{\n  context: [\n    Document {\n      pageContent: "These test cases can be uploaded in bulk, created on the fly, or exported from application traces. L"... 294 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "We provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\u200bWhi"... 347 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "this guide, we\u2019ll highlight the breadth of workflows LangSmith supports and how they fit into each s"... 343 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    },\n    Document {\n      pageContent: "will help in curation of test cases that can help track regressions/improvements and development of "... 393 more characters,\n      metadata: {\n        source: "https://docs.smith.langchain.com/user_guide",\n        loc: { lines: [Object] }\n      }\n    }\n  ]\n}\n{ answer: "" }\n{ answer: "Lang" }\n{ answer: "Smith" }\n{ answer: " offers" }\n{ answer: " a" }\n{ answer: " comprehensive" }\n{ answer: " suite" }\n{ answer: " of" }\n{ answer: " tools" }\n{ answer: " and" }\n{ answer: " workflows" }\n{ answer: " to" }\n{ answer: " support" }\n{ answer: " the" }\n{ answer: " development" }\n{ answer: " and" }\n{ answer: " testing" }\n{ answer: " of" }\n{ answer: " L" }\n{ answer: "LM" }\n{ answer: " applications" }\n{ answer: "." }\n{ answer: " Here" }\n{ answer: " are" }\n{ answer: " some" }\n{ answer: " key" }\n{ answer: " features" }\n{ answer: " and" }\n{ answer: " functionalities" }\n{ answer: ":\\n\\n" }\n{ answer: "1" }\n{ answer: "." }\n{ answer: " **" }\n{ answer: "Test" }\n{ answer: " Case" }\n{ answer: " Management" }\n{ answer: "**" }\n{ answer: ":\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Bulk" }\n{ answer: " Upload" }\n{ answer: " and" }\n{ answer: " Creation" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " You" }\n{ answer: " can" }\n{ answer: " upload" }\n{ answer: " test" }\n{ answer: " cases" }\n{ answer: " in" }\n{ answer: " bulk" }\n{ answer: "," }\n{ answer: " create" }\n{ answer: " them" }\n{ answer: " on" }\n{ answer: " the" }\n{ answer: " fly" }\n{ answer: "," }\n{ answer: " or" }\n{ answer: " export" }\n{ answer: " them" }\n{ answer: " from" }\n{ answer: " application" }\n{ answer: " traces" }\n{ answer: ".\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Datas" }\n{ answer: "ets" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " Lang" }\n{ answer: "Smith" }\n{ answer: " allows" }\n{ answer: " you" }\n{ answer: " to" }\n{ answer: " create" }\n{ answer: " datasets" }\n{ answer: "," }\n{ answer: " which" }\n{ answer: " are" }\n{ answer: " collections" }\n{ answer: " of" }\n{ answer: " inputs" }\n{ answer: " and" }\n{ answer: " reference" }\n{ answer: " outputs" }\n{ answer: "." }\n{ answer: " These" }\n{ answer: " datasets" }\n{ answer: " can" }\n{ answer: " be" }\n{ answer: " used" }\n{ answer: " to" }\n{ answer: " run" }\n{ answer: " tests" }\n{ answer: " on" }\n{ answer: " your" }\n{ answer: " L" }\n{ answer: "LM" }\n{ answer: " applications" }\n{ answer: ".\\n\\n" }\n{ answer: "2" }\n{ answer: "." }\n{ answer: " **" }\n{ answer: "Custom" }\n{ answer: " Evalu" }\n{ answer: "ations" }\n{ answer: "**" }\n{ answer: ":\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "LL" }\n{ answer: "M" }\n{ answer: " and" }\n{ answer: " He" }\n{ answer: "uristic" }\n{ answer: " Based" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " You" }\n{ answer: " can" }\n{ answer: " run" }\n{ answer: " custom" }\n{ answer: " evaluations" }\n{ answer: " using" }\n{ answer: " both" }\n{ answer: " L" }\n{ answer: "LM" }\n{ answer: "-based" }\n{ answer: " and" }\n{ answer: " heuristic" }\n{ answer: "-based" }\n{ answer: " methods" }\n{ answer: " to" }\n{ answer: " score" }\n{ answer: " test" }\n{ answer: " results" }\n{ answer: ".\\n\\n" }\n{ answer: "3" }\n{ answer: "." }\n{ answer: " **" }\n{ answer: "Comparison" }\n{ answer: " View" }\n{ answer: "**" }\n{ answer: ":\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Pro" }\n{ answer: "tot" }\n{ answer: "yp" }\n{ answer: "ing" }\n{ answer: " and" }\n{ answer: " Regression" }\n{ answer: " Tracking" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " When" }\n{ answer: " prot" }\n{ answer: "otyping" }\n{ answer: " different" }\n{ answer: " versions" }\n{ answer: " of" }\n{ answer: " your" }\n{ answer: " applications" }\n{ answer: "," }\n{ answer: " Lang" }\n{ answer: "Smith" }\n{ answer: " provides" }\n{ answer: " a" }\n{ answer: " comparison" }\n{ answer: " view" }\n{ answer: " to" }\n{ answer: " see" }\n{ answer: " if" }\n{ answer: " there" }\n{ answer: " have" }\n{ answer: " been" }\n{ answer: " any" }\n{ answer: " regress" }\n{ answer: "ions" }\n{ answer: " with" }\n{ answer: " respect" }\n{ answer: " to" }\n{ answer: " your" }\n{ answer: " initial" }\n{ answer: " test" }\n{ answer: " cases" }\n{ answer: ".\\n\\n" }\n{ answer: "4" }\n{ answer: "." }\n{ answer: " **" }\n{ answer: "Native" }\n{ answer: " Rendering" }\n{ answer: "**" }\n{ answer: ":\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Chat" }\n{ answer: " Messages" }\n{ answer: "," }\n{ answer: " Functions" }\n{ answer: "," }\n{ answer: " and" }\n{ answer: " Documents" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " Lang" }\n{ answer: "Smith" }\n{ answer: " provides" }\n{ answer: " native" }\n{ answer: " rendering" }\n{ answer: " of" }\n{ answer: " chat" }\n{ answer: " messages" }\n{ answer: "," }\n{ answer: " functions" }\n{ answer: "," }\n{ answer: " and" }\n{ answer: " retrieved" }\n{ answer: " documents" }\n{ answer: "," }\n{ answer: " making" }\n{ answer: " it" }\n{ answer: " easier" }\n{ answer: " to" }\n{ answer: " visualize" }\n{ answer: " and" }\n{ answer: " understand" }\n{ answer: " the" }\n{ answer: " outputs" }\n{ answer: ".\\n\\n" }\n{ answer: "5" }\n{ answer: "." }\n{ answer: " **" }\n{ answer: "Pro" }\n{ answer: "tot" }\n{ answer: "yp" }\n{ answer: "ing" }\n{ answer: " Support" }\n{ answer: "**" }\n{ answer: ":\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Quick" }\n{ answer: " Experiment" }\n{ answer: "ation" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " The" }\n{ answer: " platform" }\n{ answer: " supports" }\n{ answer: " quick" }\n{ answer: " experimentation" }\n{ answer: " with" }\n{ answer: " different" }\n{ answer: " prompts" }\n{ answer: "," }\n{ answer: " model" }\n{ answer: " types" }\n{ answer: "," }\n{ answer: " retrieval" }\n{ answer: " strategies" }\n{ answer: "," }\n{ answer: " and" }\n{ answer: " other" }\n{ answer: " parameters" }\n{ answer: ".\\n\\n" }\n{ answer: "6" }\n{ answer: "." }\n{ answer: " **" }\n{ answer: "Feedback" }\n{ answer: " Capture" }\n{ answer: "**" }\n{ answer: ":\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Human" }\n{ answer: " Feedback" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " When" }\n{ answer: " launching" }\n{ answer: " your" }\n{ answer: " application" }\n{ answer: " to" }\n{ answer: " an" }\n{ answer: " initial" }\n{ answer: " set" }\n{ answer: " of" }\n{ answer: " users" }\n{ answer: "," }\n{ answer: " Lang" }\n{ answer: "Smith" }\n{ answer: " allows" }\n{ answer: " you" }\n{ answer: " to" }\n{ answer: " gather" }\n{ answer: " human" }\n{ answer: " feedback" }\n{ answer: " on" }\n{ answer: " the" }\n{ answer: " responses" }\n{ answer: "." }\n{ answer: " This" }\n{ answer: " helps" }\n{ answer: " identify" }\n{ answer: " interesting" }\n{ answer: " runs" }\n{ answer: " and" }\n{ answer: " highlight" }\n{ answer: " edge" }\n{ answer: " cases" }\n{ answer: " causing" }\n{ answer: " problematic" }\n{ answer: " responses" }\n{ answer: ".\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Feedback" }\n{ answer: " Scores" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " You" }\n{ answer: " can" }\n{ answer: " attach" }\n{ answer: " feedback" }\n{ answer: " scores" }\n{ answer: " to" }\n{ answer: " logged" }\n{ answer: " traces" }\n{ answer: "," }\n{ answer: " often" }\n{ answer: " integrated" }\n{ answer: " into" }\n{ answer: " the" }\n{ answer: " system" }\n{ answer: ".\\n\\n" }\n{ answer: "7" }\n{ answer: "." }\n{ answer: " **" }\n{ answer: "Monitoring" }\n{ answer: " and" }\n{ answer: " Troubles" }\n{ answer: "hooting" }\n{ answer: "**" }\n{ answer: ":\\n" }\n{ answer: "  " }\n{ answer: " -" }\n{ answer: " **" }\n{ answer: "Logging" }\n{ answer: " and" }\n{ answer: " Visualization" }\n{ answer: "**" }\n{ answer: ":" }\n{ answer: " Lang" }\n{ answer: "Smith" }\n{ answer: " logs" }\n{ answer: " all" }\n{ answer: " traces" }\n{ answer: "," }\n{ answer: " visual" }\n{ answer: "izes" }\n{ answer: " latency" }\n{ answer: " and" }\n{ answer: " token" }\n{ answer: " usage" }\n{ answer: " statistics" }\n{ answer: "," }\n{ answer: " and" }\n{ answer: " helps" }\n{ answer: " troubleshoot" }\n{ answer: " specific" }\n{ answer: " issues" }\n{ answer: " as" }\n{ answer: " they" }\n{ answer: " arise" }\n{ answer: ".\\n\\n" }\n{ answer: "Overall" }\n{ answer: "," }\n{ answer: " Lang" }\n{ answer: "Smith" }\n{ answer: " is" }\n{ answer: " designed" }\n{ answer: " to" }\n{ answer: " support" }\n{ answer: " the" }\n{ answer: " entire" }\n{ answer: " lifecycle" }\n{ answer: " of" }\n{ answer: " L" }\n{ answer: "LM" }\n{ answer: " application" }\n{ answer: " development" }\n{ answer: "," }\n{ answer: " from" }\n{ answer: " initial" }\n{ answer: " prot" }\n{ answer: "otyping" }\n{ answer: " to" }\n{ answer: " deployment" }\n{ answer: " and" }\n{ answer: " ongoing" }\n{ answer: " monitoring" }\n{ answer: "," }\n{ answer: " making" }\n{ answer: " it" }\n{ answer: " a" }\n{ answer: " powerful" }\n{ answer: " tool" }\n{ answer: " for" }\n{ answer: " developers" }\n{ answer: " looking" }\n{ answer: " to" }\n{ answer: " build" }\n{ answer: " and" }\n{ answer: " maintain" }\n{ answer: " high" }\n{ answer: "-quality" }\n{ answer: " L" }\n{ answer: "LM" }\n{ answer: " applications" }\n{ answer: "." }\n{ answer: "" }\n'})}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,s.jsx)(e.p,{children:"You\u2019ve now learned some techniques for adding personal data as context\nto your chatbots."}),"\n",(0,s.jsxs)(e.p,{children:["This guide only scratches the surface of retrieval techniques. For more\non different ways of ingesting, preparing, and retrieving the most\nrelevant data, check out our ",(0,s.jsx)(e.a,{href:"../../docs/how_to/#retrievers",children:"how to guides on\nretrieval"}),"."]})]})}function w(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(m,{...n})}):m(n)}},63142:(n,e,a)=>{a.d(e,{A:()=>m});a(96540);var s=a(11470),t=a(19365),r=a(21432),o=a(27846),i=a(27293),c=a(74848);function l(n){let{children:e}=n;return(0,c.jsxs)(c.Fragment,{children:[(0,c.jsx)(i.A,{type:"tip",children:(0,c.jsxs)("p",{children:["See"," ",(0,c.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,c.jsx)(o.A,{children:e})]})}const h={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},d=["openai","anthropic","mistral","groq","vertex"];function m(n){const{customVarName:e,additionalDependencies:a}=n,o=e??"model",i=n.openaiParams??h.openaiParams,m=n.anthropicParams??h.anthropicParams,w=n.fireworksParams??h.fireworksParams,p=n.mistralParams??h.mistralParams,u=n.groqParams??h.groqParams,g=n.vertexParams??h.vertexParams,f=n.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],v={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${o} = new ChatOpenAI(${i});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${o} = new ChatAnthropic(${m});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${o} = new ChatFireworks(${w});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${o} = new ChatMistralAI(${p});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${o} = new ChatGroq(${u});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${o} = new ChatVertexAI(${g});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},x=(n.onlyWso?d:f).map((n=>v[n]));return(0,c.jsxs)("div",{children:[(0,c.jsx)("h3",{children:"Pick your chat model:"}),(0,c.jsx)(s.A,{groupId:"modelTabs",children:x.map((n=>(0,c.jsxs)(t.A,{value:n.value,label:n.label,children:[(0,c.jsx)("h4",{children:"Install dependencies"}),(0,c.jsx)(l,{children:[n.dependencies,a].join(" ")}),(0,c.jsx)("h4",{children:"Add environment variables"}),(0,c.jsx)(r.A,{language:"bash",children:n.envs}),(0,c.jsx)("h4",{children:"Instantiate the model"}),(0,c.jsx)(r.A,{language:"typescript",children:n.text})]},n.value)))})]})}},27846:(n,e,a)=>{a.d(e,{A:()=>i});a(96540);var s=a(11470),t=a(19365),r=a(21432),o=a(74848);function i(n){let{children:e}=n;return(0,o.jsxs)(s.A,{groupId:"npm2yarn",children:[(0,o.jsx)(t.A,{value:"npm",label:"npm",children:(0,o.jsxs)(r.A,{language:"bash",children:["npm i ",e]})}),(0,o.jsx)(t.A,{value:"yarn",label:"yarn",default:!0,children:(0,o.jsxs)(r.A,{language:"bash",children:["yarn add ",e]})}),(0,o.jsx)(t.A,{value:"pnpm",label:"pnpm",children:(0,o.jsxs)(r.A,{language:"bash",children:["pnpm add ",e]})})]})}}}]);