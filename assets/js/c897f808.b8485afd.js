"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8192],{2784:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>h,toc:()=>d});var t=a(74848),i=a(28453),s=a(63142),r=a(78847),o=a(27846);const l={sidebar_class_name:"hidden",keywords:["Runnable","Runnables","LCEL"],title:"How to chain runnables"},c=void 0,h={id:"how_to/sequence",title:"How to chain runnables",description:"One point about [LangChain Expression",source:"@site/docs/how_to/sequence.mdx",sourceDirName:"how_to",slug:"/how_to/sequence",permalink:"/docs/how_to/sequence",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/sequence.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",keywords:["Runnable","Runnables","LCEL"],title:"How to chain runnables"},sidebar:"tutorialSidebar",previous:{title:'How to do "self-querying" retrieval',permalink:"/docs/how_to/self_query"},next:{title:"How to split text by tokens",permalink:"/docs/how_to/split_by_token"}},p={},d=[{value:"The pipe method",id:"the-pipe-method",level:2},...r.toc,{value:"Coercion",id:"coercion",level:3},{value:"Next steps",id:"next-steps",level:2}];function u(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:["One point about ",(0,t.jsx)(n.a,{href:"../../docs/concepts/#langchain-expression-language",children:"LangChain Expression\nLanguage"})," is that\nany two runnables can be \u201cchained\u201d together into sequences. The output\nof the previous runnable\u2019s ",(0,t.jsx)(n.code,{children:".invoke()"})," call is passed as input to the\nnext runnable. This can be done using the ",(0,t.jsx)(n.code,{children:".pipe()"})," method."]}),"\n",(0,t.jsxs)(n.p,{children:["The resulting\n",(0,t.jsx)(n.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_runnables.RunnableSequence.html",children:(0,t.jsx)(n.code,{children:"RunnableSequence"})}),"\nis itself a runnable, which means it can be invoked, streamed, or\nfurther chained just like any other runnable. Advantages of chaining\nrunnables in this way are efficient streaming (the sequence will stream\noutput as soon as it is available), and debugging and tracing with tools\nlike ",(0,t.jsx)(n.a,{href:"../../docs/how_to/debugging",children:"LangSmith"}),"."]}),"\n",(0,t.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,t.jsx)(n.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#langchain-expression-language",children:"LangChain Expression Language\n(LCEL)"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#prompt-templates",children:"Prompt templates"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#chat-models",children:"Chat models"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"../../docs/concepts/#output-parsers",children:"Output parser"})}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"the-pipe-method",children:"The pipe method"}),"\n",(0,t.jsxs)(n.p,{children:["To show off how this works, let\u2019s go through an example. We\u2019ll walk\nthrough a common pattern in LangChain: using a ",(0,t.jsx)(n.a,{href:"../../docs/concepts#prompt-templates",children:"prompt\ntemplate"})," to format input into a\n",(0,t.jsx)(n.a,{href:"../../docs/concepts/#chat-models",children:"chat model"}),", and finally converting\nthe chat message output into a string with an [output\nparser](/docs/concepts#output-parsers."]}),"\n","\n",(0,t.jsx)(s.A,{customVarName:"model"}),"\n","\n",(0,t.jsx)(r.default,{}),"\n",(0,t.jsx)(o.A,{children:(0,t.jsx)(n.p,{children:"@langchain/core"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { StringOutputParser } from "@langchain/core/output_parsers";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\nconst prompt = ChatPromptTemplate.fromTemplate("tell me a joke about {topic}");\n\nconst chain = prompt.pipe(model).pipe(new StringOutputParser());\n'})}),"\n",(0,t.jsx)(n.p,{children:"Prompts and models are both runnable, and the output type from the\nprompt call is the same as the input type of the chat model, so we can\nchain them together. We can then invoke the resulting sequence like any\nother runnable:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'await chain.invoke({ topic: "bears" });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:'"Here\'s a bear joke for you:\\n\\nWhy did the bear dissolve in water?\\nBecause it was a polar bear!"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"coercion",children:"Coercion"}),"\n",(0,t.jsx)(n.p,{children:"We can even combine this chain with more runnables to create another\nchain. This may involve some input/output formatting using other types\nof runnables, depending on the required inputs and outputs of the chain\ncomponents."}),"\n",(0,t.jsx)(n.p,{children:"For example, let\u2019s say we wanted to compose the joke generating chain\nwith another chain that evaluates whether or not the generated joke was\nfunny."}),"\n",(0,t.jsxs)(n.p,{children:["We would need to be careful with how we format the input into the next\nchain. In the below example, the dict in the chain is automatically\nparsed and converted into a\n",(0,t.jsx)(n.a,{href:"../../docs/how_to/parallel",children:(0,t.jsx)(n.code,{children:"RunnableParallel"})}),", which runs all of its\nvalues in parallel and returns a dict with the results."]}),"\n",(0,t.jsx)(n.p,{children:"This happens to be the same format the next prompt template expects.\nHere it is in action:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { RunnableLambda } from "@langchain/core/runnables";\n\nconst analysisPrompt = ChatPromptTemplate.fromTemplate(\n  "is this a funny joke? {joke}"\n);\n\nconst composedChain = new RunnableLambda({\n  func: async (input) => {\n    const result = await chain.invoke(input);\n    return { joke: result };\n  },\n})\n  .pipe(analysisPrompt)\n  .pipe(model)\n  .pipe(new StringOutputParser());\n\nawait composedChain.invoke({ topic: "bears" });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"'Haha, that\\'s a clever play on words! Using \"polar\" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.'\n"})}),"\n",(0,t.jsx)(n.p,{children:"Functions will also be coerced into runnables, so you can add custom\nlogic to your chains too. The below chain results in the same logical\nflow as before:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { RunnableSequence } from "@langchain/core/runnables";\n\nconst composedChainWithLambda = RunnableSequence.from([\n  chain,\n  (input) => ({ joke: input }),\n  analysisPrompt,\n  model,\n  new StringOutputParser(),\n]);\n\nawait composedChainWithLambda.invoke({ topic: "beets" });\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"\"Haha, that's a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it's a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!\"\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["See the LangSmith trace for the run above\n",(0,t.jsx)(n.a,{href:"https://smith.langchain.com/public/ef1bf347-a243-4da6-9be6-54f5d73e6da2/r",children:"here"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["However, keep in mind that using functions like this may interfere with\noperations like streaming. See ",(0,t.jsx)(n.a,{href:"../../docs/how_to/functions",children:"this\nsection"})," for more information."]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,t.jsx)(n.p,{children:"You now know some ways to chain two runnables together."}),"\n",(0,t.jsx)(n.p,{children:"To learn more, see the other how-to guides on runnables in this section."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},63142:(e,n,a)=>{a.d(n,{A:()=>d});a(96540);var t=a(11470),i=a(19365),s=a(21432),r=a(27846),o=a(27293),l=a(74848);function c(e){let{children:n}=e;return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(o.A,{type:"tip",children:(0,l.jsxs)("p",{children:["See"," ",(0,l.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,l.jsx)(r.A,{children:n})]})}const h={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},p=["openai","anthropic","mistral","groq","vertex"];function d(e){const{customVarName:n,additionalDependencies:a}=e,r=n??"model",o=e.openaiParams??h.openaiParams,d=e.anthropicParams??h.anthropicParams,u=e.fireworksParams??h.fireworksParams,m=e.mistralParams??h.mistralParams,g=e.groqParams??h.groqParams,x=e.vertexParams??h.vertexParams,b=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],j={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${r} = new ChatOpenAI(${o});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${r} = new ChatAnthropic(${d});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${r} = new ChatFireworks(${u});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${r} = new ChatMistralAI(${m});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${r} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${r} = new ChatVertexAI(${x});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},f=(e.onlyWso?p:b).map((e=>j[e]));return(0,l.jsxs)("div",{children:[(0,l.jsx)("h3",{children:"Pick your chat model:"}),(0,l.jsx)(t.A,{groupId:"modelTabs",children:f.map((e=>(0,l.jsxs)(i.A,{value:e.value,label:e.label,children:[(0,l.jsx)("h4",{children:"Install dependencies"}),(0,l.jsx)(c,{children:[e.dependencies,a].join(" ")}),(0,l.jsx)("h4",{children:"Add environment variables"}),(0,l.jsx)(s.A,{language:"bash",children:e.envs}),(0,l.jsx)("h4",{children:"Instantiate the model"}),(0,l.jsx)(s.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,n,a)=>{a.d(n,{A:()=>o});a(96540);var t=a(11470),i=a(19365),s=a(21432),r=a(74848);function o(e){let{children:n}=e;return(0,r.jsxs)(t.A,{groupId:"npm2yarn",children:[(0,r.jsx)(i.A,{value:"npm",label:"npm",children:(0,r.jsxs)(s.A,{language:"bash",children:["npm i ",n]})}),(0,r.jsx)(i.A,{value:"yarn",label:"yarn",default:!0,children:(0,r.jsxs)(s.A,{language:"bash",children:["yarn add ",n]})}),(0,r.jsx)(i.A,{value:"pnpm",label:"pnpm",children:(0,r.jsxs)(s.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);