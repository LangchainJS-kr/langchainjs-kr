"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6749],{62492:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var a=t(74848),i=t(28453);const r={title:"\ub85c\uceec RAG \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ub9cc\ub4e4\uae30",sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},s=void 0,o={id:"tutorials/local_rag",title:"\ub85c\uceec RAG \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ub9cc\ub4e4\uae30",description:"The popularity of projects like",source:"@site/docs/tutorials/local_rag.mdx",sourceDirName:"tutorials",slug:"/tutorials/local_rag",permalink:"/docs/tutorials/local_rag",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/tutorials/local_rag.mdx",tags:[],version:"current",frontMatter:{title:"\ub85c\uceec RAG \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ub9cc\ub4e4\uae30",sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},sidebar:"tutorialSidebar"},l={},c=[{value:"Document Loading",id:"document-loading",level:2},{value:"Setup",id:"setup",level:2},{value:"Dependencies",id:"dependencies",level:3},{value:"LangSmith",id:"langsmith",level:3},{value:"Initial setup",id:"initial-setup",level:3},{value:"Model",id:"model",level:2},{value:"LLaMA2",id:"llama2",level:3},{value:"Using in a chain",id:"using-in-a-chain",level:2},{value:"Q&amp;A",id:"qa",level:2},{value:"Q&amp;A with retrieval",id:"qa-with-retrieval",level:2}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["The popularity of projects like\n",(0,a.jsx)(n.a,{href:"https://github.com/imartinez/privateGPT",children:"PrivateGPT"}),",\n",(0,a.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp",children:"llama.cpp"}),",\n",(0,a.jsx)(n.a,{href:"https://github.com/nomic-ai/gpt4all",children:"GPT4All"}),", and\n",(0,a.jsx)(n.a,{href:"https://github.com/Mozilla-Ocho/llamafile",children:"llamafile"})," underscore the\nimportance of running LLMs locally."]}),"\n",(0,a.jsx)(n.p,{children:"LangChain has integrations with many open-source LLMs that can be run\nlocally."}),"\n",(0,a.jsxs)(n.p,{children:["For example, here we show how to run ",(0,a.jsx)(n.code,{children:"OllamaEmbeddings"})," or ",(0,a.jsx)(n.code,{children:"LLaMA2"}),"\nlocally (e.g., on your laptop) using local embeddings and a local LLM."]}),"\n",(0,a.jsx)(n.h2,{id:"document-loading",children:"Document Loading"}),"\n",(0,a.jsx)(n.p,{children:"First, install packages needed for local embeddings and vector storage."}),"\n",(0,a.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsx)(n.h3,{id:"dependencies",children:"Dependencies"}),"\n",(0,a.jsx)(n.p,{children:"We\u2019ll use the following packages:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"npm install --save langchain @langchain/community cheerio\n"})}),"\n",(0,a.jsx)(n.h3,{id:"langsmith",children:"LangSmith"}),"\n",(0,a.jsxs)(n.p,{children:["Many of the applications you build with LangChain will contain multiple\nsteps with multiple invocations of LLM calls. As these applications get\nmore and more complex, it becomes crucial to be able to inspect what\nexactly is going on inside your chain or agent. The best way to do this\nis with ",(0,a.jsx)(n.a,{href:"https://smith.langchain.com/",children:"LangSmith"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Note that LangSmith is not needed, but it is helpful. If you do want to\nuse LangSmith, after you sign up at the link above, make sure to set\nyour environment variables to start logging traces:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"export LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=YOUR_KEY\n"})}),"\n",(0,a.jsx)(n.h3,{id:"initial-setup",children:"Initial setup"}),"\n",(0,a.jsx)(n.p,{children:"Load and split an example document."}),"\n",(0,a.jsx)(n.p,{children:"We\u2019ll use a blog post on agents as an example."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import "cheerio";\nimport { RecursiveCharacterTextSplitter } from "langchain/text_splitter";\nimport { CheerioWebBaseLoader } from "langchain/document_loaders/web/cheerio";\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const loader = new CheerioWebBaseLoader(\n  "https://lilianweng.github.io/posts/2023-06-23-agent/"\n);\nconst docs = await loader.load();\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 500,\n  chunkOverlap: 0,\n});\nconst allSplits = await textSplitter.splitDocuments(docs);\nconsole.log(allSplits.length);\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"146\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Next, we\u2019ll use ",(0,a.jsx)(n.code,{children:"OllamaEmbeddings"})," for our local embeddings. Follow\n",(0,a.jsx)(n.a,{href:"https://github.com/ollama/ollama",children:"these instructions"})," to set up and run\na local Ollama instance."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { OllamaEmbeddings } from "@langchain/community/embeddings/ollama";\nimport { MemoryVectorStore } from "langchain/vectorstores/memory";\n\nconst embeddings = new OllamaEmbeddings();\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  allSplits,\n  embeddings\n);\n'})}),"\n",(0,a.jsx)(n.p,{children:"Test similarity search is working with our local embeddings."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const question = "What are the approaches to Task Decomposition?";\nconst docs = await vectorStore.similaritySearch(question);\nconsole.log(docs.length);\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"4\n"})}),"\n",(0,a.jsx)(n.h2,{id:"model",children:"Model"}),"\n",(0,a.jsx)(n.h3,{id:"llama2",children:"LLaMA2"}),"\n",(0,a.jsxs)(n.p,{children:["For local LLMs we\u2019ll use also use ",(0,a.jsx)(n.code,{children:"ollama"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { ChatOllama } from "@langchain/community/chat_models/ollama";\n\nconst ollamaLlm = new ChatOllama({\n  baseUrl: "http://localhost:11434", // Default value\n  model: "llama2", // Default value\n});\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const response = await ollamaLlm.invoke(\n  "Simulate a rap battle between Stephen Colbert and John Oliver"\n);\nconsole.log(response.content);\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"\n[The stage is set for a fierce rap battle between two of the funniest men on television. Stephen Colbert and John Oliver are standing face to face, each with their own microphone and confident smirk on their face.]\n\nStephen Colbert:\nYo, John Oliver, I heard you've been talking smack\nAbout my show and my satire, saying it's all fake\nBut let me tell you something, brother, I'm the real deal\nI've been making fun of politicians for years, with no conceal\n\nJohn Oliver:\nOh, Stephen, you think you're so clever and smart\nBut your jokes are stale and your delivery's a work of art\nYou're just a pale imitation of the real deal, Jon Stewart\nI'm the one who's really making waves, while you're just a little bird\n\nStephen Colbert:\nWell, John, I may not be as loud as you, but I'm smarter\nMy satire is more subtle, and it goes right over their heads\nI'm the one who's been exposing the truth for years\nWhile you're just a British interloper, trying to steal the cheers\n\nJohn Oliver:\nOh, Stephen, you may have your fans, but I've got the brains\nMy show is more than just slapstick and silly jokes, it's got depth and gains\nI'm the one who's really making a difference, while you're just a clown\nMy satire is more than just a joke, it's a call to action, and I've got the crown\n\n[The crowd cheers and chants as the two comedians continue their rap battle.]\n\nStephen Colbert:\nYou may have your fans, John, but I'm the king of satire\nI've been making fun of politicians for years, and I'm still standing tall\nMy jokes are clever and smart, while yours are just plain dumb\nI'm the one who's really in control, and you're just a pretender to the throne.\n\nJohn Oliver:\nOh, Stephen, you may have your moment in the sun\nBut I'm the one who's really shining bright, and my star is just beginning to rise\nMy satire is more than just a joke, it's a call to action, and I've got the power\nI'm the one who's really making a difference, and you're just a fleeting flower.\n\n[The crowd continues to cheer and chant as the two comedians continue their rap battle.]\n"})}),"\n",(0,a.jsxs)(n.p,{children:["See the LangSmith trace\n",(0,a.jsx)(n.a,{href:"https://smith.langchain.com/public/31c178b5-4bea-4105-88c3-7ec95325c817/r",children:"here"})]}),"\n",(0,a.jsx)(n.h2,{id:"using-in-a-chain",children:"Using in a chain"}),"\n",(0,a.jsx)(n.p,{children:"We can create a summarization chain with either model by passing in the\nretrieved docs and a simple prompt."}),"\n",(0,a.jsxs)(n.p,{children:["It formats the prompt template using the input key values provided and\npasses the formatted string to ",(0,a.jsx)(n.code,{children:"LLama-V2"}),", or another specified LLM."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { RunnableSequence } from "@langchain/core/runnables";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\nimport { PromptTemplate } from "@langchain/core/prompts";\nimport { createStuffDocumentsChain } from "langchain/chains/combine_documents";\n\nconst prompt = PromptTemplate.fromTemplate(\n  "Summarize the main themes in these retrieved docs: {context}"\n);\n\nconst chain = await createStuffDocumentsChain({\n  llm: ollamaLlm,\n  outputParser: new StringOutputParser(),\n  prompt,\n});\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const question = "What are the approaches to Task Decomposition?";\nconst docs = await vectorStore.similaritySearch(question);\nawait chain.invoke({\n  context: docs,\n});\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'"The main themes retrieved from the provided documents are:\\n" +\n  "\\n" +\n  "1. Sensory Memory: The ability to retain"... 1117 more characters\n'})}),"\n",(0,a.jsxs)(n.p,{children:["See the LangSmith trace\n",(0,a.jsx)(n.a,{href:"https://smith.langchain.com/public/47cf6c2a-3d86-4f2b-9a51-ee4663b19152/r",children:"here"})]}),"\n",(0,a.jsx)(n.h2,{id:"qa",children:"Q&A"}),"\n",(0,a.jsx)(n.p,{children:"We can also use the LangChain Prompt Hub to store and fetch prompts that\nare model-specific."}),"\n",(0,a.jsxs)(n.p,{children:["Let\u2019s try with a default RAG prompt,\n",(0,a.jsx)(n.a,{href:"https://smith.langchain.com/hub/rlm/rag-prompt",children:"here"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { pull } from "langchain/hub";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\nconst ragPrompt = await pull<ChatPromptTemplate>("rlm/rag-prompt");\n\nconst chain = await createStuffDocumentsChain({\n  llm: ollamaLlm,\n  outputParser: new StringOutputParser(),\n  prompt: ragPrompt,\n});\n'})}),"\n",(0,a.jsx)(n.p,{children:"Let\u2019s see what this prompt actually looks like:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'console.log(\n  ragPrompt.promptMessages.map((msg) => msg.prompt.template).join("\\n")\n);\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question}\nContext: {context}\nAnswer:\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"await chain.invoke({ context: docs, question });\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'"Task decomposition is a crucial step in breaking down complex problems into manageable parts for eff"... 1095 more characters\n'})}),"\n",(0,a.jsxs)(n.p,{children:["See the LangSmith trace\n",(0,a.jsx)(n.a,{href:"https://smith.langchain.com/public/dd3a189b-53a1-4f31-9766-244cd04ad1f7/r",children:"here"})]}),"\n",(0,a.jsx)(n.h2,{id:"qa-with-retrieval",children:"Q&A with retrieval"}),"\n",(0,a.jsx)(n.p,{children:"Instead of manually passing in docs, we can automatically retrieve them\nfrom our vector store based on the user question."}),"\n",(0,a.jsx)(n.p,{children:"This will use a QA default prompt and will retrieve from the vectorDB."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import {\n  RunnablePassthrough,\n  RunnableSequence,\n} from "@langchain/core/runnables";\nimport { formatDocumentsAsString } from "langchain/util/document";\n\nconst retriever = vectorStore.asRetriever();\n\nconst qaChain = RunnableSequence.from([\n  {\n    context: (input: { question: string }, callbacks) => {\n      const retrieverAndFormatter = retriever.pipe(formatDocumentsAsString);\n      return retrieverAndFormatter.invoke(input.question, callbacks);\n    },\n    question: new RunnablePassthrough(),\n  },\n  ragPrompt,\n  ollamaLlm,\n  new StringOutputParser(),\n]);\n\nawait qaChain.invoke({ question });\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'"Based on the context provided, I understand that you are asking me to answer a question related to m"... 948 more characters\n'})}),"\n",(0,a.jsxs)(n.p,{children:["See the LangSmith trace\n",(0,a.jsx)(n.a,{href:"https://smith.langchain.com/public/440e65ee-0301-42cf-afc9-f09cfb52cf64/r",children:"here"})]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}}}]);