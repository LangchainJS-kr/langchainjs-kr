(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[698],{91263:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>k,contentTitle:()=>g,default:()=>x,frontMatter:()=>u,metadata:()=>m,toc:()=>_});var o=t(74848),s=t(28453),a=t(64428),i=t(32716),r=t.n(i),c=t(78847),l=t(77967),d=t.n(l),h=t(99112),p=t.n(h);const u={sidebar_class_name:"hidden",sidebar_position:5},g="How to track token usage",m={id:"how_to/chat_token_usage_tracking",title:"How to track token usage",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/chat_token_usage_tracking.mdx",sourceDirName:"how_to",slug:"/how_to/chat_token_usage_tracking",permalink:"/docs/how_to/chat_token_usage_tracking",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/chat_token_usage_tracking.mdx",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_class_name:"hidden",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"How to do per-user retrieval",permalink:"/docs/how_to/qa_per_user"},next:{title:"How to track token usage",permalink:"/docs/how_to/llm_token_usage_tracking"}},k={},_=[{value:"Using <code>AIMessage.response_metadata</code>",id:"using-aimessageresponse_metadata",level:2},...c.toc,{value:"Using callbacks",id:"using-callbacks",level:2},{value:"Next steps",id:"next-steps",level:2}];function w(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"how-to-track-token-usage",children:"How to track token usage"}),"\n",(0,o.jsxs)(e.admonition,{title:"Prerequisites",type:"info",children:[(0,o.jsx)(e.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"/docs/concepts/#chat-models",children:"Chat models"})}),"\n"]})]}),"\n",(0,o.jsx)(e.p,{children:"This notebook goes over how to track your token usage for specific calls."}),"\n",(0,o.jsxs)(e.h2,{id:"using-aimessageresponse_metadata",children:["Using ",(0,o.jsx)(e.code,{children:"AIMessage.response_metadata"})]}),"\n",(0,o.jsxs)(e.p,{children:["A number of model providers return token usage information as part of the chat generation response. When available, this is included in the ",(0,o.jsx)(e.code,{children:"AIMessage.response_metadata"})," field.\nHere's an example with OpenAI:"]}),"\n","\n","\n",(0,o.jsx)(c.default,{}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai\n"})}),"\n",(0,o.jsx)(a.A,{language:"typescript",children:r()}),"\n",(0,o.jsx)(e.p,{children:"And here's an example with Anthropic:"}),"\n","\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/anthropic\n"})}),"\n",(0,o.jsx)(a.A,{language:"typescript",children:d()}),"\n",(0,o.jsx)(e.h2,{id:"using-callbacks",children:"Using callbacks"}),"\n",(0,o.jsxs)(e.p,{children:["You can also use the ",(0,o.jsx)(e.code,{children:"handleLLMEnd"})," callback to get the full output from the LLM, including token usage for supported models.\nHere's an example of how you could do that:"]}),"\n","\n",(0,o.jsx)(a.A,{language:"typescript",children:p()}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,o.jsx)(e.p,{children:"You've now seen a few examples of how to track chat model token usage for supported providers."}),"\n",(0,o.jsxs)(e.p,{children:["Next, check out the other how-to guides on chat models in this section, like ",(0,o.jsx)(e.a,{href:"/docs/how_to/structured_output",children:"how to get a model to return structured output"})," or ",(0,o.jsx)(e.a,{href:"/docs/how_to/chat_model_caching",children:"how to add caching to your chat models"}),"."]})]})}function x(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(w,{...n})}):w(n)}},32716:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\n\nconst chatModel = new ChatOpenAI({\n  model: "gpt-4-turbo",\n});\n\nconst res = await chatModel.invoke("Tell me a joke.");\n\nconsole.log(res.response_metadata);\n\n/*\n  {\n    tokenUsage: { completionTokens: 15, promptTokens: 12, totalTokens: 27 },\n    finish_reason: \'stop\'\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}},77967:n=>{n.exports={content:"import { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst chatModel = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n});\n\nconst res = await chatModel.invoke(\"Tell me a joke.\");\n\nconsole.log(res.response_metadata);\n\n/*\n  {\n    id: 'msg_017Mgz6HdgNbi3cwL1LNB9Dw',\n    model: 'claude-3-sonnet-20240229',\n    stop_sequence: null,\n    usage: { input_tokens: 12, output_tokens: 30 },\n    stop_reason: 'end_turn'\n  }\n*/\n",imports:[{local:"ChatAnthropic",imported:"ChatAnthropic",source:"@langchain/anthropic"}]}},99112:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\n\nconst chatModel = new ChatOpenAI({\n  model: "gpt-4-turbo",\n  callbacks: [\n    {\n      handleLLMEnd(output) {\n        console.log(JSON.stringify(output, null, 2));\n      },\n    },\n  ],\n});\n\nawait chatModel.invoke("Tell me a joke.");\n\n/*\n  {\n    "generations": [\n      [\n        {\n          "text": "Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!",\n          "message": {\n            "lc": 1,\n            "type": "constructor",\n            "id": [\n              "langchain_core",\n              "messages",\n              "AIMessage"\n            ],\n            "kwargs": {\n              "content": "Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!",\n              "tool_calls": [],\n              "invalid_tool_calls": [],\n              "additional_kwargs": {},\n              "response_metadata": {\n                "tokenUsage": {\n                  "completionTokens": 17,\n                  "promptTokens": 12,\n                  "totalTokens": 29\n                },\n                "finish_reason": "stop"\n              }\n            }\n          },\n          "generationInfo": {\n            "finish_reason": "stop"\n          }\n        }\n      ]\n    ],\n    "llmOutput": {\n      "tokenUsage": {\n        "completionTokens": 17,\n        "promptTokens": 12,\n        "totalTokens": 29\n      }\n    }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}}}]);