"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[913],{6128:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>p,frontMatter:()=>l,metadata:()=>d,toc:()=>u});var a=r(74848),t=r(28453),s=r(78847),i=r(27846),o=r(63142);const l={sidebar_class_name:"hidden",title:"How to handle multiple retrievers"},c=void 0,d={id:"how_to/query_multiple_retrievers",title:"How to handle multiple retrievers",description:"This guide assumes familiarity with the following:",source:"@site/docs/how_to/query_multiple_retrievers.mdx",sourceDirName:"how_to",slug:"/how_to/query_multiple_retrievers",permalink:"/docs/how_to/query_multiple_retrievers",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/query_multiple_retrievers.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to handle multiple retrievers"},sidebar:"tutorialSidebar",previous:{title:"How to handle multiple queries",permalink:"/docs/how_to/query_multiple_queries"},next:{title:"How to handle cases where no queries are generated",permalink:"/docs/how_to/query_no_queries"}},h={},u=[{value:"Setup",id:"setup",level:2},{value:"Install dependencies",id:"install-dependencies",level:3},...s.toc,{value:"Set environment variables",id:"set-environment-variables",level:3},{value:"Create Index",id:"create-index",level:3},{value:"Query analysis",id:"query-analysis",level:2},{value:"Retrieval with query analysis",id:"retrieval-with-query-analysis",level:2},{value:"Next steps",id:"next-steps",level:2}];function m(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,a.jsx)(n.p,{children:"This guide assumes familiarity with the following:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"../../docs/tutorials/query_analysis",children:"Query analysis"})}),"\n"]})]}),"\n",(0,a.jsx)(n.p,{children:"Sometimes, a query analysis technique may allow for selection of which\nretriever to use. To use this, you will need to add some logic to select\nthe retriever to do. We will show a simple example (using mock data) of\nhow to do that."}),"\n",(0,a.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsx)(n.h3,{id:"install-dependencies",children:"Install dependencies"}),"\n","\n",(0,a.jsx)(s.default,{}),"\n",(0,a.jsx)(i.A,{children:(0,a.jsx)(n.p,{children:"@langchain/community @langchain/openai zod chromadb"})}),"\n",(0,a.jsx)(n.h3,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"OPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGCHAIN_TRACING_V2=true\n"})}),"\n",(0,a.jsx)(n.h3,{id:"create-index",children:"Create Index"}),"\n",(0,a.jsx)(n.p,{children:"We will create a vectorstore over fake information."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { Chroma } from "@langchain/community/vectorstores/chroma";\nimport { OpenAIEmbeddings } from "@langchain/openai";\nimport "chromadb";\n\nconst texts = ["Harrison worked at Kensho"];\nconst embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });\nconst vectorstore = await Chroma.fromTexts(texts, {}, embeddings, {\n  collectionName: "harrison",\n});\nconst retrieverHarrison = vectorstore.asRetriever(1);\n\nconst texts = ["Ankush worked at Facebook"];\nconst embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });\nconst vectorstore = await Chroma.fromTexts(texts, {}, embeddings, {\n  collectionName: "ankush",\n});\nconst retrieverAnkush = vectorstore.asRetriever(1);\n'})}),"\n",(0,a.jsx)(n.h2,{id:"query-analysis",children:"Query analysis"}),"\n",(0,a.jsx)(n.p,{children:"We will use function calling to structure the output. We will let it\nreturn multiple queries."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { z } from "zod";\n\nconst searchSchema = z.object({\n  query: z.string().describe("Query to look up"),\n  person: z\n    .string()\n    .describe(\n      "Person to look things up for. Should be `HARRISON` or `ANKUSH`."\n    ),\n});\n'})}),"\n","\n",(0,a.jsx)(o.A,{customVarName:"llm"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { ChatPromptTemplate } from "@langchain/core/prompts";\nimport {\n  RunnableSequence,\n  RunnablePassthrough,\n} from "@langchain/core/runnables";\n\nconst system = `You have the ability to issue search queries to get information to help answer user information.`;\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", system],\n  ["human", "{question}"],\n]);\nconst llmWithTools = llm.withStructuredOutput(searchSchema, {\n  name: "Search",\n});\nconst queryAnalyzer = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llmWithTools,\n]);\n'})}),"\n",(0,a.jsx)(n.p,{children:"We can see that this allows for routing between retrievers"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'await queryAnalyzer.invoke("where did Harrison Work");\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'{ query: "workplace of Harrison", person: "HARRISON" }\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'await queryAnalyzer.invoke("where did ankush Work");\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'{ query: "Workplace of Ankush", person: "ANKUSH" }\n'})}),"\n",(0,a.jsx)(n.h2,{id:"retrieval-with-query-analysis",children:"Retrieval with query analysis"}),"\n",(0,a.jsx)(n.p,{children:"So how would we include this in a chain? We just need some simple logic\nto select the retriever and pass in the search query"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"const retrievers = {\n  HARRISON: retrieverHarrison,\n  ANKUSH: retrieverAnkush,\n};\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { RunnableConfig, RunnableLambda } from "@langchain/core/runnables";\n\nconst chain = async (question: string, config?: RunnableConfig) => {\n  const response = await queryAnalyzer.invoke(question, config);\n  const retriever = retrievers[response.person];\n  return retriever.invoke(response.query, config);\n};\n\nconst customChain = new RunnableLambda({ func: chain });\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'await customChain.invoke("where did Harrison Work");\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'[ Document { pageContent: "Harrison worked at Kensho", metadata: {} } ]\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'await customChain.invoke("where did ankush Work");\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'[ Document { pageContent: "Ankush worked at Facebook", metadata: {} } ]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,a.jsx)(n.p,{children:"You\u2019ve now learned some techniques for handling multiple retrievers in a\nquery analysis system."}),"\n",(0,a.jsxs)(n.p,{children:["Next, check out some of the other query analysis guides in this section,\nlike ",(0,a.jsx)(n.a,{href:"../../docs/how_to/query_no_queries",children:"how to deal with cases where no query is\ngenerated"}),"."]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},63142:(e,n,r)=>{r.d(n,{A:()=>u});r(96540);var a=r(11470),t=r(19365),s=r(21432),i=r(27846),o=r(27293),l=r(74848);function c(e){let{children:n}=e;return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(o.A,{type:"tip",children:(0,l.jsxs)("p",{children:["See"," ",(0,l.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,l.jsx)(i.A,{children:n})]})}const d={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},h=["openai","anthropic","mistral","groq","vertex"];function u(e){const{customVarName:n,additionalDependencies:r}=e,i=n??"model",o=e.openaiParams??d.openaiParams,u=e.anthropicParams??d.anthropicParams,m=e.fireworksParams??d.fireworksParams,p=e.mistralParams??d.mistralParams,g=e.groqParams??d.groqParams,x=e.vertexParams??d.vertexParams,v=e.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],y={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${i} = new ChatOpenAI(${o});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${i} = new ChatAnthropic(${u});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${i} = new ChatFireworks(${m});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${i} = new ChatMistralAI(${p});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${i} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${i} = new ChatVertexAI(${x});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},w=(e.onlyWso?h:v).map((e=>y[e]));return(0,l.jsxs)("div",{children:[(0,l.jsx)("h3",{children:"Pick your chat model:"}),(0,l.jsx)(a.A,{groupId:"modelTabs",children:w.map((e=>(0,l.jsxs)(t.A,{value:e.value,label:e.label,children:[(0,l.jsx)("h4",{children:"Install dependencies"}),(0,l.jsx)(c,{children:[e.dependencies,r].join(" ")}),(0,l.jsx)("h4",{children:"Add environment variables"}),(0,l.jsx)(s.A,{language:"bash",children:e.envs}),(0,l.jsx)("h4",{children:"Instantiate the model"}),(0,l.jsx)(s.A,{language:"typescript",children:e.text})]},e.value)))})]})}},27846:(e,n,r)=>{r.d(n,{A:()=>o});r(96540);var a=r(11470),t=r(19365),s=r(21432),i=r(74848);function o(e){let{children:n}=e;return(0,i.jsxs)(a.A,{groupId:"npm2yarn",children:[(0,i.jsx)(t.A,{value:"npm",label:"npm",children:(0,i.jsxs)(s.A,{language:"bash",children:["npm i ",n]})}),(0,i.jsx)(t.A,{value:"yarn",label:"yarn",default:!0,children:(0,i.jsxs)(s.A,{language:"bash",children:["yarn add ",n]})}),(0,i.jsx)(t.A,{value:"pnpm",label:"pnpm",children:(0,i.jsxs)(s.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);