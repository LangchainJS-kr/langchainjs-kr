(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9056],{77411:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>m,default:()=>b,frontMatter:()=>c,metadata:()=>d,toc:()=>p});var a=t(74848),i=t(28453),l=t(78847),s=t(64428),o=t(19765),r=t.n(o);const c={sidebar_class_name:"web-only"},m="WebLLM",d={id:"integrations/chat/web_llm",title:"WebLLM",description:"Only available in web environments.",source:"@site/docs/integrations/chat/web_llm.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/web_llm",permalink:"/docs/integrations/chat/web_llm",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/web_llm.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"web-only"},sidebar:"integrations",previous:{title:"TogetherAI",permalink:"/docs/integrations/chat/togetherai"},next:{title:"YandexGPT",permalink:"/docs/integrations/chat/yandex"}},h={},p=[{value:"Setup",id:"setup",level:2},...l.toc,{value:"Usage",id:"usage",level:2},{value:"Example",id:"example",level:2}];function u(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"webllm",children:"WebLLM"}),"\n",(0,a.jsx)(n.admonition,{title:"Compatibility",type:"tip",children:(0,a.jsx)(n.p,{children:"Only available in web environments."})}),"\n",(0,a.jsxs)(n.p,{children:["You can run LLMs directly in your web browser using LangChain's ",(0,a.jsx)(n.a,{href:"https://webllm.mlc.ai",children:"WebLLM"})," integration."]}),"\n",(0,a.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsxs)(n.p,{children:["You'll need to install the ",(0,a.jsx)(n.a,{href:"https://www.npmjs.com/package/@mlc-ai/web-llm",children:"WebLLM SDK"})," module to communicate with your local model."]}),"\n","\n",(0,a.jsx)(l.default,{}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install -S @mlc-ai/web-llm @langchain/community\n"})}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(n.p,{children:"Note that the first time a model is called, WebLLM will download the full weights for that model. This can be multiple gigabytes, and may not be possible for all end-users of your application depending on their internet connection and computer specs.\nWhile the browser will cache future invocations of that model, we recommend using the smallest possible model you can."}),"\n",(0,a.jsxs)(n.p,{children:["We also recommend using a ",(0,a.jsx)(n.a,{href:"https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers",children:"separate web worker"})," when invoking and loading your models to\nnot block execution."]}),"\n","\n",(0,a.jsx)(s.A,{language:"typescript",children:r()}),"\n",(0,a.jsx)(n.p,{children:"Streaming is also supported."}),"\n",(0,a.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,a.jsxs)(n.p,{children:["For a full end-to-end example, check out ",(0,a.jsx)(n.a,{href:"https://github.com/jacoblee93/fully-local-pdf-chatbot",children:"this project"}),"."]})]})}function b(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}},19765:e=>{e.exports={content:'// Must be run in a web environment, e.g. a web worker\n\nimport { ChatWebLLM } from "@langchain/community/chat_models/webllm";\nimport { HumanMessage } from "@langchain/core/messages";\n\n// Initialize the ChatWebLLM model with the model record and chat options.\n// Note that if the appConfig field is set, the list of model records\n// must include the selected model record for the engine.\n\n// You can import a list of models available by default here:\n// https://github.com/mlc-ai/web-llm/blob/main/src/config.ts\n//\n// Or by importing it via:\n// import { prebuiltAppConfig } from "@mlc-ai/web-llm";\nconst model = new ChatWebLLM({\n  model: "Phi2-q4f32_1",\n  chatOptions: {\n    temperature: 0.5,\n  },\n});\n\n// Call the model with a message and await the response.\nconst response = await model.invoke([\n  new HumanMessage({ content: "What is 1 + 1?" }),\n]);\n\nconsole.log(response);\n\n/*\nAIMessage {\n  content: \' 2\\n\',\n}\n*/\n',imports:[{local:"ChatWebLLM",imported:"ChatWebLLM",source:"@langchain/community/chat_models/webllm"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}}}]);