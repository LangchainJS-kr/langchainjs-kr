(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4785],{14776:(e,t,n)=>{"use strict";n.r(t),n.d(t,{assets:()=>v,contentTitle:()=>p,default:()=>f,frontMatter:()=>h,metadata:()=>g,toc:()=>y});var o=n(74848),r=n(28453),i=n(78847),s=n(64428),a=n(43241),c=n.n(a),l=n(16952),u=n.n(l),d=n(91077),m=n.n(d);const h={sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},p="How to generate multiple embeddings per document",g={id:"how_to/multi_vector",title:"How to generate multiple embeddings per document",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/multi_vector.mdx",sourceDirName:"how_to",slug:"/how_to/multi_vector",permalink:"/docs/how_to/multi_vector",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/multi_vector.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",pagination_prev:null,pagination_next:null},sidebar:"tutorialSidebar"},v={},y=[{value:"Smaller chunks",id:"smaller-chunks",level:2},...i.toc,{value:"Summary",id:"summary",level:2},{value:"Hypothetical queries",id:"hypothetical-queries",level:2},{value:"Next steps",id:"next-steps",level:2}];function x(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"how-to-generate-multiple-embeddings-per-document",children:"How to generate multiple embeddings per document"}),"\n",(0,o.jsxs)(t.admonition,{title:"Prerequisites",type:"info",children:[(0,o.jsx)(t.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/docs/concepts/#retrievers",children:"Retrievers"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/docs/concepts/#text-splitters",children:"Text splitters"})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/docs/tutorials/rag",children:"Retrieval-augmented generation (RAG)"})}),"\n"]})]}),"\n",(0,o.jsxs)(t.p,{children:["Embedding different representations of an original document, then returning the original document when any of the representations result in a search hit, can allow you to\ntune and improve your retrieval performance. LangChain has a base ",(0,o.jsx)(t.a,{href:"https://v02.api.js.langchain.com/classes/langchain_retrievers_multi_vector.MultiVectorRetriever.html",children:(0,o.jsx)(t.code,{children:"MultiVectorRetriever"})})," designed to do just this!"]}),"\n",(0,o.jsxs)(t.p,{children:["A lot of the complexity lies in how to create the multiple vectors per document.\nThis guide covers some of the common ways to create those vectors and use the ",(0,o.jsx)(t.code,{children:"MultiVectorRetriever"}),"."]}),"\n",(0,o.jsx)(t.p,{children:"Some methods to create multiple vectors per document include:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:["smaller chunks: split a document into smaller chunks, and embed those (e.g. the ",(0,o.jsx)(t.a,{href:"/docs/how_to/parent_document_retriever",children:(0,o.jsx)(t.code,{children:"ParentDocumentRetriever"})}),")"]}),"\n",(0,o.jsx)(t.li,{children:"summary: create a summary for each document, embed that along with (or instead of) the document"}),"\n",(0,o.jsx)(t.li,{children:"hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document"}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"Note that this also enables another method of adding embeddings - manually. This is great because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control."}),"\n",(0,o.jsx)(t.h2,{id:"smaller-chunks",children:"Smaller chunks"}),"\n",(0,o.jsx)(t.p,{children:"Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks.\nThis allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream.\nNOTE: this is what the ParentDocumentRetriever does. Here we show what is going on under the hood."}),"\n","\n",(0,o.jsx)(i.default,{}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai @langchain/community\n"})}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:c()}),"\n",(0,o.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(t.p,{children:"Oftentimes a summary may be able to distill more accurately what a chunk is about, leading to better retrieval.\nHere we show how to create summaries, and then embed those."}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:u()}),"\n",(0,o.jsx)(t.h2,{id:"hypothetical-queries",children:"Hypothetical queries"}),"\n",(0,o.jsx)(t.p,{children:"An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document.\nThese questions can then be embedded and used to retrieve the original document:"}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:m()}),"\n",(0,o.jsx)(t.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,o.jsx)(t.p,{children:"You've now learned a few ways to generate multiple embeddings per document."}),"\n",(0,o.jsxs)(t.p,{children:["Next, check out the individual sections for deeper dives on specific retrievers, the ",(0,o.jsx)(t.a,{href:"/docs/tutorials/rag",children:"broader tutorial on RAG"}),", or this section to learn how to\n",(0,o.jsx)(t.a,{href:"/docs/how_to/custom_retriever/",children:"create your own custom retriever over any data source"}),"."]})]})}function f(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(x,{...e})}):x(e)}},91077:e=>{e.exports={content:'import * as uuid from "uuid";\n\nimport { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";\nimport { MultiVectorRetriever } from "langchain/retrievers/multi_vector";\nimport { FaissStore } from "@langchain/community/vectorstores/faiss";\nimport { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";\nimport { InMemoryStore } from "@langchain/core/stores";\nimport { TextLoader } from "langchain/document_loaders/fs/text";\nimport { PromptTemplate } from "@langchain/core/prompts";\nimport { RunnableSequence } from "@langchain/core/runnables";\nimport { Document } from "@langchain/core/documents";\nimport { JsonKeyOutputFunctionsParser } from "@langchain/core/output_parsers/openai_functions";\n\nconst textLoader = new TextLoader("../examples/state_of_the_union.txt");\nconst parentDocuments = await textLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 20,\n});\n\nconst docs = await splitter.splitDocuments(parentDocuments);\n\nconst functionsSchema = [\n  {\n    name: "hypothetical_questions",\n    description: "Generate hypothetical questions",\n    parameters: {\n      type: "object",\n      properties: {\n        questions: {\n          type: "array",\n          items: {\n            type: "string",\n          },\n        },\n      },\n      required: ["questions"],\n    },\n  },\n];\n\nconst functionCallingModel = new ChatOpenAI({\n  maxRetries: 0,\n  model: "gpt-4",\n}).bind({\n  functions: functionsSchema,\n  function_call: { name: "hypothetical_questions" },\n});\n\nconst chain = RunnableSequence.from([\n  { content: (doc: Document) => doc.pageContent },\n  PromptTemplate.fromTemplate(\n    `Generate a list of 3 hypothetical questions that the below document could be used to answer:\\n\\n{content}`\n  ),\n  functionCallingModel,\n  new JsonKeyOutputFunctionsParser<string[]>({ attrName: "questions" }),\n]);\n\nconst hypotheticalQuestions = await chain.batch(docs, {\n  maxConcurrency: 5,\n});\n\nconst idKey = "doc_id";\nconst docIds = docs.map((_) => uuid.v4());\nconst hypotheticalQuestionDocs = hypotheticalQuestions\n  .map((questionArray, i) => {\n    const questionDocuments = questionArray.map((question) => {\n      const questionDocument = new Document({\n        pageContent: question,\n        metadata: {\n          [idKey]: docIds[i],\n        },\n      });\n      return questionDocument;\n    });\n    return questionDocuments;\n  })\n  .flat();\n\n// The byteStore to use to store the original chunks\nconst byteStore = new InMemoryStore<Uint8Array>();\n\n// The vectorstore to use to index the child chunks\nconst vectorstore = await FaissStore.fromDocuments(\n  hypotheticalQuestionDocs,\n  new OpenAIEmbeddings()\n);\n\nconst retriever = new MultiVectorRetriever({\n  vectorstore,\n  byteStore,\n  idKey,\n});\n\nconst keyValuePairs: [string, Document][] = docs.map((originalDoc, i) => [\n  docIds[i],\n  originalDoc,\n]);\n\n// Use the retriever to add the original chunks to the document store\nawait retriever.docstore.mset(keyValuePairs);\n\n// We could also add the original chunks to the vectorstore if we wish\n// const taggedOriginalDocs = docs.map((doc, i) => {\n//   doc.metadata[idKey] = docIds[i];\n//   return doc;\n// });\n// retriever.vectorstore.addDocuments(taggedOriginalDocs);\n\n// Vectorstore alone retrieves the small chunks\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\n  "justice breyer"\n);\nconsole.log(vectorstoreResult[0].pageContent);\n/*\n  "What measures will be taken to crack down on corporations overcharging American businesses and consumers?"\n*/\n\n// Retriever returns larger result\nconst retrieverResult = await retriever.invoke("justice breyer");\nconsole.log(retrieverResult[0].pageContent.length);\n/*\n  9770\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"OpenAIEmbeddings",imported:"OpenAIEmbeddings",source:"@langchain/openai"},{local:"MultiVectorRetriever",imported:"MultiVectorRetriever",source:"langchain/retrievers/multi_vector"},{local:"FaissStore",imported:"FaissStore",source:"@langchain/community/vectorstores/faiss"},{local:"RecursiveCharacterTextSplitter",imported:"RecursiveCharacterTextSplitter",source:"@langchain/textsplitters"},{local:"InMemoryStore",imported:"InMemoryStore",source:"@langchain/core/stores"},{local:"TextLoader",imported:"TextLoader",source:"langchain/document_loaders/fs/text"},{local:"PromptTemplate",imported:"PromptTemplate",source:"@langchain/core/prompts"},{local:"RunnableSequence",imported:"RunnableSequence",source:"@langchain/core/runnables"},{local:"Document",imported:"Document",source:"@langchain/core/documents"},{local:"JsonKeyOutputFunctionsParser",imported:"JsonKeyOutputFunctionsParser",source:"@langchain/core/output_parsers/openai_functions"}]}},43241:e=>{e.exports={content:'import * as uuid from "uuid";\n\nimport { MultiVectorRetriever } from "langchain/retrievers/multi_vector";\nimport { FaissStore } from "@langchain/community/vectorstores/faiss";\nimport { OpenAIEmbeddings } from "@langchain/openai";\nimport { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";\nimport { InMemoryStore } from "@langchain/core/stores";\nimport { TextLoader } from "langchain/document_loaders/fs/text";\nimport { Document } from "@langchain/core/documents";\n\nconst textLoader = new TextLoader("../examples/state_of_the_union.txt");\nconst parentDocuments = await textLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 20,\n});\n\nconst docs = await splitter.splitDocuments(parentDocuments);\n\nconst idKey = "doc_id";\nconst docIds = docs.map((_) => uuid.v4());\n\nconst childSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 400,\n  chunkOverlap: 0,\n});\n\nconst subDocs = [];\nfor (let i = 0; i < docs.length; i += 1) {\n  const childDocs = await childSplitter.splitDocuments([docs[i]]);\n  const taggedChildDocs = childDocs.map((childDoc) => {\n    // eslint-disable-next-line no-param-reassign\n    childDoc.metadata[idKey] = docIds[i];\n    return childDoc;\n  });\n  subDocs.push(...taggedChildDocs);\n}\n\n// The byteStore to use to store the original chunks\nconst byteStore = new InMemoryStore<Uint8Array>();\n\n// The vectorstore to use to index the child chunks\nconst vectorstore = await FaissStore.fromDocuments(\n  subDocs,\n  new OpenAIEmbeddings()\n);\n\nconst retriever = new MultiVectorRetriever({\n  vectorstore,\n  byteStore,\n  idKey,\n  // Optional `k` parameter to search for more child documents in VectorStore.\n  // Note that this does not exactly correspond to the number of final (parent) documents\n  // retrieved, as multiple child documents can point to the same parent.\n  childK: 20,\n  // Optional `k` parameter to limit number of final, parent documents returned from this\n  // retriever and sent to LLM. This is an upper-bound, and the final count may be lower than this.\n  parentK: 5,\n});\n\nconst keyValuePairs: [string, Document][] = docs.map((originalDoc, i) => [\n  docIds[i],\n  originalDoc,\n]);\n\n// Use the retriever to add the original chunks to the document store\nawait retriever.docstore.mset(keyValuePairs);\n\n// Vectorstore alone retrieves the small chunks\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\n  "justice breyer"\n);\nconsole.log(vectorstoreResult[0].pageContent.length);\n/*\n  390\n*/\n\n// Retriever returns larger result\nconst retrieverResult = await retriever.invoke("justice breyer");\nconsole.log(retrieverResult[0].pageContent.length);\n/*\n  9770\n*/\n',imports:[{local:"MultiVectorRetriever",imported:"MultiVectorRetriever",source:"langchain/retrievers/multi_vector"},{local:"FaissStore",imported:"FaissStore",source:"@langchain/community/vectorstores/faiss"},{local:"OpenAIEmbeddings",imported:"OpenAIEmbeddings",source:"@langchain/openai"},{local:"RecursiveCharacterTextSplitter",imported:"RecursiveCharacterTextSplitter",source:"@langchain/textsplitters"},{local:"InMemoryStore",imported:"InMemoryStore",source:"@langchain/core/stores"},{local:"TextLoader",imported:"TextLoader",source:"langchain/document_loaders/fs/text"},{local:"Document",imported:"Document",source:"@langchain/core/documents"}]}},16952:e=>{e.exports={content:'import * as uuid from "uuid";\n\nimport { ChatOpenAI, OpenAIEmbeddings } from "@langchain/openai";\nimport { MultiVectorRetriever } from "langchain/retrievers/multi_vector";\nimport { FaissStore } from "@langchain/community/vectorstores/faiss";\nimport { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";\nimport { InMemoryStore } from "@langchain/core/stores";\nimport { TextLoader } from "langchain/document_loaders/fs/text";\nimport { PromptTemplate } from "@langchain/core/prompts";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\nimport { RunnableSequence } from "@langchain/core/runnables";\nimport { Document } from "@langchain/core/documents";\n\nconst textLoader = new TextLoader("../examples/state_of_the_union.txt");\nconst parentDocuments = await textLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 20,\n});\n\nconst docs = await splitter.splitDocuments(parentDocuments);\n\nconst chain = RunnableSequence.from([\n  { content: (doc: Document) => doc.pageContent },\n  PromptTemplate.fromTemplate(`Summarize the following document:\\n\\n{content}`),\n  new ChatOpenAI({\n    maxRetries: 0,\n  }),\n  new StringOutputParser(),\n]);\n\nconst summaries = await chain.batch(docs, {\n  maxConcurrency: 5,\n});\n\nconst idKey = "doc_id";\nconst docIds = docs.map((_) => uuid.v4());\nconst summaryDocs = summaries.map((summary, i) => {\n  const summaryDoc = new Document({\n    pageContent: summary,\n    metadata: {\n      [idKey]: docIds[i],\n    },\n  });\n  return summaryDoc;\n});\n\n// The byteStore to use to store the original chunks\nconst byteStore = new InMemoryStore<Uint8Array>();\n\n// The vectorstore to use to index the child chunks\nconst vectorstore = await FaissStore.fromDocuments(\n  summaryDocs,\n  new OpenAIEmbeddings()\n);\n\nconst retriever = new MultiVectorRetriever({\n  vectorstore,\n  byteStore,\n  idKey,\n});\n\nconst keyValuePairs: [string, Document][] = docs.map((originalDoc, i) => [\n  docIds[i],\n  originalDoc,\n]);\n\n// Use the retriever to add the original chunks to the document store\nawait retriever.docstore.mset(keyValuePairs);\n\n// We could also add the original chunks to the vectorstore if we wish\n// const taggedOriginalDocs = docs.map((doc, i) => {\n//   doc.metadata[idKey] = docIds[i];\n//   return doc;\n// });\n// retriever.vectorstore.addDocuments(taggedOriginalDocs);\n\n// Vectorstore alone retrieves the small chunks\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\n  "justice breyer"\n);\nconsole.log(vectorstoreResult[0].pageContent.length);\n/*\n  1118\n*/\n\n// Retriever returns larger result\nconst retrieverResult = await retriever.invoke("justice breyer");\nconsole.log(retrieverResult[0].pageContent.length);\n/*\n  9770\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"OpenAIEmbeddings",imported:"OpenAIEmbeddings",source:"@langchain/openai"},{local:"MultiVectorRetriever",imported:"MultiVectorRetriever",source:"langchain/retrievers/multi_vector"},{local:"FaissStore",imported:"FaissStore",source:"@langchain/community/vectorstores/faiss"},{local:"RecursiveCharacterTextSplitter",imported:"RecursiveCharacterTextSplitter",source:"@langchain/textsplitters"},{local:"InMemoryStore",imported:"InMemoryStore",source:"@langchain/core/stores"},{local:"TextLoader",imported:"TextLoader",source:"langchain/document_loaders/fs/text"},{local:"PromptTemplate",imported:"PromptTemplate",source:"@langchain/core/prompts"},{local:"StringOutputParser",imported:"StringOutputParser",source:"@langchain/core/output_parsers"},{local:"RunnableSequence",imported:"RunnableSequence",source:"@langchain/core/runnables"},{local:"Document",imported:"Document",source:"@langchain/core/documents"}]}}}]);