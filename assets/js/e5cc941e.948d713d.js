"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3257],{39928:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>c,toc:()=>u});var a=t(74848),s=t(28453),o=t(78847),l=t(27846);const r={sidebar_class_name:"hidden",title:"How to use reference examples"},i=void 0,c={id:"how_to/extraction_examples",title:"How to use reference examples",description:"This guide assumes familiarity with the following:",source:"@site/docs/how_to/extraction_examples.mdx",sourceDirName:"how_to",slug:"/how_to/extraction_examples",permalink:"/docs/how_to/extraction_examples",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/extraction_examples.mdx",tags:[],version:"current",frontMatter:{sidebar_class_name:"hidden",title:"How to use reference examples"},sidebar:"tutorialSidebar",previous:{title:"How to select examples by similarity",permalink:"/docs/how_to/example_selectors_similarity"},next:{title:"How to handle long text",permalink:"/docs/how_to/extraction_long_text"}},d={},u=[...o.toc,{value:"Define the schema",id:"define-the-schema",level:2},{value:"Define reference examples",id:"define-reference-examples",level:2},{value:"Create an extractor",id:"create-an-extractor",level:2},{value:"Without examples \ud83d\ude3f",id:"without-examples",level:2},{value:"With examples \ud83d\ude3b",id:"with-examples",level:2},{value:"Next steps",id:"next-steps",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.admonition,{title:"Prerequisites",type:"info",children:[(0,a.jsx)(n.p,{children:"This guide assumes familiarity with the following:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"../../docs/tutorials/extraction",children:"Extraction"})}),"\n"]})]}),"\n",(0,a.jsx)(n.p,{children:"The quality of extraction can often be improved by providing reference\nexamples to the LLM."}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsx)(n.p,{children:"While this tutorial focuses how to use examples with a tool calling\nmodel, this technique is generally applicable, and will work also with\nJSON more or prompt based techniques."})}),"\n",(0,a.jsxs)(n.p,{children:["We\u2019ll use OpenAI\u2019s GPT-4 this time for their robust support for\n",(0,a.jsx)(n.code,{children:"ToolMessages"}),":"]}),"\n","\n",(0,a.jsx)(o.default,{}),"\n",(0,a.jsx)(l.A,{children:(0,a.jsx)(n.p,{children:"@langchain/openai zod uuid"})}),"\n",(0,a.jsx)(n.p,{children:"Let\u2019s define a prompt:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from "@langchain/core/prompts";\n\nconst SYSTEM_PROMPT_TEMPLATE = `You are an expert extraction algorithm.\nOnly extract relevant information from the text.\nIf you do not know the value of an attribute asked to extract, you may omit the attribute\'s value.`;\n\n// Define a custom prompt to provide instructions and any additional context.\n// 1) You can add examples into the prompt template to improve extraction quality\n// 2) Introduce additional parameters to take context into account (e.g., include metadata\n//    about the document from which the text was extracted.)\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", SYSTEM_PROMPT_TEMPLATE],\n  // \u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\u2193\n  new MessagesPlaceholder("examples"),\n  // \u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\u2191\n  ["human", "{text}"],\n]);\n'})}),"\n",(0,a.jsx)(n.p,{children:"Test out the template:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { HumanMessage } from "@langchain/core/messages";\n\nconst promptValue = await prompt.invoke({\n  text: "this is some text",\n  examples: [new HumanMessage("testing 1 2 3")],\n});\n\npromptValue.toChatMessages();\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'[\n  SystemMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "You are an expert extraction algorithm.\\n" +\n        "Only extract relevant information from the text.\\n" +\n        "If you do n"... 87 more characters,\n      additional_kwargs: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "You are an expert extraction algorithm.\\n" +\n      "Only extract relevant information from the text.\\n" +\n      "If you do n"... 87 more characters,\n    name: undefined,\n    additional_kwargs: {}\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "testing 1 2 3", additional_kwargs: {} },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "testing 1 2 3",\n    name: undefined,\n    additional_kwargs: {}\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "this is some text", additional_kwargs: {} },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "this is some text",\n    name: undefined,\n    additional_kwargs: {}\n  }\n]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"define-the-schema",children:"Define the schema"}),"\n",(0,a.jsx)(n.p,{children:"Let\u2019s re-use the people schema from the quickstart."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { z } from "zod";\n\nconst personSchema = z\n  .object({\n    name: z.optional(z.string()).describe("The name of the person"),\n    hair_color: z\n      .optional(z.string())\n      .describe("The color of the person\'s hair, if known"),\n    height_in_meters: z\n      .optional(z.string())\n      .describe("Height measured in meters"),\n  })\n  .describe("Information about a person.");\n\nconst peopleSchema = z.object({\n  people: z.array(personSchema),\n});\n'})}),"\n",(0,a.jsx)(n.h2,{id:"define-reference-examples",children:"Define reference examples"}),"\n",(0,a.jsx)(n.p,{children:"Examples can be defined as a list of input-output pairs."}),"\n",(0,a.jsxs)(n.p,{children:["Each example contains an example ",(0,a.jsx)(n.code,{children:"input"})," text and an example ",(0,a.jsx)(n.code,{children:"output"}),"\nshowing what should be extracted from the text."]}),"\n",(0,a.jsxs)(n.admonition,{type:"important",children:[(0,a.jsx)(n.p,{children:"The below example is a bit more advanced - the format of the example\nneeds to match the API used (e.g., tool calling or JSON mode etc.)."}),(0,a.jsx)(n.p,{children:"Here, the formatted examples will match the format expected for the\nOpenAI tool calling API since that\u2019s what we\u2019re using."})]}),"\n",(0,a.jsx)(n.p,{children:"To provide reference examples to the model, we will mock out a fake chat\nhistory containing successful usages of the given tool. Because the\nmodel can choose to call multiple tools at once (or the same tool\nmultiple times), the example\u2019s outputs are an array:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import {\n  AIMessage,\n  type BaseMessage,\n  HumanMessage,\n  ToolMessage,\n} from "@langchain/core/messages";\nimport { v4 as uuid } from "uuid";\n\ntype OpenAIToolCall = {\n  id: string;\n  type: "function";\n  function: {\n    name: string;\n    arguments: string;\n  };\n};\n\ntype Example = {\n  input: string;\n  toolCallOutputs: Record<string, any>[];\n};\n\n/**\n * This function converts an example into a list of messages that can be fed into an LLM.\n *\n * This code serves as an adapter that transforms our example into a list of messages\n * that can be processed by a chat model.\n *\n * The list of messages for each example includes:\n *\n * 1) HumanMessage: This contains the content from which information should be extracted.\n * 2) AIMessage: This contains the information extracted by the model.\n * 3) ToolMessage: This provides confirmation to the model that the tool was requested correctly.\n *\n * The inclusion of ToolMessage is necessary because some chat models are highly optimized for agents,\n * making them less suitable for an extraction use case.\n */\nfunction toolExampleToMessages(example: Example): BaseMessage[] {\n  const openAIToolCalls: OpenAIToolCall[] = example.toolCallOutputs.map(\n    (output) => {\n      return {\n        id: uuid(),\n        type: "function",\n        function: {\n          // The name of the function right now corresponds\n          // to the passed name.\n          name: "extract",\n          arguments: JSON.stringify(output),\n        },\n      };\n    }\n  );\n  const messages: BaseMessage[] = [\n    new HumanMessage(example.input),\n    new AIMessage({\n      content: "",\n      additional_kwargs: { tool_calls: openAIToolCalls },\n    }),\n  ];\n  const toolMessages = openAIToolCalls.map((toolCall, i) => {\n    // Return the mocked successful result for a given tool call.\n    return new ToolMessage({\n      content: "You have correctly called this tool.",\n      tool_call_id: toolCall.id,\n    });\n  });\n  return messages.concat(toolMessages);\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"Next let\u2019s define our examples and then convert them into message\nformat."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const examples: Example[] = [\n  {\n    input:\n      "The ocean is vast and blue. It\'s more than 20,000 feet deep. There are many fish in it.",\n    toolCallOutputs: [{}],\n  },\n  {\n    input: "Fiona traveled far from France to Spain.",\n    toolCallOutputs: [\n      {\n        name: "Fiona",\n      },\n    ],\n  },\n];\n\nconst exampleMessages = [];\nfor (const example of examples) {\n  exampleMessages.push(...toolExampleToMessages(example));\n}\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"6\n"})}),"\n",(0,a.jsx)(n.p,{children:"Let\u2019s test out the prompt"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const promptValue = await prompt.invoke({\n  text: "this is some text",\n  examples: exampleMessages,\n});\n\npromptValue.toChatMessages();\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'[\n  SystemMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "You are an expert extraction algorithm.\\n" +\n        "Only extract relevant information from the text.\\n" +\n        "If you do n"... 87 more characters,\n      additional_kwargs: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "You are an expert extraction algorithm.\\n" +\n      "Only extract relevant information from the text.\\n" +\n      "If you do n"... 87 more characters,\n    name: undefined,\n    additional_kwargs: {}\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "The ocean is vast and blue. It\'s more than 20,000 feet deep. There are many fish in it.",\n      additional_kwargs: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "The ocean is vast and blue. It\'s more than 20,000 feet deep. There are many fish in it.",\n    name: undefined,\n    additional_kwargs: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "", additional_kwargs: { tool_calls: [ [Object] ] } },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "",\n    name: undefined,\n    additional_kwargs: {\n      tool_calls: [\n        {\n          id: "8fa4d00d-801f-470e-8737-51ee9dc82259",\n          type: "function",\n          function: [Object]\n        }\n      ]\n    }\n  },\n  ToolMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "You have correctly called this tool.",\n      tool_call_id: "8fa4d00d-801f-470e-8737-51ee9dc82259",\n      additional_kwargs: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "You have correctly called this tool.",\n    name: undefined,\n    additional_kwargs: {},\n    tool_call_id: "8fa4d00d-801f-470e-8737-51ee9dc82259"\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "Fiona traveled far from France to Spain.",\n      additional_kwargs: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "Fiona traveled far from France to Spain.",\n    name: undefined,\n    additional_kwargs: {}\n  },\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "", additional_kwargs: { tool_calls: [ [Object] ] } },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "",\n    name: undefined,\n    additional_kwargs: {\n      tool_calls: [\n        {\n          id: "14ad6217-fcbd-47c7-9006-82f612e36c66",\n          type: "function",\n          function: [Object]\n        }\n      ]\n    }\n  },\n  ToolMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: "You have correctly called this tool.",\n      tool_call_id: "14ad6217-fcbd-47c7-9006-82f612e36c66",\n      additional_kwargs: {}\n    },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "You have correctly called this tool.",\n    name: undefined,\n    additional_kwargs: {},\n    tool_call_id: "14ad6217-fcbd-47c7-9006-82f612e36c66"\n  },\n  HumanMessage {\n    lc_serializable: true,\n    lc_kwargs: { content: "this is some text", additional_kwargs: {} },\n    lc_namespace: [ "langchain_core", "messages" ],\n    content: "this is some text",\n    name: undefined,\n    additional_kwargs: {}\n  }\n]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"create-an-extractor",children:"Create an extractor"}),"\n",(0,a.jsxs)(n.p,{children:["Here, we\u2019ll create an extractor using ",(0,a.jsx)(n.strong,{children:"gpt-4"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { ChatOpenAI } from "@langchain/openai";\n\n// We will be using tool calling mode, which\n// requires a tool calling capable model.\nconst llm = new ChatOpenAI({\n  // Consider benchmarking with the best model you can to get\n  // a sense of the best possible quality.\n  model: "gpt-4-0125-preview",\n  temperature: 0,\n});\n\n// For function/tool calling, we can also supply an name for the schema\n// to give the LLM additional context about what it\'s extracting.\nconst extractionRunnable = prompt.pipe(\n  llm.withStructuredOutput(peopleSchema, { name: "people" })\n);\n'})}),"\n",(0,a.jsx)(n.h2,{id:"without-examples",children:"Without examples \ud83d\ude3f"}),"\n",(0,a.jsxs)(n.p,{children:["Notice that even though we\u2019re using ",(0,a.jsx)(n.code,{children:"gpt-4"}),", it\u2019s unreliable with a\n",(0,a.jsx)(n.strong,{children:"very simple"})," test case!"]}),"\n",(0,a.jsx)(n.p,{children:"We run it 5 times below to emphasize this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const text = "The solar system is large, but earth has only 1 moon.";\n\nfor (let i = 0; i < 5; i++) {\n  const result = await extractionRunnable.invoke({\n    text,\n    examples: [],\n  });\n  console.log(result);\n}\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'{\n  people: [ { name: "earth", hair_color: "grey", height_in_meters: "1" } ]\n}\n{ people: [ { name: "earth", hair_color: "moon" } ] }\n{ people: [ { name: "earth", hair_color: "moon" } ] }\n{ people: [ { name: "earth", hair_color: "1 moon" } ] }\n{ people: [] }\n'})}),"\n",(0,a.jsx)(n.h2,{id:"with-examples",children:"With examples \ud83d\ude3b"}),"\n",(0,a.jsx)(n.p,{children:"Reference examples help fix the failure!"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'const text = "The solar system is large, but earth has only 1 moon.";\n\nfor (let i = 0; i < 5; i++) {\n  const result = await extractionRunnable.invoke({\n    text,\n    // Example messages from above\n    examples: exampleMessages,\n  });\n  console.log(result);\n}\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"{ people: [] }\n{ people: [] }\n{ people: [] }\n{ people: [] }\n{ people: [] }\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'await extractionRunnable.invoke({\n  text: "My name is Hair-ison. My hair is black. I am 3 meters tall.",\n  examples: exampleMessages,\n});\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'{\n  people: [ { name: "Hair-ison", hair_color: "black", height_in_meters: "3" } ]\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,a.jsx)(n.p,{children:"You\u2019ve now learned how to improve extraction quality using few-shot\nexamples."}),"\n",(0,a.jsxs)(n.p,{children:["Next, check out some of the other guides in this section, such as ",(0,a.jsx)(n.a,{href:"../../docs/how_to/extraction_long_text",children:"some\ntips on how to perform extraction on long\ntext"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},19365:(e,n,t)=>{t.d(n,{A:()=>l});t(96540);var a=t(34164);const s={tabItem:"tabItem_Ymn6"};var o=t(74848);function l(e){let{children:n,hidden:t,className:l}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,a.A)(s.tabItem,l),hidden:t,children:n})}},11470:(e,n,t)=>{t.d(n,{A:()=>y});var a=t(96540),s=t(34164),o=t(23104),l=t(56347),r=t(205),i=t(57485),c=t(31682),d=t(89466);function u(e){return a.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,a.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:s}}=e;return{value:n,label:t,attributes:a,default:s}}))}(t);return function(e){const n=(0,c.X)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function p(e){let{queryString:n=!1,groupId:t}=e;const s=(0,l.W6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,i.aZ)(o),(0,a.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(s.location.search);n.set(o,e),s.replace({...s.location,search:n.toString()})}),[o,s])]}function g(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,o=h(e),[l,i]=(0,a.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[c,u]=p({queryString:t,groupId:s}),[g,x]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[s,o]=(0,d.Dv)(t);return[s,(0,a.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:s}),f=(()=>{const e=c??g;return m({value:e,tabValues:o})?e:null})();(0,r.A)((()=>{f&&i(f)}),[f]);return{selectedValue:l,selectValue:(0,a.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);i(e),u(e),x(e)}),[u,x,o]),tabValues:o}}var x=t(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var b=t(74848);function _(e){let{className:n,block:t,selectedValue:a,selectValue:l,tabValues:r}=e;const i=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),d=e=>{const n=e.currentTarget,t=i.indexOf(n),s=r[t].value;s!==a&&(c(n),l(s))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":t},n),children:r.map((e=>{let{value:n,label:t,attributes:o}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:a===n?0:-1,"aria-selected":a===n,ref:e=>i.push(e),onKeyDown:u,onClick:d,...o,className:(0,s.A)("tabs__item",f.tabItem,o?.className,{"tabs__item--active":a===n}),children:t??n},n)}))})}function w(e){let{lazy:n,children:t,selectedValue:s}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===s));return e?(0,a.cloneElement)(e,{className:"margin-top--md"}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:o.map(((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==s})))})}function v(e){const n=g(e);return(0,b.jsxs)("div",{className:(0,s.A)("tabs-container",f.tabList),children:[(0,b.jsx)(_,{...n,...e}),(0,b.jsx)(w,{...n,...e})]})}function y(e){const n=(0,x.A)();return(0,b.jsx)(v,{...e,children:u(e.children)},String(n))}},27846:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(11470),s=t(19365),o=t(21432),l=t(74848);function r(e){let{children:n}=e;return(0,l.jsxs)(a.A,{groupId:"npm2yarn",children:[(0,l.jsx)(s.A,{value:"npm",label:"npm",children:(0,l.jsxs)(o.A,{language:"bash",children:["npm i ",n]})}),(0,l.jsx)(s.A,{value:"yarn",label:"yarn",default:!0,children:(0,l.jsxs)(o.A,{language:"bash",children:["yarn add ",n]})}),(0,l.jsx)(s.A,{value:"pnpm",label:"pnpm",children:(0,l.jsxs)(o.A,{language:"bash",children:["pnpm add ",n]})})]})}}}]);