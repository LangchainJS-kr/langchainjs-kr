"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[2986],{23288:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>d,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var t=a(74848),s=a(28453),r=a(63142);const o={sidebar_class_name:"hidden",sidebar_position:1.5,title:"How to stream chat model responses"},i=void 0,l={id:"how_to/chat_streaming",title:"How to stream chat model responses",description:"All [chat",source:"@site/docs/how_to/chat_streaming.mdx",sourceDirName:"how_to",slug:"/how_to/chat_streaming",permalink:"/docs/how_to/chat_streaming",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/chat_streaming.mdx",tags:[],version:"current",sidebarPosition:1.5,frontMatter:{sidebar_class_name:"hidden",sidebar_position:1.5,title:"How to stream chat model responses"},sidebar:"tutorialSidebar",previous:{title:"How to stream responses from an LLM",permalink:"/docs/how_to/streaming_llm"},next:{title:"How to embed text data",permalink:"/docs/how_to/embed_text"}},d={},c=[{value:"Streaming",id:"streaming",level:2},{value:"Stream events",id:"stream-events",level:2},{value:"Next steps",id:"next-steps",level:2}];function h(n){const e={a:"a",admonition:"admonition",code:"code",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:["All ",(0,t.jsx)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html",children:"chat\nmodels"}),"\nimplement the ",(0,t.jsx)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html",children:"Runnable\ninterface"}),",\nwhich comes with a ",(0,t.jsx)(e.strong,{children:"default"})," implementations of standard runnable\nmethods (i.e.\xa0",(0,t.jsx)(e.code,{children:"invoke"}),", ",(0,t.jsx)(e.code,{children:"batch"}),", ",(0,t.jsx)(e.code,{children:"stream"}),", ",(0,t.jsx)(e.code,{children:"streamEvents"}),")."]}),"\n",(0,t.jsxs)(e.p,{children:["The ",(0,t.jsx)(e.strong,{children:"default"})," streaming implementation provides an ",(0,t.jsx)(e.code,{children:"AsyncGenerator"}),"\nthat yields a single value: the final output from the underlying chat\nmodel provider."]}),"\n",(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsxs)(e.p,{children:["The ",(0,t.jsx)(e.strong,{children:"default"})," implementation does ",(0,t.jsx)(e.strong,{children:"not"})," provide support for\ntoken-by-token streaming, but it ensures that the the model can be\nswapped in for any other model as it supports the same standard\ninterface."]})}),"\n",(0,t.jsx)(e.p,{children:"The ability to stream the output token-by-token depends on whether the\nprovider has implemented proper streaming support."}),"\n",(0,t.jsxs)(e.p,{children:["See which ",(0,t.jsx)(e.a,{href:"../../docs/integrations/chat/",children:"integrations support token-by-token streaming\nhere"}),"."]}),"\n",(0,t.jsx)(e.h2,{id:"streaming",children:"Streaming"}),"\n",(0,t.jsxs)(e.p,{children:["Below, we use a ",(0,t.jsx)(e.code,{children:"---"})," to help visualize the delimiter between tokens."]}),"\n","\n",(0,t.jsx)(r.A,{}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'for await (const chunk of await model.stream(\n  "Write me a 1 verse song about goldfish on the moon"\n)) {\n  console.log(`${chunk.content}\n---`);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:"\n---\nHere\n---\n is\n---\n a\n---\n\n---\n1\n---\n\n---\nverse\n---\n song\n---\n about\n---\n gol\n---\ndfish\n---\n on\n---\n the\n---\n moon\n---\n:\n---\n\n\nGol\n---\ndfish\n---\n on\n---\n the\n---\n moon\n---\n,\n---\n swimming\n---\n through\n---\n the\n---\n sk\n---\nies\n---\n,\n---\n\nFloating\n---\n in\n---\n the\n---\n darkness\n---\n,\n---\n beneath\n---\n the\n---\n lunar\n---\n eyes\n---\n.\n---\n\nWeight\n---\nless\n---\n as\n---\n they\n---\n drift\n---\n,\n---\n through\n---\n the\n---\n endless\n---\n voi\n---\nd,\n---\n\nD\n---\nrif\n---\nting\n---\n,\n---\n swimming\n---\n,\n---\n exploring\n---\n,\n---\n this\n---\n new\n---\n worl\n---\nd unexp\n---\nlo\n---\nye\n---\nd.\n---\n\n---\n\n---\n"})}),"\n",(0,t.jsx)(e.h2,{id:"stream-events",children:"Stream events"}),"\n",(0,t.jsxs)(e.p,{children:["Chat models also support the standard\n",(0,t.jsx)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html#streamEvents",children:"streamEvents()"}),"\nmethod."]}),"\n",(0,t.jsx)(e.p,{children:"This method is useful if you\u2019re streaming output from a larger LLM\napplication that contains multiple steps (e.g., a chain composed of a\nprompt, chat model and parser)."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-typescript",children:'let idx = 0;\n\nfor await (const event of model.streamEvents(\n  "Write me a 1 verse song about goldfish on the moon",\n  {\n    version: "v1",\n  }\n)) {\n  idx += 1;\n  if (idx >= 5) {\n    console.log("...Truncated");\n    break;\n  }\n  console.log(event);\n}\n'})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-text",children:'{\n  run_id: "a84e1294-d281-4757-8f3f-dc4440612949",\n  event: "on_llm_start",\n  name: "ChatAnthropic",\n  tags: [],\n  metadata: {},\n  data: { input: "Write me a 1 verse song about goldfish on the moon" }\n}\n{\n  event: "on_llm_stream",\n  run_id: "a84e1294-d281-4757-8f3f-dc4440612949",\n  tags: [],\n  metadata: {},\n  name: "ChatAnthropic",\n  data: {\n    chunk: AIMessageChunk {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "",\n        additional_kwargs: {\n          id: "msg_01DqDQ9in33ZhmrCzdZaRNMZ",\n          type: "message",\n          role: "assistant",\n          model: "claude-3-haiku-20240307"\n        },\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: [],\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "",\n      name: undefined,\n      additional_kwargs: {\n        id: "msg_01DqDQ9in33ZhmrCzdZaRNMZ",\n        type: "message",\n        role: "assistant",\n        model: "claude-3-haiku-20240307"\n      },\n      response_metadata: {},\n      tool_calls: [],\n      invalid_tool_calls: [],\n      tool_call_chunks: []\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  run_id: "a84e1294-d281-4757-8f3f-dc4440612949",\n  tags: [],\n  metadata: {},\n  name: "ChatAnthropic",\n  data: {\n    chunk: AIMessageChunk {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: "Here",\n        additional_kwargs: {},\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: [],\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: "Here",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {},\n      tool_calls: [],\n      invalid_tool_calls: [],\n      tool_call_chunks: []\n    }\n  }\n}\n{\n  event: "on_llm_stream",\n  run_id: "a84e1294-d281-4757-8f3f-dc4440612949",\n  tags: [],\n  metadata: {},\n  name: "ChatAnthropic",\n  data: {\n    chunk: AIMessageChunk {\n      lc_serializable: true,\n      lc_kwargs: {\n        content: " is",\n        additional_kwargs: {},\n        tool_calls: [],\n        invalid_tool_calls: [],\n        tool_call_chunks: [],\n        response_metadata: {}\n      },\n      lc_namespace: [ "langchain_core", "messages" ],\n      content: " is",\n      name: undefined,\n      additional_kwargs: {},\n      response_metadata: {},\n      tool_calls: [],\n      invalid_tool_calls: [],\n      tool_call_chunks: []\n    }\n  }\n}\n...Truncated\n'})}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,t.jsx)(e.p,{children:"You\u2019ve now seen a few ways you can stream chat model responses."}),"\n",(0,t.jsxs)(e.p,{children:["Next, check out this guide for more on ",(0,t.jsx)(e.a,{href:"../../docs/how_to/streaming",children:"streaming with other LangChain\nmodules"}),"."]})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(h,{...n})}):h(n)}},63142:(n,e,a)=>{a.d(e,{A:()=>m});a(96540);var t=a(11470),s=a(19365),r=a(21432),o=a(27846),i=a(27293),l=a(74848);function d(n){let{children:e}=n;return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(i.A,{type:"tip",children:(0,l.jsxs)("p",{children:["See"," ",(0,l.jsx)("a",{href:"/docs/get_started/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})}),(0,l.jsx)(o.A,{children:e})]})}const c={openaiParams:'{\n  model: "gpt-3.5-turbo",\n  temperature: 0\n}',anthropicParams:'{\n  model: "claude-3-sonnet-20240229",\n  temperature: 0\n}',fireworksParams:'{\n  model: "accounts/fireworks/models/firefunction-v1",\n  temperature: 0\n}',mistralParams:'{\n  model: "mistral-large-latest",\n  temperature: 0\n}',groqParams:'{\n  model: "mixtral-8x7b-32768",\n  temperature: 0\n}',vertexParams:'{\n  model: "gemini-1.5-pro",\n  temperature: 0\n}'},h=["openai","anthropic","mistral","groq","vertex"];function m(n){const{customVarName:e,additionalDependencies:a}=n,o=e??"model",i=n.openaiParams??c.openaiParams,m=n.anthropicParams??c.anthropicParams,p=n.fireworksParams??c.fireworksParams,u=n.mistralParams??c.mistralParams,g=n.groqParams??c.groqParams,_=n.vertexParams??c.vertexParams,x=n.providers??["openai","anthropic","fireworks","mistral","groq","vertex"],v={openai:{value:"openai",label:"OpenAI",default:!0,text:`import { ChatOpenAI } from "@langchain/openai";\n\nconst ${o} = new ChatOpenAI(${i});`,envs:"OPENAI_API_KEY=your-api-key",dependencies:"@langchain/openai"},anthropic:{value:"anthropic",label:"Anthropic",default:!1,text:`import { ChatAnthropic } from "@langchain/anthropic";\n\nconst ${o} = new ChatAnthropic(${m});`,envs:"ANTHROPIC_API_KEY=your-api-key",dependencies:"@langchain/anthropic"},fireworks:{value:"fireworks",label:"FireworksAI",default:!1,text:`import { ChatFireworks } from "@langchain/community/chat_models/fireworks";\n\nconst ${o} = new ChatFireworks(${p});`,envs:"FIREWORKS_API_KEY=your-api-key",dependencies:"@langchain/community"},mistral:{value:"mistral",label:"MistralAI",default:!1,text:`import { ChatMistralAI } from "@langchain/mistralai";\n\nconst ${o} = new ChatMistralAI(${u});`,envs:"MISTRAL_API_KEY=your-api-key",dependencies:"@langchain/mistralai"},groq:{value:"groq",label:"Groq",default:!1,text:`import { ChatGroq } from "@langchain/groq";\n\nconst ${o} = new ChatGroq(${g});`,envs:"GROQ_API_KEY=your-api-key",dependencies:"@langchain/groq"},vertex:{value:"vertex",label:"VertexAI",default:!1,text:`import { ChatVertexAI } from "@langchain/google-vertexai";\n\nconst ${o} = new ChatVertexAI(${_});`,envs:"GOOGLE_APPLICATION_CREDENTIALS=credentials.json",dependencies:"@langchain/google-vertexai"}},f=(n.onlyWso?h:x).map((n=>v[n]));return(0,l.jsxs)("div",{children:[(0,l.jsx)("h3",{children:"Pick your chat model:"}),(0,l.jsx)(t.A,{groupId:"modelTabs",children:f.map((n=>(0,l.jsxs)(s.A,{value:n.value,label:n.label,children:[(0,l.jsx)("h4",{children:"Install dependencies"}),(0,l.jsx)(d,{children:[n.dependencies,a].join(" ")}),(0,l.jsx)("h4",{children:"Add environment variables"}),(0,l.jsx)(r.A,{language:"bash",children:n.envs}),(0,l.jsx)("h4",{children:"Instantiate the model"}),(0,l.jsx)(r.A,{language:"typescript",children:n.text})]},n.value)))})]})}},27846:(n,e,a)=>{a.d(e,{A:()=>i});a(96540);var t=a(11470),s=a(19365),r=a(21432),o=a(74848);function i(n){let{children:e}=n;return(0,o.jsxs)(t.A,{groupId:"npm2yarn",children:[(0,o.jsx)(s.A,{value:"npm",label:"npm",children:(0,o.jsxs)(r.A,{language:"bash",children:["npm i ",e]})}),(0,o.jsx)(s.A,{value:"yarn",label:"yarn",default:!0,children:(0,o.jsxs)(r.A,{language:"bash",children:["yarn add ",e]})}),(0,o.jsx)(s.A,{value:"pnpm",label:"pnpm",children:(0,o.jsxs)(r.A,{language:"bash",children:["pnpm add ",e]})})]})}}}]);