(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8096,7817],{87895:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>S,contentTitle:()=>M,default:()=>N,frontMatter:()=>v,metadata:()=>T,toc:()=>H});var o=t(74848),a=t(28453),s=t(64428),i=t(15082),r=t.n(i),l=t(78847),c=t(2280),p=t(48805),d=t.n(p),u=t(26177),m=t.n(u),h=t(92022),g=t.n(h),f=t(94071),w=t.n(f),b=t(94220),y=t.n(b),x=t(6582),_=t.n(x),k=t(55376),I=t.n(k),O=t(26714),j=t.n(O),A=t(49477),C=t.n(A);const v={sidebar_label:"OpenAI"},M="ChatOpenAI",T={id:"integrations/chat/openai",title:"ChatOpenAI",description:"You can use OpenAI's chat models as follows:",source:"@site/docs/integrations/chat/openai.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/openai",permalink:"/docs/integrations/chat/openai",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/openai.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"OpenAI"},sidebar:"integrations",previous:{title:"Ollama Functions",permalink:"/docs/integrations/chat/ollama_functions"},next:{title:"PremAI",permalink:"/docs/integrations/chat/premai"}},S={},H=[...l.toc,...c.toc,{value:"Multimodal messages",id:"multimodal-messages",level:2},{value:"Tool calling",id:"tool-calling",level:2},{value:"<code>.withStructuredOutput({ ... })</code>",id:"withstructuredoutput--",level:3},{value:"Custom URLs",id:"custom-urls",level:2},{value:"Calling fine-tuned models",id:"calling-fine-tuned-models",level:2},{value:"Generation metadata",id:"generation-metadata",level:2},{value:"With callbacks",id:"with-callbacks",level:3},{value:"With <code>.generate()</code>",id:"with-generate",level:3}];function R(n){const e={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"chatopenai",children:"ChatOpenAI"}),"\n",(0,o.jsx)(e.p,{children:"You can use OpenAI's chat models as follows:"}),"\n","\n","\n",(0,o.jsx)(l.default,{}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai\n"})}),"\n","\n",(0,o.jsx)(c.default,{}),"\n",(0,o.jsx)(s.A,{language:"typescript",children:r()}),"\n",(0,o.jsxs)(e.p,{children:["If you're part of an organization, you can set ",(0,o.jsx)(e.code,{children:"process.env.OPENAI_ORGANIZATION"})," with your OpenAI organization id, or pass it in as ",(0,o.jsx)(e.code,{children:"organization"})," when\ninitializing the model."]}),"\n",(0,o.jsx)(e.h2,{id:"multimodal-messages",children:"Multimodal messages"}),"\n",(0,o.jsx)(e.admonition,{type:"info",children:(0,o.jsx)(e.p,{children:"This feature is currently in preview. The message schema may change in future releases."})}),"\n",(0,o.jsxs)(e.p,{children:["OpenAI supports interleaving images with text in input messages with their ",(0,o.jsx)(e.code,{children:"gpt-4-vision-preview"}),". Here's an example of how this looks:"]}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:d()}),"\n",(0,o.jsx)(e.h2,{id:"tool-calling",children:"Tool calling"}),"\n",(0,o.jsx)(e.admonition,{type:"info",children:(0,o.jsxs)(e.p,{children:["This feature is currently only available for ",(0,o.jsx)(e.code,{children:"gpt-3.5-turbo-1106"})," and ",(0,o.jsx)(e.code,{children:"gpt-4-1106-preview"})," models."]})}),"\n",(0,o.jsx)(e.p,{children:"More recent OpenAI chat models support calling multiple functions to get all required data to answer a question.\nHere's an example how a conversation turn with this functionality might look:"}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:m()}),"\n",(0,o.jsx)(e.h3,{id:"withstructuredoutput--",children:(0,o.jsx)(e.code,{children:".withStructuredOutput({ ... })"})}),"\n",(0,o.jsx)(e.admonition,{type:"info",children:(0,o.jsxs)(e.p,{children:["The ",(0,o.jsx)(e.code,{children:".withStructuredOutput"})," method is in beta. It is actively being worked on, so the API may change."]})}),"\n",(0,o.jsxs)(e.p,{children:["You can also use the ",(0,o.jsx)(e.code,{children:".withStructuredOutput({ ... })"})," method to coerce ",(0,o.jsx)(e.code,{children:"ChatOpenAI"})," into returning a structured output."]}),"\n",(0,o.jsxs)(e.p,{children:["The method allows for passing in either a Zod object, or a valid JSON schema (like what is returned from ",(0,o.jsx)(e.a,{href:"https://www.npmjs.com/package/zod-to-json-schema",children:(0,o.jsx)(e.code,{children:"zodToJsonSchema"})}),")."]}),"\n",(0,o.jsxs)(e.p,{children:["Using the method is simple. Just define your LLM and call ",(0,o.jsx)(e.code,{children:".withStructuredOutput({ ... })"})," on it, passing the desired schema."]}),"\n",(0,o.jsxs)(e.p,{children:["Here is an example using a Zod schema and the ",(0,o.jsx)(e.code,{children:"functionCalling"})," mode (default mode):"]}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:g()}),"\n",(0,o.jsx)(e.p,{children:"Additionally, you can pass in an OpenAI function definition or JSON schema directly:"}),"\n",(0,o.jsx)(e.admonition,{type:"info",children:(0,o.jsxs)(e.p,{children:["If using ",(0,o.jsx)(e.code,{children:"jsonMode"})," as the ",(0,o.jsx)(e.code,{children:"method"})," you must include context in your prompt about the structured output you want. This ",(0,o.jsx)(e.em,{children:"must"})," include the keyword: ",(0,o.jsx)(e.code,{children:"JSON"}),"."]})}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:w()}),"\n",(0,o.jsx)(e.h2,{id:"custom-urls",children:"Custom URLs"}),"\n",(0,o.jsxs)(e.p,{children:["You can customize the base URL the SDK sends requests to by passing a ",(0,o.jsx)(e.code,{children:"configuration"})," parameter like this:"]}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:y()}),"\n",(0,o.jsxs)(e.p,{children:["You can also pass other ",(0,o.jsx)(e.code,{children:"ClientOptions"})," parameters accepted by the official SDK."]}),"\n",(0,o.jsxs)(e.p,{children:["If you are hosting on Azure OpenAI, see the ",(0,o.jsx)(e.a,{href:"/docs/integrations/chat/azure",children:"dedicated page instead"}),"."]}),"\n",(0,o.jsx)(e.h2,{id:"calling-fine-tuned-models",children:"Calling fine-tuned models"}),"\n",(0,o.jsxs)(e.p,{children:["You can call fine-tuned OpenAI models by passing in your corresponding ",(0,o.jsx)(e.code,{children:"modelName"})," parameter."]}),"\n",(0,o.jsxs)(e.p,{children:["This generally takes the form of ",(0,o.jsx)(e.code,{children:"ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}"}),". For example:"]}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:_()}),"\n",(0,o.jsx)(e.h2,{id:"generation-metadata",children:"Generation metadata"}),"\n",(0,o.jsxs)(e.p,{children:["If you need additional information like logprobs or token usage, these will be returned directly in the ",(0,o.jsx)(e.code,{children:".invoke"})," response."]}),"\n",(0,o.jsx)(e.admonition,{type:"tip",children:(0,o.jsxs)(e.p,{children:["Requires ",(0,o.jsx)(e.code,{children:"@langchain/core"})," version >=0.1.48."]})}),"\n","\n",(0,o.jsx)(s.A,{language:"typescript",children:I()}),"\n",(0,o.jsx)(e.h3,{id:"with-callbacks",children:"With callbacks"}),"\n",(0,o.jsx)(e.p,{children:"You can also use the callbacks system:"}),"\n",(0,o.jsx)(s.A,{language:"typescript",children:C()}),"\n",(0,o.jsxs)(e.h3,{id:"with-generate",children:["With ",(0,o.jsx)(e.code,{children:".generate()"})]}),"\n",(0,o.jsx)(s.A,{language:"typescript",children:j()})]})}function N(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(R,{...n})}):R(n)}},2280:(n,e,t)=>{"use strict";t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var o=t(74848),a=t(28453);const s={},i=void 0,r={id:"mdx_components/unified_model_params_tooltip",title:"unified_model_params_tooltip",description:"We're unifying model params across all packages. We now suggest using model instead of modelName, and apiKey for API keys.",source:"@site/docs/mdx_components/unified_model_params_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/unified_model_params_tooltip",permalink:"/docs/mdx_components/unified_model_params_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/unified_model_params_tooltip.mdx",tags:[],version:"current",frontMatter:{}},l={},c=[];function p(n){const e={admonition:"admonition",code:"code",p:"p",...(0,a.R)(),...n.components};return(0,o.jsx)(e.admonition,{type:"tip",children:(0,o.jsxs)(e.p,{children:["We're unifying model params across all packages. We now suggest using ",(0,o.jsx)(e.code,{children:"model"})," instead of ",(0,o.jsx)(e.code,{children:"modelName"}),", and ",(0,o.jsx)(e.code,{children:"apiKey"})," for API keys."]})})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},15082:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY\n});\n\n// You can also pass tools or functions to the model, learn more here\n// https://platform.openai.com/docs/guides/gpt/function-calling\n\nconst modelForFunctionCalling = new ChatOpenAI({\n  model: "gpt-4",\n  temperature: 0,\n});\n\nawait modelForFunctionCalling.invoke(\n  [new HumanMessage("What is the weather in New York?")],\n  {\n    functions: [\n      {\n        name: "get_current_weather",\n        description: "Get the current weather in a given location",\n        parameters: {\n          type: "object",\n          properties: {\n            location: {\n              type: "string",\n              description: "The city and state, e.g. San Francisco, CA",\n            },\n            unit: { type: "string", enum: ["celsius", "fahrenheit"] },\n          },\n          required: ["location"],\n        },\n      },\n    ],\n    // You can set the `function_call` arg to force the model to use a function\n    function_call: {\n      name: "get_current_weather",\n    },\n  }\n);\n/*\nAIMessage {\n  text: \'\',\n  name: undefined,\n  additional_kwargs: {\n    function_call: {\n      name: \'get_current_weather\',\n      arguments: \'{\\n  "location": "New York"\\n}\'\n    }\n  }\n}\n*/\n\n// Coerce response type with JSON mode.\n// Requires "gpt-4-1106-preview" or later\nconst jsonModeModel = new ChatOpenAI({\n  model: "gpt-4-1106-preview",\n  maxTokens: 128,\n}).bind({\n  response_format: {\n    type: "json_object",\n  },\n});\n\n// Must be invoked with a system message containing the string "JSON":\n// https://platform.openai.com/docs/guides/text-generation/json-mode\nconst res = await jsonModeModel.invoke([\n  ["system", "Only return JSON"],\n  ["human", "Hi there!"],\n]);\nconsole.log(res);\n\n/*\n  AIMessage {\n    content: \'{\\n  "response": "How can I assist you today?"\\n}\',\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},49477:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\n\n// See https://cookbook.openai.com/examples/using_logprobs for details\nconst model = new ChatOpenAI({\n  logprobs: true,\n  // topLogprobs: 5,\n});\n\nconst result = await model.invoke("Hi there!", {\n  callbacks: [\n    {\n      handleLLMEnd(output) {\n        console.log("GENERATION OUTPUT:", JSON.stringify(output, null, 2));\n      },\n    },\n  ],\n});\n\nconsole.log("FINAL OUTPUT", result);\n\n/*\n  GENERATION OUTPUT: {\n    "generations": [\n      [\n        {\n          "text": "Hello! How can I assist you today?",\n          "message": {\n            "lc": 1,\n            "type": "constructor",\n            "id": [\n              "langchain_core",\n              "messages",\n              "AIMessage"\n            ],\n            "kwargs": {\n              "content": "Hello! How can I assist you today?",\n              "additional_kwargs": {}\n            }\n          },\n          "generationInfo": {\n            "finish_reason": "stop",\n            "logprobs": {\n              "content": [\n                {\n                  "token": "Hello",\n                  "logprob": -0.0010195904,\n                  "bytes": [\n                    72,\n                    101,\n                    108,\n                    108,\n                    111\n                  ],\n                  "top_logprobs": []\n                },\n                {\n                  "token": "!",\n                  "logprob": -0.0004447316,\n                  "bytes": [\n                    33\n                  ],\n                  "top_logprobs": []\n                },\n                {\n                  "token": " How",\n                  "logprob": -0.00006682846,\n                  "bytes": [\n                    32,\n                    72,\n                    111,\n                    119\n                  ],\n                  "top_logprobs": []\n                },\n                ...\n              ]\n            }\n          }\n        }\n      ]\n    ],\n    "llmOutput": {\n      "tokenUsage": {\n        "completionTokens": 9,\n        "promptTokens": 10,\n        "totalTokens": 19\n      }\n    }\n  }\n  FINAL OUTPUT AIMessage {\n    content: \'Hello! How can I assist you today?\',\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}},94220:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  configuration: {\n    baseURL: "https://your_custom_url.com",\n  },\n});\n\nconst message = await model.invoke("Hi there!");\n\nconsole.log(message);\n\n/*\n  AIMessage {\n    content: \'Hello! How can I assist you today?\',\n    additional_kwargs: { function_call: undefined }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}},6582:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  model: "ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}",\n});\n\nconst message = await model.invoke("Hi there!");\n\nconsole.log(message);\n\n/*\n  AIMessage {\n    content: \'Hello! How can I assist you today?\',\n    additional_kwargs: { function_call: undefined }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}},26714:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\nimport { HumanMessage } from "@langchain/core/messages";\n\n// See https://cookbook.openai.com/examples/using_logprobs for details\nconst model = new ChatOpenAI({\n  logprobs: true,\n  // topLogprobs: 5,\n});\n\nconst generations = await model.invoke([new HumanMessage("Hi there!")]);\n\nconsole.log(JSON.stringify(generations, null, 2));\n\n/*\n  {\n    "generations": [\n      [\n        {\n          "text": "Hello! How can I assist you today?",\n          "message": {\n            "lc": 1,\n            "type": "constructor",\n            "id": [\n              "langchain_core",\n              "messages",\n              "AIMessage"\n            ],\n            "kwargs": {\n              "content": "Hello! How can I assist you today?",\n              "additional_kwargs": {}\n            }\n          },\n          "generationInfo": {\n            "finish_reason": "stop",\n            "logprobs": {\n              "content": [\n                {\n                  "token": "Hello",\n                  "logprob": -0.0011337858,\n                  "bytes": [\n                    72,\n                    101,\n                    108,\n                    108,\n                    111\n                  ],\n                  "top_logprobs": []\n                },\n                {\n                  "token": "!",\n                  "logprob": -0.00044127836,\n                  "bytes": [\n                    33\n                  ],\n                  "top_logprobs": []\n                },\n                {\n                  "token": " How",\n                  "logprob": -0.000065994034,\n                  "bytes": [\n                    32,\n                    72,\n                    111,\n                    119\n                  ],\n                  "top_logprobs": []\n                },\n                ...\n              ]\n            }\n          }\n        }\n      ]\n    ],\n    "llmOutput": {\n      "tokenUsage": {\n        "completionTokens": 9,\n        "promptTokens": 10,\n        "totalTokens": 19\n      }\n    }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},55376:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\n\n// See https://cookbook.openai.com/examples/using_logprobs for details\nconst model = new ChatOpenAI({\n  logprobs: true,\n  // topLogprobs: 5,\n});\n\nconst responseMessage = await model.invoke("Hi there!");\n\nconsole.log(JSON.stringify(responseMessage, null, 2));\n\n/*\n  {\n    "lc": 1,\n    "type": "constructor",\n    "id": [\n      "langchain_core",\n      "messages",\n      "AIMessage"\n    ],\n    "kwargs": {\n      "content": "Hello! How can I assist you today?",\n      "additional_kwargs": {},\n      "response_metadata": {\n        "tokenUsage": {\n          "completionTokens": 9,\n          "promptTokens": 10,\n          "totalTokens": 19\n        },\n        "finish_reason": "stop",\n        "logprobs": {\n          "content": [\n            {\n              "token": "Hello",\n              "logprob": -0.0006793116,\n              "bytes": [\n                72,\n                101,\n                108,\n                108,\n                111\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": "!",\n              "logprob": -0.00011725161,\n              "bytes": [\n                33\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": " How",\n              "logprob": -0.000038457987,\n              "bytes": [\n                32,\n                72,\n                111,\n                119\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": " can",\n              "logprob": -0.00094290765,\n              "bytes": [\n                32,\n                99,\n                97,\n                110\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": " I",\n              "logprob": -0.0000013856493,\n              "bytes": [\n                32,\n                73\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": " assist",\n              "logprob": -0.14702488,\n              "bytes": [\n                32,\n                97,\n                115,\n                115,\n                105,\n                115,\n                116\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": " you",\n              "logprob": -0.000001147242,\n              "bytes": [\n                32,\n                121,\n                111,\n                117\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": " today",\n              "logprob": -0.000067901296,\n              "bytes": [\n                32,\n                116,\n                111,\n                100,\n                97,\n                121\n              ],\n              "top_logprobs": []\n            },\n            {\n              "token": "?",\n              "logprob": -0.000014974867,\n              "bytes": [\n                63\n              ],\n              "top_logprobs": []\n            }\n          ]\n        }\n      }\n    }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}},26177:n=>{n.exports={content:'import { ChatOpenAI } from "@langchain/openai";\nimport { ToolMessage } from "@langchain/core/messages";\n\n// Mocked out function, could be a database/API call in production\nfunction getCurrentWeather(location: string, _unit?: string) {\n  if (location.toLowerCase().includes("tokyo")) {\n    return JSON.stringify({ location, temperature: "10", unit: "celsius" });\n  } else if (location.toLowerCase().includes("san francisco")) {\n    return JSON.stringify({\n      location,\n      temperature: "72",\n      unit: "fahrenheit",\n    });\n  } else {\n    return JSON.stringify({ location, temperature: "22", unit: "celsius" });\n  }\n}\n\n// Bind function to the model as a tool\nconst chat = new ChatOpenAI({\n  model: "gpt-3.5-turbo-1106",\n  maxTokens: 128,\n}).bind({\n  tools: [\n    {\n      type: "function",\n      function: {\n        name: "get_current_weather",\n        description: "Get the current weather in a given location",\n        parameters: {\n          type: "object",\n          properties: {\n            location: {\n              type: "string",\n              description: "The city and state, e.g. San Francisco, CA",\n            },\n            unit: { type: "string", enum: ["celsius", "fahrenheit"] },\n          },\n          required: ["location"],\n        },\n      },\n    },\n  ],\n  tool_choice: "auto",\n});\n\n// Ask initial question that requires multiple tool calls\nconst res = await chat.invoke([\n  ["human", "What\'s the weather like in San Francisco, Tokyo, and Paris?"],\n]);\nconsole.log(res.additional_kwargs.tool_calls);\n/*\n  [\n    {\n      id: \'call_IiOsjIZLWvnzSh8iI63GieUB\',\n      type: \'function\',\n      function: {\n        name: \'get_current_weather\',\n        arguments: \'{"location": "San Francisco", "unit": "celsius"}\'\n      }\n    },\n    {\n      id: \'call_blQ3Oz28zSfvS6Bj6FPEUGA1\',\n      type: \'function\',\n      function: {\n        name: \'get_current_weather\',\n        arguments: \'{"location": "Tokyo", "unit": "celsius"}\'\n      }\n    },\n    {\n      id: \'call_Kpa7FaGr3F1xziG8C6cDffsg\',\n      type: \'function\',\n      function: {\n        name: \'get_current_weather\',\n        arguments: \'{"location": "Paris", "unit": "celsius"}\'\n      }\n    }\n  ]\n*/\n\n// Format the results from calling the tool calls back to OpenAI as ToolMessages\nconst toolMessages = res.additional_kwargs.tool_calls?.map((toolCall) => {\n  const toolCallResult = getCurrentWeather(\n    JSON.parse(toolCall.function.arguments).location\n  );\n  return new ToolMessage({\n    tool_call_id: toolCall.id,\n    name: toolCall.function.name,\n    content: toolCallResult,\n  });\n});\n\n// Send the results back as the next step in the conversation\nconst finalResponse = await chat.invoke([\n  ["human", "What\'s the weather like in San Francisco, Tokyo, and Paris?"],\n  res,\n  ...(toolMessages ?? []),\n]);\n\nconsole.log(finalResponse);\n/*\n  AIMessage {\n    content: \'The current weather in:\\n\' +\n      \'- San Francisco is 72\xb0F\\n\' +\n      \'- Tokyo is 10\xb0C\\n\' +\n      \'- Paris is 22\xb0C\',\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"ToolMessage",imported:"ToolMessage",source:"@langchain/core/messages"}]}},48805:n=>{n.exports={content:'import * as fs from "node:fs/promises";\n\nimport { ChatOpenAI } from "@langchain/openai";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst imageData = await fs.readFile("./hotdog.jpg");\nconst chat = new ChatOpenAI({\n  model: "gpt-4-vision-preview",\n  maxTokens: 1024,\n});\nconst message = new HumanMessage({\n  content: [\n    {\n      type: "text",\n      text: "What\'s in this image?",\n    },\n    {\n      type: "image_url",\n      image_url: {\n        url: `data:image/jpeg;base64,${imageData.toString("base64")}`,\n      },\n    },\n  ],\n});\n\nconst res = await chat.invoke([message]);\nconsole.log({ res });\n\n/*\n  {\n    res: AIMessage {\n      content: \'The image shows a hot dog, which consists of a grilled or steamed sausage served in the slit of a partially sliced bun. This particular hot dog appears to be plain, without any visible toppings or condiments.\',\n      additional_kwargs: { function_call: undefined }\n    }\n  }\n*/\n\nconst hostedImageMessage = new HumanMessage({\n  content: [\n    {\n      type: "text",\n      text: "What does this image say?",\n    },\n    {\n      type: "image_url",\n      image_url:\n        "https://www.freecodecamp.org/news/content/images/2023/05/Screenshot-2023-05-29-at-5.40.38-PM.png",\n    },\n  ],\n});\nconst res2 = await chat.invoke([hostedImageMessage]);\nconsole.log({ res2 });\n\n/*\n  {\n    res2: AIMessage {\n      content: \'The image contains the text "LangChain" with a graphical depiction of a parrot on the left and two interlocked rings on the left side of the text.\',\n      additional_kwargs: { function_call: undefined }\n    }\n  }\n*/\n\nconst lowDetailImage = new HumanMessage({\n  content: [\n    {\n      type: "text",\n      text: "Summarize the contents of this image.",\n    },\n    {\n      type: "image_url",\n      image_url: {\n        url: "https://blog.langchain.dev/content/images/size/w1248/format/webp/2023/10/Screenshot-2023-10-03-at-4.55.29-PM.png",\n        detail: "low",\n      },\n    },\n  ],\n});\nconst res3 = await chat.invoke([lowDetailImage]);\nconsole.log({ res3 });\n\n/*\n  {\n    res3: AIMessage {\n      content: \'The image shows a user interface for a service named "WebLangChain," which appears to be powered by "Twalv." It includes a text box with the prompt "Ask me anything about anything!" suggesting that users can enter questions on various topics. Below the text box, there are example questions that users might ask, such as "what is langchain?", "history of mesopotamia," "how to build a discord bot," "leonardo dicaprio girlfriend," "fun gift ideas for software engineers," "how does a prism separate light," and "what beer is best." The interface also includes a round blue button with a paper plane icon, presumably to submit the question. The overall theme of the image is dark with blue accents.\',\n      additional_kwargs: { function_call: undefined }\n    }\n  }\n*/\n',imports:[{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},94071:n=>{n.exports={content:'import { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { ChatOpenAI } from "@langchain/openai";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n  model: "gpt-4-turbo-preview",\n});\n\nconst calculatorSchema = {\n  type: "object",\n  properties: {\n    operation: {\n      type: "string",\n      enum: ["add", "subtract", "multiply", "divide"],\n    },\n    number1: { type: "number" },\n    number2: { type: "number" },\n  },\n  required: ["operation", "number1", "number2"],\n};\n\n// Default mode is "functionCalling"\nconst modelWithStructuredOutput = model.withStructuredOutput(calculatorSchema);\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    `You are VERY bad at math and must always use a calculator.\nRespond with a JSON object containing three keys:\n\'operation\': the type of operation to execute, either \'add\', \'subtract\', \'multiply\' or \'divide\',\n\'number1\': the first number to operate on,\n\'number2\': the second number to operate on.\n`,\n  ],\n  ["human", "Please help me!! What is 2 + 2?"],\n]);\nconst chain = prompt.pipe(modelWithStructuredOutput);\nconst result = await chain.invoke({});\nconsole.log(result);\n/*\n{ operation: \'add\', number1: 2, number2: 2 }\n */\n\n/**\n * You can also specify \'includeRaw\' to return the parsed\n * and raw output in the result, as well as a "name" field\n * to give the LLM additional context as to what you are generating.\n */\nconst includeRawModel = model.withStructuredOutput(calculatorSchema, {\n  name: "calculator",\n  includeRaw: true,\n  method: "jsonMode",\n});\n\nconst includeRawChain = prompt.pipe(includeRawModel);\nconst includeRawResult = await includeRawChain.invoke({});\nconsole.log(JSON.stringify(includeRawResult, null, 2));\n/*\n{\n  "raw": {\n    "kwargs": {\n      "content": "{\\n  \\"operation\\": \\"add\\",\\n  \\"number1\\": 2,\\n  \\"number2\\": 2\\n}",\n      "additional_kwargs": {}\n    }\n  },\n  "parsed": {\n    "operation": "add",\n    "number1": 2,\n    "number2": 2\n  }\n}\n */\n',imports:[{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}},92022:n=>{n.exports={content:'import { ChatPromptTemplate } from "@langchain/core/prompts";\nimport { ChatOpenAI } from "@langchain/openai";\nimport { z } from "zod";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n  model: "gpt-4-turbo-preview",\n});\n\nconst calculatorSchema = z.object({\n  operation: z.enum(["add", "subtract", "multiply", "divide"]),\n  number1: z.number(),\n  number2: z.number(),\n});\n\nconst modelWithStructuredOutput = model.withStructuredOutput(calculatorSchema);\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  ["system", "You are VERY bad at math and must always use a calculator."],\n  ["human", "Please help me!! What is 2 + 2?"],\n]);\nconst chain = prompt.pipe(modelWithStructuredOutput);\nconst result = await chain.invoke({});\nconsole.log(result);\n/*\n{ operation: \'add\', number1: 2, number2: 2 }\n */\n\n/**\n * You can also specify \'includeRaw\' to return the parsed\n * and raw output in the result.\n */\nconst includeRawModel = model.withStructuredOutput(calculatorSchema, {\n  name: "calculator",\n  includeRaw: true,\n});\n\nconst includeRawChain = prompt.pipe(includeRawModel);\nconst includeRawResult = await includeRawChain.invoke({});\nconsole.log(JSON.stringify(includeRawResult, null, 2));\n/*\n{\n  "raw": {\n    "kwargs": {\n      "content": "",\n      "additional_kwargs": {\n        "tool_calls": [\n          {\n            "id": "call_A8yzNBDMiRrCB8dFYqJLhYW7",\n            "type": "function",\n            "function": {\n              "name": "calculator",\n              "arguments": "{\\"operation\\":\\"add\\",\\"number1\\":2,\\"number2\\":2}"\n            }\n          }\n        ]\n      }\n    }\n  },\n  "parsed": {\n    "operation": "add",\n    "number1": 2,\n    "number2": 2\n  }\n}\n */\n',imports:[{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"ChatOpenAI",imported:"ChatOpenAI",source:"@langchain/openai"}]}}}]);