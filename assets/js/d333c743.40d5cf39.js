"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8871],{79006:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var s=t(74848),a=t(28453);const o={sidebar_class_name:"hidden",sidebar_position:3},r="How to create a custom LLM class",i={id:"how_to/custom_llm",title:"How to create a custom LLM class",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/custom_llm.mdx",sourceDirName:"how_to",slug:"/how_to/custom_llm",permalink:"/docs/how_to/custom_llm",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/custom_llm.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_class_name:"hidden",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"How to cache chat model responses",permalink:"/docs/how_to/chat_model_caching"},next:{title:"How to use few shot examples",permalink:"/docs/how_to/few_shot_examples"}},l={},c=[{value:"Richer outputs",id:"richer-outputs",level:2}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"how-to-create-a-custom-llm-class",children:"How to create a custom LLM class"}),"\n",(0,s.jsxs)(e.admonition,{title:"Prerequisites",type:"info",children:[(0,s.jsx)(e.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"/docs/concepts/#llms",children:"LLMs"})}),"\n"]})]}),"\n",(0,s.jsx)(e.p,{children:"This notebook goes over how to create a custom LLM wrapper, in case you want to use your own LLM or a different wrapper than one that is directly supported in LangChain."}),"\n",(0,s.jsxs)(e.p,{children:["There are a few required things that a custom LLM needs to implement after extending the ",(0,s.jsxs)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_language_models_llms.LLM.html",children:[(0,s.jsx)(e.code,{children:"LLM"})," class"]}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["A ",(0,s.jsx)(e.code,{children:"_call"})," method that takes in a string and call options (which includes things like ",(0,s.jsx)(e.code,{children:"stop"})," sequences), and returns a string."]}),"\n",(0,s.jsxs)(e.li,{children:["A ",(0,s.jsx)(e.code,{children:"_llmType"})," method that returns a string. Used for logging purposes only."]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"You can also implement the following optional method:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["A ",(0,s.jsx)(e.code,{children:"_streamResponseChunks"})," method that returns an ",(0,s.jsx)(e.code,{children:"AsyncIterator"})," and yields ",(0,s.jsx)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_outputs.GenerationChunk.html",children:(0,s.jsx)(e.code,{children:"GenerationChunks"})}),". This allows the LLM to support streaming outputs."]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:["Let\u2019s implement a very simple custom LLM that just echoes back the first ",(0,s.jsx)(e.code,{children:"n"})," characters of the input."]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'import { LLM, type BaseLLMParams } from "@langchain/core/language_models/llms";\nimport type { CallbackManagerForLLMRun } from "langchain/callbacks";\nimport { GenerationChunk } from "langchain/schema";\n\nexport interface CustomLLMInput extends BaseLLMParams {\n  n: number;\n}\n\nexport class CustomLLM extends LLM {\n  n: number;\n\n  constructor(fields: CustomLLMInput) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  _llmType() {\n    return "custom";\n  }\n\n  async _call(\n    prompt: string,\n    options: this["ParsedCallOptions"],\n    runManager: CallbackManagerForLLMRun\n  ): Promise<string> {\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    return prompt.slice(0, this.n);\n  }\n\n  async *_streamResponseChunks(\n    prompt: string,\n    options: this["ParsedCallOptions"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    for (const letter of prompt.slice(0, this.n)) {\n      yield new GenerationChunk({\n        text: letter,\n      });\n      // Trigger the appropriate callback\n      await runManager?.handleLLMNewToken(letter);\n    }\n  }\n}\n'})}),"\n",(0,s.jsx)(e.p,{children:"We can now use this as any other LLM:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'const llm = new CustomLLM({ n: 4 });\n\nawait llm.invoke("I am an LLM");\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"I am\n"})}),"\n",(0,s.jsx)(e.p,{children:"And support streaming:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-typescript",children:'const stream = await llm.stream("I am an LLM");\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"I\n\na\nm\n"})}),"\n",(0,s.jsx)(e.h2,{id:"richer-outputs",children:"Richer outputs"}),"\n",(0,s.jsxs)(e.p,{children:["If you want to take advantage of LangChain's callback system for functionality like token tracking, you can extend the ",(0,s.jsx)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_language_models_llms.BaseLLM.html",children:(0,s.jsx)(e.code,{children:"BaseLLM"})})," class and implement the lower level\n",(0,s.jsx)(e.code,{children:"_generate"})," method. Rather than taking a single string as input and a single string output, it can take multiple input strings and map each to multiple string outputs.\nAdditionally, it returns a ",(0,s.jsx)(e.code,{children:"Generation"})," output with fields for additional metadata rather than just a string."]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-ts",children:'import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";\nimport { LLMResult } from "@langchain/core/outputs";\nimport {\n  BaseLLM,\n  BaseLLMCallOptions,\n  BaseLLMParams,\n} from "@langchain/core/language_models/llms";\n\nexport interface AdvancedCustomLLMCallOptions extends BaseLLMCallOptions {}\n\nexport interface AdvancedCustomLLMParams extends BaseLLMParams {\n  n: number;\n}\n\nexport class AdvancedCustomLLM extends BaseLLM<AdvancedCustomLLMCallOptions> {\n  n: number;\n\n  constructor(fields: AdvancedCustomLLMParams) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  _llmType() {\n    return "advanced_custom_llm";\n  }\n\n  async _generate(\n    inputs: string[],\n    options: this["ParsedCallOptions"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult> {\n    const outputs = inputs.map((input) => input.slice(0, this.n));\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n\n    // One input could generate multiple outputs.\n    const generations = outputs.map((output) => [\n      {\n        text: output,\n        // Optional additional metadata for the generation\n        generationInfo: { outputCount: 1 },\n      },\n    ]);\n    const tokenUsage = {\n      usedTokens: this.n,\n    };\n    return {\n      generations,\n      llmOutput: { tokenUsage },\n    };\n  }\n}\n'})}),"\n",(0,s.jsx)(e.p,{children:"This will pass the additional returned information in callback events and in the `streamEvents method:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-ts",children:'const llm = new AdvancedCustomLLM({ n: 4 });\n\nconst eventStream = await llm.streamEvents("I am an LLM", {\n  version: "v1",\n});\n\nfor await (const event of eventStream) {\n  if (event.event === "on_llm_end") {\n    console.log(JSON.stringify(event, null, 2));\n  }\n}\n'})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:'{\n  "event": "on_llm_end",\n  "name": "AdvancedCustomLLM",\n  "run_id": "a883a705-c651-4236-8095-cb515e2d4885",\n  "tags": [],\n  "metadata": {},\n  "data": {\n    "output": {\n      "generations": [\n        [\n          {\n            "text": "I am",\n            "generationInfo": {\n              "outputCount": 1\n            }\n          }\n        ]\n      ],\n      "llmOutput": {\n        "tokenUsage": {\n          "usedTokens": 4\n        }\n      }\n    }\n  }\n}\n'})})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}}}]);