(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7288],{91600:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>w,contentTitle:()=>f,default:()=>v,frontMatter:()=>g,metadata:()=>y,toc:()=>x});var a=t(74848),s=t(28453),o=t(64428),i=t(78847),r=t(51421),c=t.n(r),h=t(7910),l=t.n(h),d=t(12843),p=t.n(d),m=t(73017),u=t.n(m);const g={sidebar_class_name:"hidden",sidebar_position:2},f="How to cache model responses",y={id:"how_to/llm_caching",title:"How to cache model responses",description:"LangChain provides an optional caching layer for LLMs. This is useful for two reasons:",source:"@site/docs/how_to/llm_caching.mdx",sourceDirName:"how_to",slug:"/how_to/llm_caching",permalink:"/docs/how_to/llm_caching",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/llm_caching.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_class_name:"hidden",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"How to use few shot examples in chat models",permalink:"/docs/how_to/few_shot_examples_chat"},next:{title:"How to cache chat model responses",permalink:"/docs/how_to/chat_model_caching"}},w={},x=[...i.toc,{value:"In Memory Cache",id:"in-memory-cache",level:2},{value:"Caching with Momento",id:"caching-with-momento",level:2},{value:"Caching with Redis",id:"caching-with-redis",level:2},{value:"Caching with Upstash Redis",id:"caching-with-upstash-redis",level:2},{value:"Caching with Cloudflare KV",id:"caching-with-cloudflare-kv",level:2},{value:"Caching on the File System",id:"caching-on-the-file-system",level:2},{value:"Next steps",id:"next-steps",level:2}];function j(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"how-to-cache-model-responses",children:"How to cache model responses"}),"\n",(0,a.jsx)(n.p,{children:"LangChain provides an optional caching layer for LLMs. This is useful for two reasons:"}),"\n",(0,a.jsx)(n.p,{children:"It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\nIt can speed up your application by reducing the number of API calls you make to the LLM provider."}),"\n","\n","\n",(0,a.jsx)(i.default,{}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/openai\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { OpenAI } from "@langchain/openai";\n\nconst model = new OpenAI({\n  model: "gpt-3.5-turbo-instruct",\n  cache: true,\n});\n'})}),"\n",(0,a.jsx)(n.h2,{id:"in-memory-cache",children:"In Memory Cache"}),"\n",(0,a.jsx)(n.p,{children:"The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'console.time();\n\n// The first time, it is not yet in cache, so it should take longer\nconst res = await model.invoke("Tell me a long joke");\n\nconsole.log(res);\n\nconsole.timeEnd();\n\n/*\n  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.\n\n  The bartender explains, "We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar."\n\n  Intrigued, the man asks what the tasks are.\n\n  The bartender replies, "First, you have to drink a whole bottle of tequila without making a face. Second, there\'s a pitbull out back with a sore tooth. You have to pull it out. And third, there\'s an old lady upstairs who has never had an orgasm. You have to give her one."\n\n  The man thinks for a moment and then confidently says, "I\'ll do it."\n\n  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull\'s tooth in hand.\n\n  The bar erupts in cheers and the bartender leads the man upstairs to the old lady\'s room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.\n\n  The bartender hands the man the jar of money and asks, "How\n\n  default: 4.187s\n*/\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'console.time();\n\n// The second time it is, so it goes faster\nconst res2 = await model.invoke("Tell me a joke");\n\nconsole.log(res2);\n\nconsole.timeEnd();\n\n/*\n  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.\n\n  The bartender explains, "We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar."\n\n  Intrigued, the man asks what the tasks are.\n\n  The bartender replies, "First, you have to drink a whole bottle of tequila without making a face. Second, there\'s a pitbull out back with a sore tooth. You have to pull it out. And third, there\'s an old lady upstairs who has never had an orgasm. You have to give her one."\n\n  The man thinks for a moment and then confidently says, "I\'ll do it."\n\n  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull\'s tooth in hand.\n\n  The bar erupts in cheers and the bartender leads the man upstairs to the old lady\'s room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.\n\n  The bartender hands the man the jar of money and asks, "How\n\n  default: 175.74ms\n*/\n'})}),"\n",(0,a.jsx)(n.h2,{id:"caching-with-momento",children:"Caching with Momento"}),"\n",(0,a.jsxs)(n.p,{children:["LangChain also provides a Momento-based cache. ",(0,a.jsx)(n.a,{href:"https://gomomento.com",children:"Momento"})," is a distributed, serverless cache that requires zero setup or infrastructure maintenance. Given Momento's compatibility with Node.js, browser, and edge environments, ensure you install the relevant package."]}),"\n",(0,a.jsxs)(n.p,{children:["To install for ",(0,a.jsx)(n.strong,{children:"Node.js"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @gomomento/sdk\n"})}),"\n",(0,a.jsxs)(n.p,{children:["To install for ",(0,a.jsx)(n.strong,{children:"browser/edge workers"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @gomomento/sdk-web\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Next you'll need to sign up and create an API key. Once you've done that, pass a ",(0,a.jsx)(n.code,{children:"cache"})," option when you instantiate the LLM like this:"]}),"\n","\n",(0,a.jsx)(o.A,{language:"typescript",children:c()}),"\n",(0,a.jsx)(n.h2,{id:"caching-with-redis",children:"Caching with Redis"}),"\n",(0,a.jsxs)(n.p,{children:["LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the ",(0,a.jsx)(n.code,{children:"redis"})," package:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install ioredis\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Then, you can pass a ",(0,a.jsx)(n.code,{children:"cache"})," option when you instantiate the LLM. For example:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'import { OpenAI } from "@langchain/openai";\nimport { RedisCache } from "@langchain/community/caches/ioredis";\nimport { Redis } from "ioredis";\n\n// See https://github.com/redis/ioredis for connection options\nconst client = new Redis({});\n\nconst cache = new RedisCache(client);\n\nconst model = new OpenAI({ cache });\n'})}),"\n",(0,a.jsx)(n.h2,{id:"caching-with-upstash-redis",children:"Caching with Upstash Redis"}),"\n",(0,a.jsxs)(n.p,{children:["LangChain provides an Upstash Redis-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Upstash Redis client uses HTTP and supports edge environments. To use it, you'll need to install the ",(0,a.jsx)(n.code,{children:"@upstash/redis"})," package:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @upstash/redis\n"})}),"\n",(0,a.jsxs)(n.p,{children:["You'll also need an ",(0,a.jsx)(n.a,{href:"https://docs.upstash.com/redis#create-account",children:"Upstash account"})," and a ",(0,a.jsx)(n.a,{href:"https://docs.upstash.com/redis#create-a-database",children:"Redis database"})," to connect to. Once you've done that, retrieve your REST URL and REST token."]}),"\n",(0,a.jsxs)(n.p,{children:["Then, you can pass a ",(0,a.jsx)(n.code,{children:"cache"})," option when you instantiate the LLM. For example:"]}),"\n","\n",(0,a.jsx)(o.A,{language:"typescript",children:l()}),"\n",(0,a.jsxs)(n.p,{children:["You can also directly pass in a previously created ",(0,a.jsx)(n.a,{href:"https://docs.upstash.com/redis/sdks/javascriptsdk/overview",children:"@upstash/redis"})," client instance:"]}),"\n","\n",(0,a.jsx)(o.A,{language:"typescript",children:p()}),"\n",(0,a.jsx)(n.h2,{id:"caching-with-cloudflare-kv",children:"Caching with Cloudflare KV"}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsx)(n.p,{children:"This integration is only supported in Cloudflare Workers."})}),"\n",(0,a.jsx)(n.p,{children:"If you're deploying your project as a Cloudflare Worker, you can use LangChain's Cloudflare KV-powered LLM cache."}),"\n",(0,a.jsxs)(n.p,{children:["For information on how to set up KV in Cloudflare, see ",(0,a.jsx)(n.a,{href:"https://developers.cloudflare.com/kv/",children:"the official documentation"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," If you are using TypeScript, you may need to install types if they aren't already present:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install -S @cloudflare/workers-types\n"})}),"\n","\n",(0,a.jsx)(o.A,{language:"typescript",children:u()}),"\n",(0,a.jsx)(n.h2,{id:"caching-on-the-file-system",children:"Caching on the File System"}),"\n",(0,a.jsx)(n.admonition,{type:"warning",children:(0,a.jsx)(n.p,{children:"This cache is not recommended for production use. It is only intended for local development."})}),"\n",(0,a.jsx)(n.p,{children:"LangChain provides a simple file system cache.\nBy default the cache is stored a temporary directory, but you can specify a custom directory if you want."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"const cache = await LocalFileCache.create();\n"})}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next steps"}),"\n",(0,a.jsx)(n.p,{children:"You've now learned how to cache model responses to save time and money."}),"\n",(0,a.jsxs)(n.p,{children:["Next, check out the other how-to guides on LLMs, like ",(0,a.jsx)(n.a,{href:"/docs/how_to/custom_llm",children:"how to create your own custom LLM class"}),"."]})]})}function v(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(j,{...e})}):j(e)}},73017:e=>{e.exports={content:'import type { KVNamespace } from "@cloudflare/workers-types";\n\nimport { OpenAI } from "@langchain/openai";\nimport { CloudflareKVCache } from "@langchain/cloudflare";\n\nexport interface Env {\n  KV_NAMESPACE: KVNamespace;\n  OPENAI_API_KEY: string;\n}\n\nexport default {\n  async fetch(_request: Request, env: Env) {\n    try {\n      const cache = new CloudflareKVCache(env.KV_NAMESPACE);\n      const model = new OpenAI({\n        cache,\n        model: "gpt-3.5-turbo-instruct",\n        apiKey: env.OPENAI_API_KEY,\n      });\n      const response = await model.invoke("How are you today?");\n      return new Response(JSON.stringify(response), {\n        headers: { "content-type": "application/json" },\n      });\n    } catch (err: any) {\n      console.log(err.message);\n      return new Response(err.message, { status: 500 });\n    }\n  },\n};\n',imports:[{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"},{local:"CloudflareKVCache",imported:"CloudflareKVCache",source:"@langchain/cloudflare"}]}},51421:e=>{e.exports={content:'import { OpenAI } from "@langchain/openai";\nimport {\n  CacheClient,\n  Configurations,\n  CredentialProvider,\n} from "@gomomento/sdk";\nimport { MomentoCache } from "@langchain/community/caches/momento";\n\n// See https://github.com/momentohq/client-sdk-javascript for connection options\nconst client = new CacheClient({\n  configuration: Configurations.Laptop.v1(),\n  credentialProvider: CredentialProvider.fromEnvironmentVariable({\n    environmentVariableName: "MOMENTO_API_KEY",\n  }),\n  defaultTtlSeconds: 60 * 60 * 24,\n});\nconst cache = await MomentoCache.fromProps({\n  client,\n  cacheName: "langchain",\n});\n\nconst model = new OpenAI({ cache });\n',imports:[{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"},{local:"MomentoCache",imported:"MomentoCache",source:"@langchain/community/caches/momento"}]}},7910:e=>{e.exports={content:'import { OpenAI } from "@langchain/openai";\nimport { UpstashRedisCache } from "@langchain/community/caches/upstash_redis";\n\n// See https://docs.upstash.com/redis/howto/connectwithupstashredis#quick-start for connection options\nconst cache = new UpstashRedisCache({\n  config: {\n    url: "UPSTASH_REDIS_REST_URL",\n    token: "UPSTASH_REDIS_REST_TOKEN",\n  },\n});\n\nconst model = new OpenAI({ cache });\n',imports:[{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"},{local:"UpstashRedisCache",imported:"UpstashRedisCache",source:"@langchain/community/caches/upstash_redis"}]}},12843:e=>{e.exports={content:'import { Redis } from "@upstash/redis";\nimport https from "https";\n\nimport { OpenAI } from "@langchain/openai";\nimport { UpstashRedisCache } from "@langchain/community/caches/upstash_redis";\n\n// const client = new Redis({\n//   url: process.env.UPSTASH_REDIS_REST_URL!,\n//   token: process.env.UPSTASH_REDIS_REST_TOKEN!,\n//   agent: new https.Agent({ keepAlive: true }),\n// });\n\n// Or simply call Redis.fromEnv() to automatically load the UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN environment variables.\nconst client = Redis.fromEnv({\n  agent: new https.Agent({ keepAlive: true }),\n});\n\nconst cache = new UpstashRedisCache({ client });\nconst model = new OpenAI({ cache });\n',imports:[{local:"OpenAI",imported:"OpenAI",source:"@langchain/openai"},{local:"UpstashRedisCache",imported:"UpstashRedisCache",source:"@langchain/community/caches/upstash_redis"}]}}}]);