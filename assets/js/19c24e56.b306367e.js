"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7846],{42821:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var a=t(74848),s=t(28453);const o={sidebar_class_name:"hidden",sidebar_position:4},r="How to create a custom chat model class",i={id:"how_to/custom_chat",title:"How to create a custom chat model class",description:"This guide assumes familiarity with the following concepts:",source:"@site/docs/how_to/custom_chat.mdx",sourceDirName:"how_to",slug:"/how_to/custom_chat",permalink:"/docs/how_to/custom_chat",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/how_to/custom_chat.mdx",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_class_name:"hidden",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"How to add ad-hoc tool calling capability to LLMs and Chat Models",permalink:"/docs/how_to/tools_prompting"},next:{title:"How to do per-user retrieval",permalink:"/docs/how_to/qa_per_user"}},l={},c=[{value:"Richer outputs",id:"richer-outputs",level:2},{value:"Tracing (advanced)",id:"tracing-advanced",level:2}];function d(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"how-to-create-a-custom-chat-model-class",children:"How to create a custom chat model class"}),"\n",(0,a.jsxs)(e.admonition,{title:"Prerequisites",type:"info",children:[(0,a.jsx)(e.p,{children:"This guide assumes familiarity with the following concepts:"}),(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"/docs/concepts/#chat-models",children:"Chat models"})}),"\n"]})]}),"\n",(0,a.jsx)(e.p,{children:"This notebook goes over how to create a custom chat model wrapper, in case you want to use your own chat model or a different wrapper than one that is directly supported in LangChain."}),"\n",(0,a.jsxs)(e.p,{children:["There are a few required things that a chat model needs to implement after extending the ",(0,a.jsxs)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_language_models_chat_models.SimpleChatModel.html",children:[(0,a.jsx)(e.code,{children:"SimpleChatModel"})," class"]}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["A ",(0,a.jsx)(e.code,{children:"_call"})," method that takes in a list of messages and call options (which includes things like ",(0,a.jsx)(e.code,{children:"stop"})," sequences), and returns a string."]}),"\n",(0,a.jsxs)(e.li,{children:["A ",(0,a.jsx)(e.code,{children:"_llmType"})," method that returns a string. Used for logging purposes only."]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"You can also implement the following optional method:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["A ",(0,a.jsx)(e.code,{children:"_streamResponseChunks"})," method that returns an ",(0,a.jsx)(e.code,{children:"AsyncGenerator"})," and yields ",(0,a.jsx)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_outputs.ChatGenerationChunk.html",children:(0,a.jsx)(e.code,{children:"ChatGenerationChunks"})}),". This allows the LLM to support streaming outputs."]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["Let's implement a very simple custom chat model that just echoes back the first ",(0,a.jsx)(e.code,{children:"n"})," characters of the input."]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-typescript",children:'import {\n  SimpleChatModel,\n  type BaseChatModelParams,\n} from "@langchain/core/language_models/chat_models";\nimport { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";\nimport { AIMessageChunk, type BaseMessage } from "@langchain/core/messages";\nimport { ChatGenerationChunk } from "@langchain/core/outputs";\n\nexport interface CustomChatModelInput extends BaseChatModelParams {\n  n: number;\n}\n\nexport class CustomChatModel extends SimpleChatModel {\n  n: number;\n\n  constructor(fields: CustomChatModelInput) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  _llmType() {\n    return "custom";\n  }\n\n  async _call(\n    messages: BaseMessage[],\n    options: this["ParsedCallOptions"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<string> {\n    if (!messages.length) {\n      throw new Error("No messages provided.");\n    }\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    if (typeof messages[0].content !== "string") {\n      throw new Error("Multimodal messages are not supported.");\n    }\n    return messages[0].content.slice(0, this.n);\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this["ParsedCallOptions"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    if (!messages.length) {\n      throw new Error("No messages provided.");\n    }\n    if (typeof messages[0].content !== "string") {\n      throw new Error("Multimodal messages are not supported.");\n    }\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    for (const letter of messages[0].content.slice(0, this.n)) {\n      yield new ChatGenerationChunk({\n        message: new AIMessageChunk({\n          content: letter,\n        }),\n        text: letter,\n      });\n      // Trigger the appropriate callback for new chunks\n      await runManager?.handleLLMNewToken(letter);\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(e.p,{children:"We can now use this as any other chat model:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-typescript",children:'const chatModel = new CustomChatModel({ n: 4 });\n\nawait chatModel.invoke([["human", "I am an LLM"]]);\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"AIMessage {\n  content: 'I am',\n  additional_kwargs: {}\n}\n"})}),"\n",(0,a.jsx)(e.p,{children:"And support streaming:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-typescript",children:'const stream = await chatModel.stream([["human", "I am an LLM"]]);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"AIMessageChunk {\n  content: 'I',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: ' ',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: 'a',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: 'm',\n  additional_kwargs: {}\n}\n"})}),"\n",(0,a.jsx)(e.h2,{id:"richer-outputs",children:"Richer outputs"}),"\n",(0,a.jsxs)(e.p,{children:["If you want to take advantage of LangChain's callback system for functionality like token tracking, you can extend the ",(0,a.jsx)(e.a,{href:"https://v02.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html",children:(0,a.jsx)(e.code,{children:"BaseChatModel"})})," class and implement the lower level\n",(0,a.jsx)(e.code,{children:"_generate"})," method. It also takes a list of ",(0,a.jsx)(e.code,{children:"BaseMessage"}),"s as input, but requires you to construct and return a ",(0,a.jsx)(e.code,{children:"ChatGeneration"})," object that permits additional metadata.\nHere's an example:"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-ts",children:'import { AIMessage, BaseMessage } from "@langchain/core/messages";\nimport { ChatResult } from "@langchain/core/outputs";\nimport {\n  BaseChatModel,\n  BaseChatModelCallOptions,\n  BaseChatModelParams,\n} from "@langchain/core/language_models/chat_models";\nimport { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";\n\nexport interface AdvancedCustomChatModelOptions\n  extends BaseChatModelCallOptions {}\n\nexport interface AdvancedCustomChatModelParams extends BaseChatModelParams {\n  n: number;\n}\n\nexport class AdvancedCustomChatModel extends BaseChatModel<AdvancedCustomChatModelOptions> {\n  n: number;\n\n  static lc_name(): string {\n    return "AdvancedCustomChatModel";\n  }\n\n  constructor(fields: AdvancedCustomChatModelParams) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this["ParsedCallOptions"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (!messages.length) {\n      throw new Error("No messages provided.");\n    }\n    if (typeof messages[0].content !== "string") {\n      throw new Error("Multimodal messages are not supported.");\n    }\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    const content = messages[0].content.slice(0, this.n);\n    const tokenUsage = {\n      usedTokens: this.n,\n    };\n    return {\n      generations: [{ message: new AIMessage({ content }), text: content }],\n      llmOutput: { tokenUsage },\n    };\n  }\n\n  _llmType(): string {\n    return "advanced_custom_chat_model";\n  }\n}\n'})}),"\n",(0,a.jsx)(e.p,{children:"This will pass the additional returned information in callback events and in the `streamEvents method:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-ts",children:'const chatModel = new AdvancedCustomChatModel({ n: 4 });\n\nconst eventStream = await chatModel.streamEvents([["human", "I am an LLM"]], {\n  version: "v1",\n});\n\nfor await (const event of eventStream) {\n  if (event.event === "on_llm_end") {\n    console.log(JSON.stringify(event, null, 2));\n  }\n}\n'})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:'{\n  "event": "on_llm_end",\n  "name": "AdvancedCustomChatModel",\n  "run_id": "b500b98d-bee5-4805-9b92-532a491f5c70",\n  "tags": [],\n  "metadata": {},\n  "data": {\n    "output": {\n      "generations": [\n        [\n          {\n            "message": {\n              "lc": 1,\n              "type": "constructor",\n              "id": [\n                "langchain_core",\n                "messages",\n                "AIMessage"\n              ],\n              "kwargs": {\n                "content": "I am",\n                "additional_kwargs": {}\n              }\n            },\n            "text": "I am"\n          }\n        ]\n      ],\n      "llmOutput": {\n        "tokenUsage": {\n          "usedTokens": 4\n        }\n      }\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"tracing-advanced",children:"Tracing (advanced)"}),"\n",(0,a.jsxs)(e.p,{children:["If you are implementing a custom chat model and want to use it with a tracing service like ",(0,a.jsx)(e.a,{href:"https://smith.langchain.com/",children:"LangSmith"}),",\nyou can automatically log params used for a given invocation by implementing the ",(0,a.jsx)(e.code,{children:"invocationParams()"})," method on the model."]}),"\n",(0,a.jsx)(e.p,{children:"This method is purely optional, but anything it returns will be logged as metadata for the trace."}),"\n",(0,a.jsx)(e.p,{children:"Here's one pattern you might use:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-ts",children:'export interface CustomChatModelOptions extends BaseChatModelCallOptions {\n  // Some required or optional inner args\n  tools: Record<string, any>[];\n}\n\nexport interface CustomChatModelParams extends BaseChatModelParams {\n  temperature: number;\n}\n\nexport class CustomChatModel extends BaseChatModel<CustomChatModelOptions> {\n  temperature: number;\n\n  static lc_name(): string {\n    return "CustomChatModel";\n  }\n\n  constructor(fields: CustomChatModelParams) {\n    super(fields);\n    this.temperature = fields.temperature;\n  }\n\n  // Anything returned in this method will be logged as metadata in the trace.\n  // It is common to pass it any options used to invoke the function.\n  invocationParams(options?: this["ParsedCallOptions"]) {\n    return {\n      tools: options?.tools,\n      n: this.n,\n    };\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this["ParsedCallOptions"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (!messages.length) {\n      throw new Error("No messages provided.");\n    }\n    if (typeof messages[0].content !== "string") {\n      throw new Error("Multimodal messages are not supported.");\n    }\n    const additionalParams = this.invocationParams(options);\n    const content = await someAPIRequest(messages, additionalParams);\n    return {\n      generations: [{ message: new AIMessage({ content }), text: content }],\n      llmOutput: {},\n    };\n  }\n\n  _llmType(): string {\n    return "advanced_custom_chat_model";\n  }\n}\n'})})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);