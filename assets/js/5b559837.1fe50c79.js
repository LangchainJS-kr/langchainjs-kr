(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8565,7817],{97528:(n,e,a)=>{"use strict";a.r(e),a.d(e,{assets:()=>j,contentTitle:()=>A,default:()=>I,frontMatter:()=>y,metadata:()=>v,toc:()=>k});var t=a(74848),s=a(28453),i=a(64428),o=a(78847),m=a(2280),l=a(5341),c=a.n(l),r=a(95778),h=a.n(r),d=a(37050),p=a.n(d),u=a(69880),g=a.n(u),M=a(62477),x=a.n(M),f=a(7392),_=a.n(f),w=a(93431),b=a.n(w);const y={sidebar_label:"Minimax"},A="Minimax",v={id:"integrations/chat/minimax",title:"Minimax",description:"Minimax is a Chinese startup that provides natural language processing models for companies and individuals.",source:"@site/docs/integrations/chat/minimax.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/minimax",permalink:"/docs/integrations/chat/minimax",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/minimax.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"Minimax"},sidebar:"integrations",previous:{title:"Llama CPP",permalink:"/docs/integrations/chat/llama_cpp"},next:{title:"Mistral AI",permalink:"/docs/integrations/chat/mistral"}},j={},k=[{value:"Setup",id:"setup",level:2},...o.toc,...m.toc,{value:"Basic usage",id:"basic-usage",level:2},{value:"Chain model calls",id:"chain-model-calls",level:2},{value:"With function calls",id:"with-function-calls",level:2},{value:"Functions with Zod",id:"functions-with-zod",level:2},{value:"With glyph",id:"with-glyph",level:2},{value:"With sample messages",id:"with-sample-messages",level:2},{value:"With plugins",id:"with-plugins",level:2}];function C(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"minimax",children:"Minimax"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:"https://api.minimax.chat",children:"Minimax"})," is a Chinese startup that provides natural language processing models for companies and individuals."]}),"\n",(0,t.jsx)(e.p,{children:"This example demonstrates using LangChain.js to interact with Minimax."}),"\n",(0,t.jsx)(e.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsxs)(e.p,{children:["To use Minimax models, you'll need a ",(0,t.jsx)(e.a,{href:"https://api.minimax.chat",children:"Minimax account"}),", an ",(0,t.jsx)(e.a,{href:"https://api.minimax.chat/user-center/basic-information/interface-key",children:"API key"}),", and a ",(0,t.jsx)(e.a,{href:"https://api.minimax.chat/user-center/basic-information",children:"Group ID"})]}),"\n","\n",(0,t.jsx)(o.default,{}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/community\n"})}),"\n","\n",(0,t.jsx)(m.default,{}),"\n",(0,t.jsx)(e.h2,{id:"basic-usage",children:"Basic usage"}),"\n","\n",(0,t.jsx)(i.A,{language:"typescript",children:c()}),"\n",(0,t.jsx)(e.h2,{id:"chain-model-calls",children:"Chain model calls"}),"\n","\n",(0,t.jsx)(i.A,{language:"typescript",children:h()}),"\n",(0,t.jsx)(e.h2,{id:"with-function-calls",children:"With function calls"}),"\n","\n",(0,t.jsx)(i.A,{language:"typescript",children:p()}),"\n",(0,t.jsx)(e.h2,{id:"functions-with-zod",children:"Functions with Zod"}),"\n",(0,t.jsx)(i.A,{language:"typescript",children:g()}),"\n",(0,t.jsx)(e.h2,{id:"with-glyph",children:"With glyph"}),"\n",(0,t.jsx)(e.p,{children:"This feature can help users force the model to return content in the requested format."}),"\n","\n",(0,t.jsx)(i.A,{language:"typescript",children:x()}),"\n",(0,t.jsx)(e.h2,{id:"with-sample-messages",children:"With sample messages"}),"\n",(0,t.jsx)(e.p,{children:"This feature can help the model better understand the return information the user wants to get,\nincluding but not limited to the content, format, and response mode of the information."}),"\n","\n",(0,t.jsx)(i.A,{language:"typescript",children:_()}),"\n",(0,t.jsx)(e.h2,{id:"with-plugins",children:"With plugins"}),"\n",(0,t.jsx)(e.p,{children:"This feature supports calling tools like a search engine to get additional data that can assist the model."}),"\n","\n",(0,t.jsx)(i.A,{language:"typescript",children:b()})]})}function I(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(C,{...n})}):C(n)}},2280:(n,e,a)=>{"use strict";a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>m,toc:()=>c});var t=a(74848),s=a(28453);const i={},o=void 0,m={id:"mdx_components/unified_model_params_tooltip",title:"unified_model_params_tooltip",description:"We're unifying model params across all packages. We now suggest using model instead of modelName, and apiKey for API keys.",source:"@site/docs/mdx_components/unified_model_params_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/unified_model_params_tooltip",permalink:"/docs/mdx_components/unified_model_params_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/unified_model_params_tooltip.mdx",tags:[],version:"current",frontMatter:{}},l={},c=[];function r(n){const e={admonition:"admonition",code:"code",p:"p",...(0,s.R)(),...n.components};return(0,t.jsx)(e.admonition,{type:"tip",children:(0,t.jsxs)(e.p,{children:["We're unifying model params across all packages. We now suggest using ",(0,t.jsx)(e.code,{children:"model"})," instead of ",(0,t.jsx)(e.code,{children:"modelName"}),", and ",(0,t.jsx)(e.code,{children:"apiKey"})," for API keys."]})})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(r,{...n})}):r(n)}},5341:n=>{n.exports={content:'import { ChatMinimax } from "@langchain/community/chat_models/minimax";\nimport { HumanMessage } from "@langchain/core/messages";\n\n// Use abab5.5\nconst abab5_5 = new ChatMinimax({\n  model: "abab5.5-chat",\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n});\nconst messages = [\n  new HumanMessage({\n    content: "Hello",\n  }),\n];\n\nconst res = await abab5_5.invoke(messages);\nconsole.log(res);\n\n/*\nAIChatMessage {\n  text: \'Hello! How may I assist you today?\',\n  name: undefined,\n  additional_kwargs: {}\n  }\n}\n*/\n\n// use abab5\nconst abab5 = new ChatMinimax({\n  proVersion: false,\n  model: "abab5-chat",\n  minimaxGroupId: process.env.MINIMAX_GROUP_ID, // In Node.js defaults to process.env.MINIMAX_GROUP_ID\n  minimaxApiKey: process.env.MINIMAX_API_KEY, // In Node.js defaults to process.env.MINIMAX_API_KEY\n});\n\nconst result = await abab5.invoke([\n  new HumanMessage({\n    content: "Hello",\n    name: "XiaoMing",\n  }),\n]);\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'Hello! Can I help you with anything?\',\n    additional_kwargs: { function_call: undefined }\n  },\n  lc_namespace: [ \'langchain\', \'schema\' ],\n  content: \'Hello! Can I help you with anything?\',\n  name: undefined,\n  additional_kwargs: { function_call: undefined }\n}\n */\n',imports:[{local:"ChatMinimax",imported:"ChatMinimax",source:"@langchain/community/chat_models/minimax"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},95778:n=>{n.exports={content:'import { LLMChain } from "langchain/chains";\nimport { ChatMinimax } from "@langchain/community/chat_models/minimax";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  SystemMessagePromptTemplate,\n} from "@langchain/core/prompts";\n\n// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.\nconst chat = new ChatMinimax({ temperature: 0.01 });\n\nconst chatPrompt = ChatPromptTemplate.fromMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    "You are a helpful assistant that translates {input_language} to {output_language}."\n  ),\n  HumanMessagePromptTemplate.fromTemplate("{text}"),\n]);\nconst chainB = new LLMChain({\n  prompt: chatPrompt,\n  llm: chat,\n});\n\nconst resB = await chainB.invoke({\n  input_language: "English",\n  output_language: "Chinese",\n  text: "I love programming.",\n});\nconsole.log({ resB });\n',imports:[{local:"LLMChain",imported:"LLMChain",source:"langchain/chains"},{local:"ChatMinimax",imported:"ChatMinimax",source:"@langchain/community/chat_models/minimax"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"HumanMessagePromptTemplate",imported:"HumanMessagePromptTemplate",source:"@langchain/core/prompts"},{local:"SystemMessagePromptTemplate",imported:"SystemMessagePromptTemplate",source:"@langchain/core/prompts"}]}},37050:n=>{n.exports={content:'import { ChatMinimax } from "@langchain/community/chat_models/minimax";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst functionSchema = {\n  name: "get_weather",\n  description: " Get weather information.",\n  parameters: {\n    type: "object",\n    properties: {\n      location: {\n        type: "string",\n        description: " The location to get the weather",\n      },\n    },\n    required: ["location"],\n  },\n};\n\n// Bind function arguments to the model.\n// All subsequent invoke calls will use the bound parameters.\n// "functions.parameters" must be formatted as JSON Schema\nconst model = new ChatMinimax({\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n}).bind({\n  functions: [functionSchema],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content: " What is the weather like in NewYork tomorrow?",\n    name: "I",\n  }),\n]);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: \'\', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ \'langchain\', \'schema\' ],\n  content: \'\',\n  name: undefined,\n  additional_kwargs: {\n    function_call: { name: \'get_weather\', arguments: \'{"location": "NewYork"}\' }\n  }\n}\n*/\n\n// Alternatively, you can pass function call arguments as an additional argument as a one-off:\n\nconst minimax = new ChatMinimax({\n  model: "abab5.5-chat",\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n});\n\nconst result2 = await minimax.invoke(\n  [new HumanMessage("What is the weather like in NewYork tomorrow?")],\n  {\n    functions: [functionSchema],\n  }\n);\nconsole.log(result2);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: \'\', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ \'langchain\', \'schema\' ],\n  content: \'\',\n  name: undefined,\n  additional_kwargs: {\n    function_call: { name: \'get_weather\', arguments: \'{"location": "NewYork"}\' }\n  }\n}\n */\n',imports:[{local:"ChatMinimax",imported:"ChatMinimax",source:"@langchain/community/chat_models/minimax"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},69880:n=>{n.exports={content:'import { z } from "zod";\nimport { zodToJsonSchema } from "zod-to-json-schema";\nimport { ChatMinimax } from "@langchain/community/chat_models/minimax";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst extractionFunctionZodSchema = z.object({\n  location: z.string().describe(" The location to get the weather"),\n});\n\n// Bind function arguments to the model.\n// "functions.parameters" must be formatted as JSON Schema.\n// We translate the above Zod schema into JSON schema using the "zodToJsonSchema" package.\n\nconst model = new ChatMinimax({\n  model: "abab5.5-chat",\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n}).bind({\n  functions: [\n    {\n      name: "get_weather",\n      description: " Get weather information.",\n      parameters: zodToJsonSchema(extractionFunctionZodSchema),\n    },\n  ],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content: " What is the weather like in Shanghai tomorrow?",\n    name: "XiaoMing",\n  }),\n]);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: \'\', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ \'langchain\', \'schema\' ],\n  content: \'\',\n  name: undefined,\n  additional_kwargs: {\n    function_call: { name: \'get_weather\', arguments: \'{"location": "Shanghai"}\' }\n  }\n}\n*/\n',imports:[{local:"ChatMinimax",imported:"ChatMinimax",source:"@langchain/community/chat_models/minimax"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},62477:n=>{n.exports={content:'import { ChatMinimax } from "@langchain/community/chat_models/minimax";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n} from "@langchain/core/prompts";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst model = new ChatMinimax({\n  model: "abab5.5-chat",\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n}).bind({\n  replyConstraints: {\n    sender_type: "BOT",\n    sender_name: "MM Assistant",\n    glyph: {\n      type: "raw",\n      raw_glyph: "The translated text\uff1a{{gen \'content\'}}",\n    },\n  },\n});\n\nconst messagesTemplate = ChatPromptTemplate.fromMessages([\n  HumanMessagePromptTemplate.fromTemplate(\n    " Please help me translate the following sentence in English\uff1a {text}"\n  ),\n]);\n\nconst messages = await messagesTemplate.formatMessages({ text: "\u6211\u662f\u8c01" });\nconst result = await model.invoke(messages);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'The translated text\uff1a Who am I\\x02\',\n    additional_kwargs: { function_call: undefined }\n  },\n  lc_namespace: [ \'langchain\', \'schema\' ],\n  content: \'The translated text\uff1a Who am I\\x02\',\n  name: undefined,\n  additional_kwargs: { function_call: undefined }\n}\n*/\n\n// use json_value\n\nconst modelMinimax = new ChatMinimax({\n  model: "abab5.5-chat",\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n}).bind({\n  replyConstraints: {\n    sender_type: "BOT",\n    sender_name: "MM Assistant",\n    glyph: {\n      type: "json_value",\n      json_properties: {\n        name: {\n          type: "string",\n        },\n        age: {\n          type: "number",\n        },\n        is_student: {\n          type: "boolean",\n        },\n        is_boy: {\n          type: "boolean",\n        },\n        courses: {\n          type: "object",\n          properties: {\n            name: {\n              type: "string",\n            },\n            score: {\n              type: "number",\n            },\n          },\n        },\n      },\n    },\n  },\n});\n\nconst result2 = await modelMinimax.invoke([\n  new HumanMessage({\n    content:\n      " My name is Yue Wushuang, 18 years old this year, just finished the test with 99.99 points.",\n    name: "XiaoMing",\n  }),\n]);\n\nconsole.log(result2);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'{\\n\' +\n      \'  "name": "Yue Wushuang",\\n\' +\n      \'  "is_student": true,\\n\' +\n      \'  "is_boy": false,\\n\' +\n      \'  "courses":   {\\n\' +\n      \'    "name": "Mathematics",\\n\' +\n      \'    "score": 99.99\\n\' +\n      \'   },\\n\' +\n      \'  "age": 18\\n\' +\n      \' }\',\n    additional_kwargs: { function_call: undefined }\n  }\n}\n\n */\n',imports:[{local:"ChatMinimax",imported:"ChatMinimax",source:"@langchain/community/chat_models/minimax"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"},{local:"HumanMessagePromptTemplate",imported:"HumanMessagePromptTemplate",source:"@langchain/core/prompts"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},93431:n=>{n.exports={content:'import { ChatMinimax } from "@langchain/community/chat_models/minimax";\nimport { HumanMessage } from "@langchain/core/messages";\n\nconst model = new ChatMinimax({\n  model: "abab5.5-chat",\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n}).bind({\n  plugins: ["plugin_web_search"],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content: " What is the weather like in NewYork tomorrow?",\n  }),\n]);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: \'The weather in Shanghai tomorrow is expected to be hot. Please note that this is just a forecast and the actual weather conditions may vary.\',\n    additional_kwargs: { function_call: undefined }\n  },\n  lc_namespace: [ \'langchain\', \'schema\' ],\n  content: \'The weather in Shanghai tomorrow is expected to be hot. Please note that this is just a forecast and the actual weather conditions may vary.\',\n  name: undefined,\n  additional_kwargs: { function_call: undefined }\n}\n*/\n',imports:[{local:"ChatMinimax",imported:"ChatMinimax",source:"@langchain/community/chat_models/minimax"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}},7392:n=>{n.exports={content:'import { ChatMinimax } from "@langchain/community/chat_models/minimax";\nimport { AIMessage, HumanMessage } from "@langchain/core/messages";\n\nconst model = new ChatMinimax({\n  model: "abab5.5-chat",\n  botSetting: [\n    {\n      bot_name: "MM Assistant",\n      content: "MM Assistant is an AI Assistant developed by minimax.",\n    },\n  ],\n}).bind({\n  sampleMessages: [\n    new HumanMessage({\n      content: " Turn A5 into red and modify the content to minimax.",\n    }),\n    new AIMessage({\n      content: "select A5 color red change minimax",\n    }),\n  ],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content:\n      \' Please reply to my content according to the following requirements: According to the following interface list, give the order and parameters of calling the interface for the content I gave. You just need to give the order and parameters of calling the interface, and do not give any other output. The following is the available interface list: select: select specific table position, input parameter use letters and numbers to determine, for example "B13"; color: dye the selected table position, input parameters use the English name of the color, for example "red"; change: modify the selected table position, input parameters use strings.\',\n  }),\n  new HumanMessage({\n    content: " Process B6 to gray and modify the content to question.",\n  }),\n]);\n\nconsole.log(result);\n',imports:[{local:"ChatMinimax",imported:"ChatMinimax",source:"@langchain/community/chat_models/minimax"},{local:"AIMessage",imported:"AIMessage",source:"@langchain/core/messages"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}}}]);