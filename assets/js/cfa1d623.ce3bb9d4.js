(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8577,65],{81739:(a,n,e)=>{"use strict";e.r(n),e.d(n,{assets:()=>f,contentTitle:()=>u,default:()=>b,frontMatter:()=>d,metadata:()=>g,toc:()=>x});var t=e(74848),o=e(28453),l=e(78847),s=e(64428),i=e(87274),r=e.n(i),m=e(71365),c=e.n(m),p=e(32571),h=e.n(p);const d={sidebar_label:"Ollama"},u="ChatOllama",g={id:"integrations/chat/ollama",title:"ChatOllama",description:"Ollama allows you to run open-source large language models, such as Llama 2, locally.",source:"@site/docs/integrations/chat/ollama.mdx",sourceDirName:"integrations/chat",slug:"/integrations/chat/ollama",permalink:"/docs/integrations/chat/ollama",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/integrations/chat/ollama.mdx",tags:[],version:"current",frontMatter:{sidebar_label:"Ollama"},sidebar:"integrations",previous:{title:"NIBittensorChatModel",permalink:"/docs/integrations/chat/ni_bittensor"},next:{title:"Ollama Functions",permalink:"/docs/integrations/chat/ollama_functions"}},f={},x=[{value:"Setup",id:"setup",level:2},...l.toc,{value:"Usage",id:"usage",level:2},{value:"JSON mode",id:"json-mode",level:2},{value:"Multimodal models",id:"multimodal-models",level:2}];function j(a){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,o.R)(),...a.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chatollama",children:"ChatOllama"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"https://ollama.ai/",children:"Ollama"})," allows you to run open-source large language models, such as Llama 2, locally."]}),"\n",(0,t.jsx)(n.p,{children:"Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage."}),"\n",(0,t.jsxs)(n.p,{children:["This example goes over how to use LangChain to interact with an Ollama-run Llama 2 7b instance as a chat model.\nFor a complete list of supported models and model variants, see the ",(0,t.jsx)(n.a,{href:"https://github.com/jmorganca/ollama#model-library",children:"Ollama model library"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,t.jsxs)(n.p,{children:["Follow ",(0,t.jsx)(n.a,{href:"https://github.com/jmorganca/ollama",children:"these instructions"})," to set up and run a local Ollama instance."]}),"\n","\n",(0,t.jsx)(l.default,{}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:"npm2yarn",children:"npm install @langchain/community\n"})}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n","\n",(0,t.jsx)(s.A,{language:"typescript",children:r()}),"\n",(0,t.jsx)(n.h2,{id:"json-mode",children:"JSON mode"}),"\n",(0,t.jsx)(n.p,{children:"Ollama also supports a JSON mode that coerces model outputs to only return JSON. Here's an example of how this can be useful for extraction:"}),"\n","\n",(0,t.jsx)(s.A,{language:"typescript",children:c()}),"\n",(0,t.jsxs)(n.p,{children:["You can see a simple LangSmith trace of this here: ",(0,t.jsx)(n.a,{href:"https://smith.langchain.com/public/92aebeca-d701-4de0-a845-f55df04eff04/r",children:"https://smith.langchain.com/public/92aebeca-d701-4de0-a845-f55df04eff04/r"})]}),"\n",(0,t.jsx)(n.h2,{id:"multimodal-models",children:"Multimodal models"}),"\n",(0,t.jsxs)(n.p,{children:["Ollama supports open source multimodal models like ",(0,t.jsx)(n.a,{href:"https://ollama.ai/library/llava",children:"LLaVA"})," in versions 0.1.15 and up.\nYou can pass images as part of a message's ",(0,t.jsx)(n.code,{children:"content"})," field to multimodal-capable models like this:"]}),"\n","\n",(0,t.jsx)(s.A,{language:"typescript",children:h()}),"\n",(0,t.jsx)(n.p,{children:"This will currently not use the image's position within the prompt message as additional information, and will just pass\nthe image along as context with the rest of the prompt messages."})]})}function b(a={}){const{wrapper:n}={...(0,o.R)(),...a.components};return n?(0,t.jsx)(n,{...a,children:(0,t.jsx)(j,{...a})}):j(a)}},78847:(a,n,e)=>{"use strict";e.r(n),e.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>p,frontMatter:()=>l,metadata:()=>i,toc:()=>m});var t=e(74848),o=e(28453);const l={},s=void 0,i={id:"mdx_components/integration_install_tooltip",title:"integration_install_tooltip",description:"See this section for general instructions on installing integration packages.",source:"@site/docs/mdx_components/integration_install_tooltip.mdx",sourceDirName:"mdx_components",slug:"/mdx_components/integration_install_tooltip",permalink:"/docs/mdx_components/integration_install_tooltip",draft:!1,unlisted:!1,editUrl:"https://langchainjs-kr.site/docs/mdx_components/integration_install_tooltip.mdx",tags:[],version:"current",frontMatter:{}},r={},m=[];function c(a){const n={a:"a",admonition:"admonition",p:"p",...(0,o.R)(),...a.components};return(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["See ",(0,t.jsx)(n.a,{href:"/docs/how_to/installation#installing-integration-packages",children:"this section for general instructions on installing integration packages"}),"."]})})}function p(a={}){const{wrapper:n}={...(0,o.R)(),...a.components};return n?(0,t.jsx)(n,{...a,children:(0,t.jsx)(c,{...a})}):c(a)}},87274:a=>{a.exports={content:'import { ChatOllama } from "@langchain/community/chat_models/ollama";\nimport { StringOutputParser } from "@langchain/core/output_parsers";\n\nconst model = new ChatOllama({\n  baseUrl: "http://localhost:11434", // Default value\n  model: "llama2", // Default value\n});\n\nconst stream = await model\n  .pipe(new StringOutputParser())\n  .stream(`Translate "I love programming" into German.`);\n\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks.join(""));\n\n/*\n  Thank you for your question! I\'m happy to help. However, I must point out that the phrase "I love programming" is not grammatically correct in German. The word "love" does not have a direct translation in German, and it would be more appropriate to say "I enjoy programming" or "I am passionate about programming."\n\n  In German, you can express your enthusiasm for something like this:\n\n  * Ich m\xf6chte Programmieren (I want to program)\n  * Ich mag Programmieren (I like to program)\n  * Ich bin passioniert \xfcber Programmieren (I am passionate about programming)\n\n  I hope this helps! Let me know if you have any other questions.\n*/\n',imports:[{local:"ChatOllama",imported:"ChatOllama",source:"@langchain/community/chat_models/ollama"},{local:"StringOutputParser",imported:"StringOutputParser",source:"@langchain/core/output_parsers"}]}},71365:a=>{a.exports={content:'import { ChatOllama } from "@langchain/community/chat_models/ollama";\nimport { ChatPromptTemplate } from "@langchain/core/prompts";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    "system",\n    `You are an expert translator. Format all responses as JSON objects with two keys: "original" and "translated".`,\n  ],\n  ["human", `Translate "{input}" into {language}.`],\n]);\n\nconst model = new ChatOllama({\n  baseUrl: "http://localhost:11434", // Default value\n  model: "llama2", // Default value\n  format: "json",\n});\n\nconst chain = prompt.pipe(model);\n\nconst result = await chain.invoke({\n  input: "I love programming",\n  language: "German",\n});\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: \'{"original": "I love programming", "translated": "Ich liebe das Programmieren"}\',\n    additional_kwargs: {}\n  }\n*/\n',imports:[{local:"ChatOllama",imported:"ChatOllama",source:"@langchain/community/chat_models/ollama"},{local:"ChatPromptTemplate",imported:"ChatPromptTemplate",source:"@langchain/core/prompts"}]}},32571:a=>{a.exports={content:'import { ChatOllama } from "@langchain/community/chat_models/ollama";\nimport { HumanMessage } from "@langchain/core/messages";\nimport * as fs from "node:fs/promises";\n\nconst imageData = await fs.readFile("./hotdog.jpg");\nconst chat = new ChatOllama({\n  model: "llava",\n  baseUrl: "http://127.0.0.1:11434",\n});\nconst res = await chat.invoke([\n  new HumanMessage({\n    content: [\n      {\n        type: "text",\n        text: "What is in this image?",\n      },\n      {\n        type: "image_url",\n        image_url: `data:image/jpeg;base64,${imageData.toString("base64")}`,\n      },\n    ],\n  }),\n]);\nconsole.log(res);\n\n/*\n  AIMessage {\n    content: \' The image shows a hot dog with ketchup on it, placed on top of a bun. It appears to be a close-up view, possibly taken in a kitchen setting or at an outdoor event.\',\n    name: undefined,\n    additional_kwargs: {}\n  }\n*/\n',imports:[{local:"ChatOllama",imported:"ChatOllama",source:"@langchain/community/chat_models/ollama"},{local:"HumanMessage",imported:"HumanMessage",source:"@langchain/core/messages"}]}}}]);